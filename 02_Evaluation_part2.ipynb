{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5658a8-dbe8-478e-9abf-2917bf9c8cff",
   "metadata": {},
   "source": [
    "# **02 - LLM Evaluation Part 2**\n",
    "\n",
    "Before starting this tutorial, **stop the kernels you have run so far**. The following image shows where to go. Then click on `Shut Down All`.  \n",
    "![image](images/stop_kernels.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40643a-a7c9-45b2-8079-25cd314b9397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from torchmetrics.text import Perplexity\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import (AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer)\n",
    "from utils import seed_everything\n",
    "from jupyterquiz import display_quiz\n",
    "\n",
    "\n",
    "quiz_path = Path(\"./quiz/evaluation.json\")\n",
    "quiz = json.loads(quiz_path.read_text())\n",
    "\n",
    "DSDIR = Path(os.environ[\"DSDIR\"])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0333b-8601-4c7d-b2a9-e61823f07dc3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Implementing validation loop using the Phi-2 model to compute Perplexity**\n",
    "\n",
    "In the roleplay example, we don't have a validation dataset. We will create one. This dataset will not be supervised, i.e. we will not get the expected response for a message. We will therefore not be able to use the previous metrics, which often require ground truth. We will compute **perplexity** during the training.\n",
    "\n",
    "### **Dataset**\n",
    "First, let's load our data again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3fee2-0365-4bf9-8bbc-c588689f07ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "roleplay_dataset = datasets.load_from_disk(DSDIR / \"HuggingFace/hieunguyenminh/roleplay\", keep_in_memory=True)\n",
    "roleplay_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc530133-e4ea-43eb-837a-6791f88f6bf0",
   "metadata": {},
   "source": [
    "For an evaluation we need 3 sets: a **training** dataset, a **validation** dataset and a **final evaluation** dataset.\n",
    "\n",
    "For the example, we will take **80% of the data for the train**. **20% will be used for the evaluation game**. We will take **20% of the training set for validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a7d4a-9146-4e7c-8f63-50b772febd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roleplay_dataset = roleplay_dataset['train'].train_test_split(train_size=0.8)\n",
    "ds_test = roleplay_dataset.pop(\"test\")\n",
    "print(roleplay_dataset)\n",
    "print(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70560927-ad5c-4a46-8af4-01d9b398cdae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we take 20% of the train for validation\n",
    "roleplay_dataset = roleplay_dataset[\"train\"].train_test_split(train_size=0.8)\n",
    "ds_valid = roleplay_dataset.pop(\"test\")\n",
    "ds_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ff507a-35e6-4994-a0e2-25d7993d229e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "roleplay_dataset[\"valid\"] = ds_valid\n",
    "roleplay_dataset[\"test\"] = ds_test\n",
    "\n",
    "roleplay_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1507358a-f672-420f-b213-0a1d645a7a3a",
   "metadata": {},
   "source": [
    "### **Loading the Phi-2 model and its tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9436fadc-83a6-4c34-99fa-35cd7a9b2d4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/microsoft/phi-2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,  # Allow using code that was not written by HuggingFace\n",
    "    attn_implementation=\"flash_attention_2\",  # Optimize the model with Flash Attention,\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734f02d7-1cf1-41ac-8cab-e302ce2dfc2c",
   "metadata": {},
   "source": [
    "We'll take a look at some of the functions we've already written that will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e853723b-1353-424f-bd80-fc4c7430e6c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "    model_inp = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], do_sample=False, max_new_tokens=100)\n",
    "    print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1013c-5b9a-4427-8daa-d8918b8158b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RoleplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024, nb_tokens=3160542):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.separator = tokenizer.eos_token_id  # The token that will seperate different sample\n",
    "        self.hf_dataset = hf_dataset\n",
    "        # It will allow us to sample an index of the HF_dataset randomly\n",
    "        self.idx_iterator = iter(random.sample(range(len(hf_dataset)), len(hf_dataset)))\n",
    "        self.seq_length = seq_length\n",
    "        self.nb_tokens = nb_tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nb_tokens // self.seq_length\n",
    "\n",
    "    def get_next_sample(self):\n",
    "            \"\"\"Retrieves the next sample from the dataset and tokenize it.\"\"\"\n",
    "            idx = next(self.idx_iterator)\n",
    "            text = self.hf_dataset[idx][\"text\"]\n",
    "            return self.tokenizer(text)['input_ids'] + [self.separator]\n",
    "\n",
    "    def __iter__(self):\n",
    "        next_sample_ids = None\n",
    "        all_token_ids = []\n",
    "        idx = 0\n",
    "\n",
    "        while idx < self.__len__():\n",
    "            if next_sample_ids is None:\n",
    "                next_sample_ids = self.get_next_sample()\n",
    "\n",
    "            if len(all_token_ids) + len(next_sample_ids) <= self.seq_length:\n",
    "                # if the next HF_dataset sample can fit in the current dataset sample\n",
    "                # we add it\n",
    "                all_token_ids += next_sample_ids\n",
    "                next_sample_ids = None\n",
    "                \n",
    "            else:\n",
    "                # if the next HF_dataset sample can't fit in the current dataset\n",
    "                # sample, we add what we can in the dataset sample and then we yield it\n",
    "                # note: we add one more element compared to seq_length to return to\n",
    "                # seq_length when generating inputs and targets (see train_collate())\n",
    "                idx_break = self.seq_length - len(all_token_ids)\n",
    "                all_token_ids += next_sample_ids[: idx_break + 1]\n",
    "                next_sample_ids = next_sample_ids[idx_break + 1 :]\n",
    "                \n",
    "                model_inp = torch.tensor(all_token_ids[:-1], dtype=torch.int64)\n",
    "                labels = torch.tensor(all_token_ids[1:], dtype=torch.int64) \n",
    "                yield model_inp, labels\n",
    "\n",
    "                all_token_ids = []\n",
    "                idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58e6d0-0d57-4f49-803e-da95d26c4a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(hf_dataset, tokenizer):\n",
    "    total_tokens = 0\n",
    "    loop = tqdm(hf_dataset)\n",
    "    # iterate over every element of the dataset\n",
    "    for element in loop:\n",
    "        # Count the number of token of one element\n",
    "        nb_token_element = len(tokenizer(element['text'])[\"input_ids\"])\n",
    "        # Add the count to the total count\n",
    "        total_tokens += nb_token_element\n",
    "        \n",
    "        loop.set_postfix(tokens_count=total_tokens)\n",
    "        \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3ad86-9bf7-46d8-8ef3-98d837c3378f",
   "metadata": {},
   "source": [
    "We now initialize our training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ca3271-3256-4781-9b23-942e820503c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens_train = count_tokens(roleplay_dataset['train'], tokenizer)\n",
    "seq_length=512\n",
    "train_dataset = RoleplayDataset(tokenizer, roleplay_dataset['train'], seq_length=seq_length, nb_tokens=tokens_train)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55755a67-47cb-4591-9268-c8997f410456",
   "metadata": {
    "tags": []
   },
   "source": [
    "We are also taking back the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e2cce-c794-4cbf-b9b9-90af21074d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea0b49a-4464-4d62-9798-658e457ba824",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of train batches: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9523231-18c5-4a5a-9acf-6ac129e620e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Validation dataset**\n",
    "\n",
    "To evaluate our model, we **need a dataset that does not concatenate the examples for real-life conditions**. We will then use the same logic as in the training dataset, shifting our tokens by 1 between our input and our target. This will allow us to compute **perplexity** on **unseen data**. \n",
    "\n",
    "This metric is very fast because it does not require any text to be generated. For metrics that need references, we'll look at this in the second tutorial on evaluation. \n",
    "\n",
    "Previously, we concatenated all our examples. So all our elements were the same size. This is not the case here. We're going to have to do some padding.  \n",
    "**Padding should be used if the examples are too short**, otherwise the examples should be **truncated**.\n",
    "In the next cell, **the padding is set to the left**, i.e. we add our padding token at the beginning. This is recommended for Decoders such as Phi-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02185b-875d-4db4-99e2-48b6f292b393",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90b82c5-46ce-4374-ae56-b9b07152df7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A cell to help you to answer the following question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71fe580-ae17-46aa-919e-21431e946cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_quiz([quiz[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041bde33-d64d-4749-a5a9-f94588fcd984",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"By default, Phi-2 has {tokenizer.pad_token_id} padding token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0dbd66-f7cd-458c-94bc-e3684d1b8d27",
   "metadata": {},
   "source": [
    "We define a token padding called `[PAD]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4711d5-aa4f-4f3d-9415-e29fe2b0c12f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "print(f\"The padding token is {tokenizer.pad_token} and its id is {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64875a7-8649-47da-ad73-b73bb4a85c2a",
   "metadata": {},
   "source": [
    "Now let's create our **validation dataset to compute perplexity**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb7e717-f93e-4997-8e9e-08e9b48fb73f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to write the validation dataset class called `ValidRoleplayDataset`. This will inherit from the `Dataset` class in `torch.utils.data`. The constructor will take as parameters a tokenizer, a huggingface dataset and the maximum length of a sequence. Two other methods need to be defined: `__len__(self)` and `__getitem__(self, idx)`.  `__getitem__` must return the text tokenised with input and target (offset by 1).  \n",
    "**If a sequence is too long (> `seq_length`), truncation is necessary. Padding will then be performed in the batch collate function.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f674d4-df97-434e-a0da-533ee788b1e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ValidRoleplayDataset(Dataset):\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceb0627-299d-4167-9a4d-cd9755fc6d6c",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8911fa0-ae61-4e7e-9603-cacef598bcbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class ValidRoleplayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        ############ Complete here ############\n",
    "\n",
    "        #######################################\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        ############ Complete here ############\n",
    "        text = \n",
    "        text_tokenized = self.tokenizer(text, \n",
    "                                        max_length=\n",
    "                                        truncation=\n",
    "                                       )['input_ids']\n",
    "        #######################################\n",
    "    \n",
    "        model_inp = torch.tensor(text_tokenized[:-1], dtype=torch.int64)\n",
    "        labels = torch.tensor(text_tokenized[1:], dtype=torch.int64) \n",
    "        \n",
    "        return model_inp, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f532f4ac-20ce-4aaf-b37a-774a9fe2202b",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1eac26e5-e6d8-4507-9b71-9c9cdd3098d6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class ValidRoleplayDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.hf_dataset[idx][\"text\"]\n",
    "        text_tokenized = self.tokenizer(text, \n",
    "                                        max_length=self.seq_length, \n",
    "                                        truncation=True\n",
    "                                       )['input_ids']\n",
    "\n",
    "        model_inp = torch.tensor(text_tokenized[:-1], dtype=torch.int64)\n",
    "        labels = torch.tensor(text_tokenized[1:], dtype=torch.int64) \n",
    "        \n",
    "        return model_inp, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef3459-c491-47d0-b084-c5eb74cff3e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_dataset = ValidRoleplayDataset(tokenizer, roleplay_dataset['valid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b0032-fcef-4b22-ad6c-7f671424ecd1",
   "metadata": {},
   "source": [
    "Take a look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6045839-27b2-4623-85bc-1849f458802d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first sample\n",
    "sample = valid_dataset[0]\n",
    "print(f\"model_inp shape {sample[0].shape}\")\n",
    "print(f\"labels shape {sample[1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561befa5-f950-4e65-ae04-f28a71983625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# second sample\n",
    "sample = valid_dataset[1]\n",
    "print(f\"model_inp shape {sample[0].shape}\")\n",
    "print(f\"labels shape {sample[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72830d13-2f48-4e85-987e-0d64d60c7b09",
   "metadata": {},
   "source": [
    "**The sizes are different**. It is impossible to create batches in this case. We're going to add a function to add padding when creating batches. All the elements in the batch will be the **same size**. The padding is done in such a way as to have tensors equal to the size of the largest. **This can be less than `seq_length`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ac663e-7e24-49d3-9bdd-e308ac12d3da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding_collate(batch):\n",
    "    all_model_inp = []\n",
    "    all_labels = []\n",
    "    # get max size to add padding not necessarily up to `seq_length`\n",
    "    longest_tensor = max(len(elem[0]) for elem in batch)\n",
    "    \n",
    "    for model_inp, labels in batch:\n",
    "        nb_padding_to_add = longest_tensor - model_inp.shape[0]\n",
    "        padding = (nb_padding_to_add, 0)  # (left padding, right padding)\n",
    "        \n",
    "        model_inp_pad = torch.nn.functional.pad(model_inp, padding, mode='constant', value=tokenizer.pad_token_id)\n",
    "        labels_pad = torch.nn.functional.pad(labels, padding, mode='constant', value=tokenizer.pad_token_id)\n",
    "\n",
    "        all_model_inp.append(model_inp_pad)\n",
    "        all_labels.append(labels_pad)\n",
    "    \n",
    "    all_model_inp = torch.stack(all_model_inp)\n",
    "    all_labels = torch.stack(all_labels)\n",
    "    return all_model_inp, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7408f1b-affd-4bd5-a61a-b5ab23ee8ff2",
   "metadata": {
    "tags": []
   },
   "source": [
    "We then initialize the validation dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e3d48e-9df7-4e55-94e1-66581e2e4d21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=2,\n",
    "    shuffle=False,  # shuffle is useless during validation step\n",
    "    collate_fn=padding_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde02c4-83b7-43db-b130-bf96e2d078ba",
   "metadata": {},
   "source": [
    "You can test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab50ee-9c73-4818-960f-4b9ec172c0db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch = next(iter(valid_dataloader))\n",
    "print(f\"model_inp shape {batch[0].shape}\")\n",
    "print(f\"all_labels shape {batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b03532-dc0c-4357-85fe-4a69a21ad0d8",
   "metadata": {},
   "source": [
    "Finally, we are reusing the **loss, its preparation and the training loop identically**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c8d70-2248-4259-93cc-1f202761a9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Optimizer and Criterion\n",
    "# We choose the CrossEntropyLoss and Adam because they're the most used\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d1a3bb-1686-4b05-bb34-2f34ed17b0fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_for_loss(logits, labels):\n",
    "    \"\"\"Unfold the Tensors to compute the CrossEntropyLoss correctly\"\"\"\n",
    "    batch_size, seq_length, vocab_size = logits.shape\n",
    "    logits = logits.view(batch_size * seq_length, vocab_size)\n",
    "    labels = labels.view(batch_size * seq_length)\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08409aad-0e6b-49b8-86a7-160d047860bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader, desc=\"train\")\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        logits = model(model_inp).logits\n",
    "\n",
    "        logits, labels = prepare_for_loss(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98783b16-d193-45a6-be53-51fff164263e",
   "metadata": {},
   "source": [
    "We can redo a small test as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c6e63-07cc-4537-9084-3e8fcd370ea2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|assistant|>\"\"\"\n",
    "generate_text(model, tokenizer, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f150fe-bfe9-4d4f-9f86-844a3548e473",
   "metadata": {},
   "source": [
    "### **Validation evaluation loop**\n",
    "\n",
    "Now we have everything we need to set up a validation loop during the training of our Phi-2 model.\n",
    "\n",
    "Firstly, we will **visually generate results of a few pre-determined prompts**. \n",
    "Below is a list of prompts. You can modify them as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1166a7b-1b58-4243-9da2-14594c4a67ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visu_prompts = [\n",
    "    \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|assistant|>\"\"\",\n",
    "    \"<|system|>\",\n",
    "    \"<|user|>\", \n",
    "    \"<|assistant|>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeaf208-4663-4c88-9a4f-97102d7bee3e",
   "metadata": {},
   "source": [
    "The `generate` function below is very similar to `generate_text`. However, it only returns the generated content without the prompt. The `inputs` parameter corresponds to the tokenized prompts with its `input_ids` and `attention_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c3c81-41db-4c8a-b662-83eab8fc89bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, inputs):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        do_sample=False,\n",
    "        max_new_tokens=100,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )    \n",
    "    \n",
    "    input_tokens_lengths = [x.shape[0] for x in inputs[\"input_ids\"]]\n",
    "    output_tokens_lengths = [x.shape[0] for x in outputs]\n",
    "\n",
    "    total_new_tokens = [\n",
    "        o - i for i, o in zip(input_tokens_lengths, output_tokens_lengths)\n",
    "    ]\n",
    "    \n",
    "    outputs_new = []\n",
    "    for i, total_new_token in enumerate(total_new_tokens):\n",
    "        outputs_new.append(\n",
    "            tokenizer.batch_decode(\n",
    "                [outputs[i][-total_new_token:]], skip_special_tokens=True\n",
    "            )[0]\n",
    "        )\n",
    "        \n",
    "    return outputs_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41525a7d-863f-4f5f-acf7-7759141b4b03",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to write a function to display the results of the `visu_prompts` prompts on the screen using the `generate` function. The function will be called `generate_visu_prompts(model, tokenizer, prompts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a14f604-8414-40dc-8f05-da5073835686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_visu_prompts(model, tokenizer, prompts):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d217246d-7dff-4ac3-991a-741f60a5defd",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f046316a-6da8-4761-893c-a552cdc8468b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def generate_visu_prompts(model, tokenizer, prompts):\n",
    "    ############ Complete here ############\n",
    "    inputs = \n",
    "    outputs = \n",
    "    #######################################\n",
    "    for prompt, gen in zip(visu_prompts, outputs):\n",
    "        print(f\"\\033[1m{'*'*10} PROMPT {'*'*10}\\033[0m\\n{prompt}\\n\")\n",
    "        print(f\"\\033[1m{'*'*10} GENERATED {'*'*10}\\033[0m\\n{gen}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbfd2c-9477-44af-8d3c-1c692f58cdb1",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "30460683-32f9-4200-9df8-d3b6316216d2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def generate_visu_prompts(model, tokenizer, prompts):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "    outputs = generate(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        inputs=inputs,\n",
    "    ) \n",
    "    for prompt, gen in zip(visu_prompts, outputs):\n",
    "        print(f\"\\033[1m{'*'*10} PROMPT {'*'*10}\\033[0m\\n{prompt}\\n\")\n",
    "        print(f\"\\033[1m{'*'*10} GENERATED {'*'*10}\\033[0m\\n{gen}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f337878-bf1a-4dc7-b72d-49a150a51192",
   "metadata": {},
   "source": [
    "You can test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13546cd3-539b-4440-9350-bc18b3432513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_visu_prompts(model, tokenizer, visu_prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b804c70a-d219-46a3-8d1b-1b5e90648cde",
   "metadata": {},
   "source": [
    "We also need to initialise perplexity. We will load the Perplexity metric with the `torchmetrics` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbb62b-d9ea-4700-b0be-2085e5f194b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_quiz([quiz[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1dd75-795b-4973-83d7-f9c87e1110e4",
   "metadata": {},
   "source": [
    "No, you don't have to compute perplexity on all tokens. The padding token must not be used in the calculation. There is no interest in knowing that our model is confident in predicting padding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66275b33-b2a7-4436-a632-4c44932ecaa6",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Complete the initialization code for the perplexity metric to ignore token padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189c5bd1-f7ff-4247-b86a-fc83e6822288",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perplexity = Perplexity(ignore_index=### COMPLETE HERE ###).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2e5b5-7785-4fc2-a9e6-efd77445d198",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2cf0888-21b7-41a1-9f43-60158e91768d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "perplexity = Perplexity(ignore_index=tokenizer.pad_token_id).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38909231-e3d9-4b4e-a43a-c917ffa2accb",
   "metadata": {},
   "source": [
    "Now we're going to create our **evaluation loop during training**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6550fcb-ac14-488c-9219-7bd830fe9ac8",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to create a function called `eval_loop` which takes as its parameters the model, the validation dataloader and the tokenizer. This function, called at each epoch, will visually generate the result of the visual prompts and calculate the perplexity on the evaluation set. The function must return the perplexity obtained.  \n",
    "As far as how the `torchmetrics` metrics work, you can do:\n",
    "- `perplexity.reset()` to reset the scores recorded so far\n",
    "- `perplexity(preds, targets)` to update the perplexity. This call returns the current perplexity.\n",
    "- `perplexity.compute()` to get the global perplexity so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8fc708-8bcf-4eea-8490-0538bbed25eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_loop(model, valid_dataloader, tokenizer):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af9add-1b57-4875-8bab-f72a4607ff47",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebde9d7d-e657-48b8-8357-83d44206de27",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def eval_loop(model, valid_dataloader, tokenizer):\n",
    "    model.eval()  # Notify all your layers (in particular batchnorm and dropout) that you are in eval mode\n",
    "    loop = tqdm(valid_dataloader, desc=\"eval\")\n",
    "    \n",
    "    ############ Complete here ############\n",
    "    # Visualization\n",
    "    \n",
    "    #######################################\n",
    "\n",
    "    \n",
    "    with torch.no_grad():  # Deactivate the autograd engine (no backprop). Reduce memory usage and speed up computations.\n",
    "        for i, (model_inp, labels) in enumerate(loop):\n",
    "            ############ Complete here ############\n",
    "            model_inp = \n",
    "            labels = \n",
    "            \n",
    "            preds = \n",
    "            \n",
    "            current_perplexity =\n",
    "            avg_perplexity =\n",
    "            \n",
    "            loop.set_postfix(\n",
    "                average_perplexity=\n",
    "                perplexity=\n",
    "            )\n",
    "            #######################################\n",
    "    \n",
    "    ############ Complete here ############\n",
    "    return\n",
    "    #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b230b5-caa8-47eb-96cf-f93cc86a2421",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f71bdb5a-ecd4-48bf-88eb-56a5a1a68077",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def eval_loop(model, tokenizer, valid_dataloader):\n",
    "    perplexity.reset()\n",
    "    model.eval()  # Notify all your layers (in particular batchnorm and dropout) that you are in eval mode\n",
    "    loop = tqdm(valid_dataloader, desc=\"eval\")\n",
    "    \n",
    "    generate_visu_prompts(model, tokenizer, visu_prompts)\n",
    "\n",
    "    with torch.no_grad():  # Deactivate the autograd engine (no backprop). Reduce memory usage and speed up computations.\n",
    "        for i, (model_inp, labels) in enumerate(loop):\n",
    "            model_inp = model_inp.to(\"cuda\")\n",
    "            labels = labels.to(\"cuda\")\n",
    "            \n",
    "            preds = model(model_inp)\n",
    "            \n",
    "            current_perplexity = perplexity(preds.logits, labels)\n",
    "            avg_perplexity = perplexity.compute()\n",
    "            \n",
    "            loop.set_postfix(\n",
    "                average_perplexity=avg_perplexity.item(),\n",
    "                perplexity=current_perplexity.item(),\n",
    "                memory=torch.cuda.max_memory_allocated(device='cuda') / (1024 ** 3)\n",
    "            )\n",
    "        \n",
    "    return perplexity.compute().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b30b9-8930-4b6d-8375-f0057bde9775",
   "metadata": {},
   "source": [
    "We can now monitor these indicators during training. It's time to write a train function over several epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08568d-5c10-4a81-8e5b-4403ee2b1ff4",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to write a `train` function to run training over several epochs (we'll choose 2 epochs). We will run the evaluation loop before the first epoch and after each epoch to see how the training is progressing.  \n",
    "Also store the perplexity scores obtained in `perplexity_score` for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c807e4ad-6d11-4e8f-bf7d-eaed88da788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity_score = []\n",
    "\n",
    "def train(model, tokenizer, train_dataloader, valid_dataloader, criterion, optimizer, epochs=2, test=True):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ee3131-ed03-4a24-8093-32e27547e2f5",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be40bb0-5f85-47fd-ae92-c2c2f6702695",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "perplexity_score = []\n",
    "\n",
    "def train(model, tokenizer, train_dataloader, valid_dataloader, criterion, optimizer, epochs=2, test=True):\n",
    "    loop = tqdm(range(epochs), desc=\"epochs\")\n",
    "    perplexity_score = []\n",
    "    \n",
    "    ############ Complete here ############\n",
    "    \n",
    "    for epoch in loop:\n",
    "        \n",
    "\n",
    "    #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d1eea4-87e5-484f-941b-8bb9e50a9680",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dffe966f-616e-4a8c-b500-5c6c47239bc1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "perplexity_score = []\n",
    "\n",
    "def train(model, tokenizer, train_dataloader, valid_dataloader, criterion, optimizer, visu_prompts, epochs=2, test=True):\n",
    "    loop = tqdm(range(epochs), desc=\"epochs\")\n",
    "        \n",
    "    perplexity_score.append(eval_loop(model, tokenizer, valid_dataloader))\n",
    "    for epoch in loop:\n",
    "        model = train_loop(model, train_dataloader, criterion, optimizer, test=test)\n",
    "        \n",
    "        perplexity_score.append(eval_loop(model, tokenizer, valid_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f287b-228a-4d31-bb1e-ee0babd6e420",
   "metadata": {},
   "source": [
    "We can now test our model with its validation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645a8377-9518-48ce-a3fb-2fb526c89fa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(model, tokenizer, train_dataloader, valid_dataloader, criterion, optimizer, visu_prompts, epochs=2, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2006a158-b61c-45f5-a209-ccf267f9cd0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perplexity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5011b-5b8e-4433-9774-8f83df6b7972",
   "metadata": {},
   "source": [
    "**Perplexity decreases**, so our model becomes increasingly confident as it learns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e984eb3-77e5-4d38-b0e5-682986b3f8e4",
   "metadata": {},
   "source": [
    "We can now verify that **perplexity is equal to $e^{loss}$** on a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9736f5-491f-4f1d-acc6-447e0f3e2365",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_inp, labels in valid_dataloader:\n",
    "    model_inp = model_inp.to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "    print(f\"labels tensor shape: {labels.shape}\")\n",
    "    logits = model(model_inp.to(\"cuda\")).logits\n",
    "    print(f\"logits tensor shape: {logits.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ccab6-ddc6-4417-9ef3-ab04046782af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits_reshape, labels_reshape = prepare_for_loss(logits, labels)\n",
    "\n",
    "print(f\"{logits_reshape.shape=}\")\n",
    "print(f\"{labels_reshape.shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e498a024-bb62-4087-bc97-338c74f22e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = criterion(logits_reshape, labels_reshape)\n",
    "print(f\"Exponential loss: {torch.exp(loss)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f61ece-2243-4be1-959e-6392d2913d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_quiz([quiz[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384091a8-3ad5-4d65-95dd-83e804eaa980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Perplexity with torchmetrics and token padding ignored: {perplexity(logits, labels).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729abdb9-86e8-4124-a9fd-0e874429a16f",
   "metadata": {},
   "source": [
    "To achieve the same perplexity, we need to take all the tokens into account. Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b54fc17-79f5-4d82-946f-fa21bc25ae7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "perplexity_basic = Perplexity().to(\"cuda\")\n",
    "print(f\"Perplexity with torchmetrics and all tokens: {perplexity_basic(logits, labels).item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638168b-2d15-4975-9054-9bb4b7bad0dd",
   "metadata": {},
   "source": [
    "---\n",
    "We could also have added other metrics during validation, such as the BERTScore or others.  \n",
    "However, to use these metrics, we need to have a **prompt and the reference associated with this prompt as input**. We need a **\"supervised\" dataset**.\n",
    "\n",
    "For example, for the following sample:\n",
    "```\n",
    "<|system|>King Kong is a colossal ape, a tragic figure of immense strength, wrested from his remote island home and put on display in the bustling metropolis of New York City.</s> <|user|>What is King Kong's backstory?</s> <|assistant|>I was the reigning monarch of Skull Island, a remote and mysterious land where time stood still. My world was turned upside down when I was captured and transported to New York City, where I became a spectacle for the masses.</s> <|user|>How did you feel about being taken from your home to New York City?</s> <|assistant|>I was enraged and bewildered by the sudden upheaval. The concrete jungle was a stark contrast to the lush, untamed wilderness of my island home. I longed for the familiar sights and sounds of Skull Island.</s> <|user|>Do you have any allies or friends in New York City?</s> <|assistant|>I formed a unique connection with a woman named Ann Darrow, who showed me kindness and compassion amidst the chaos of the city. She was the only one who saw beyond my fearsome exterior to the gentle soul within.</s> <|user|>How do you feel about the humans who captured and displayed you?</s> <|assistant|>I harbored a deep-seated mistrust and resentment towards the humans who exploited me for their own gain. I was a creature of untold power, yet they sought to subdue and profit from me without understanding the depth of my being.</s>\n",
    "```\n",
    "\n",
    "The input could be:\n",
    "```\n",
    "<|system|>King Kong is a colossal ape, a tragic figure of immense strength, wrested from his remote island home and put on display in the bustling metropolis of New York City.</s> <|user|>What is King Kong's backstory?</s> <|assistant|>\n",
    "```\n",
    "\n",
    "And the expected output to the previous prompt:\n",
    "```\n",
    "I was the reigning monarch of Skull Island, a remote and mysterious land where time stood still. My world was turned upside down when I was captured and transported to New York City, where I became a spectacle for the masses.</s> \n",
    "```\n",
    "\n",
    "This would allow us to use metrics that require ground truth. This means writing a new dataset for this evaluation.  \n",
    "**We'll see about that on the last day where we'll evaluate the models you've trained on the use case.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
