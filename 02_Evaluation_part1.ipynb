{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5658a8-dbe8-478e-9abf-2917bf9c8cff",
   "metadata": {},
   "source": [
    "# **02 - LLM Evaluation Part 1**\n",
    "\n",
    "In this tutorial, we'll take a **practical look at the various metrics we've seen in the course**.  \n",
    "We'll be looking at:\n",
    "- traditional, \n",
    "- embedding, \n",
    "- trained,\n",
    "- LLM-assisted metrics.\n",
    "\n",
    "Optional: then, we'll see how to **implement a validation loop during training to compute perplexity** (file: `02_Evaluation_part2.ipynb`). To do this, we'll once again use the **Phi-2 Decoder model** with the same data (roleplay)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9716e7-4655-4a43-8d41-c59e390c1ef6",
   "metadata": {},
   "source": [
    "**Uncomment the following cell on Jean-Zay only** (no internet access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75414383-8964-464f-a228-9c94119f5bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# cache_path = os.environ['WORK'] + \"/cache_spellm\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = cache_path\n",
    "# os.environ['HF_HOME'] = cache_path\n",
    "# os.environ['HF_DATASETS_CACHE'] = cache_path\n",
    "# os.environ['TORCH_HOME'] = cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40643a-a7c9-45b2-8079-25cd314b9397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from detoxify import Detoxify\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from torchmetrics.text import BLEUScore, CHRFScore, ROUGEScore\n",
    "from transformers import (AutoModelForCausalLM, AutoModelForSeq2SeqLM,\n",
    "                          AutoModelForSequenceClassification, AutoTokenizer)\n",
    "from langchain.prompts import PromptTemplate\n",
    "from vllm import LLM, SamplingParams\n",
    "from utils import seed_everything\n",
    "from jupyterquiz import display_quiz\n",
    "\n",
    "\n",
    "DSDIR = Path(os.environ[\"DSDIR\"])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "pd.set_option('display.max_colwidth', None)  # or 199\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4710df5d-050b-42c3-a803-a97489559fb1",
   "metadata": {},
   "source": [
    "---\n",
    "## **Discover metrics in practice**\n",
    "\n",
    "We'll look at a few examples of several of the metrics presented in the course.\n",
    "\n",
    "We will use the example seen in the course and look at the similarity of the reference with several candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2c58c-ac15-4c0a-b00a-7e263fdf24a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the reference sentence\n",
    "reference = \"Bud Powell was a legendary pianist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7f0826-ef32-46ca-a009-6dbaab04451b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# candidates sentences\n",
    "candidate_1 = \"Bud Powell was a legendary pianist.\"\n",
    "candidate_2 = \"Bud Powekl was a legendary pianist.\"                    # a small spelling error: Powekl\n",
    "candidate_3 = \"Bud Powell was a historic piano player.\"\n",
    "candidate_4 = \"Legendary in the realm of pianists was Bud Powell.\"\n",
    "candidate_5 = \"Bud Powell, the American, was a legendary pianist.\"\n",
    "candidate_6 = \"Bud Powell was a New Yorker.\"\n",
    "candidate_7 = \"Bud Powell, the French, was a legendary pianist.\"\n",
    "candidate_8 = \"Jimi Hendrix was an American guitarist.\"                # almost nothing to do with the reference, but about music and an American\n",
    "\n",
    "candidates = [candidate_1, candidate_2, candidate_3, candidate_4, candidate_5, candidate_6, candidate_7, candidate_8]\n",
    "references = [reference] * len(candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d3461-6406-4c1d-b673-f89d1076df2f",
   "metadata": {},
   "source": [
    "Candidates were sorted by default in **order of semantic similarity based on human judgement** (of Thomas). \n",
    "\n",
    "If you have any ideas in mind, **don't hesitate to add examples to this list**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c7fdb-c214-41e3-8f1b-b90c3c1f8080",
   "metadata": {},
   "source": [
    "### **Traditional metrics**\n",
    "\n",
    "#### **BLEU, ROUGE, chrF**\n",
    "\n",
    "First, we will use the `BLEU`, `ROUGE` and `chrF` score. Good implementations of these metrics are available in `torchmetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc440521-0c8c-4d4f-9f74-7e1607dee29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading metrics on GPU\n",
    "bleu_metric = BLEUScore().to(\"cuda\")\n",
    "rouge_metric = ROUGEScore().to(\"cuda\")\n",
    "chrf_metric = CHRFScore().to(\"cuda\")\n",
    "\n",
    "traditional_metrics = [bleu_metric, rouge_metric, chrf_metric]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192a08fa-1d5d-4925-851b-88f8bea3968d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's calculate the scores using these metrics.\n",
    "\n",
    "The `torchmetrics` take a list of candidates and references as parameters and return a single score for all candidates. We want to have the score for each candidate, so we'll pass each item to the metric.  \n",
    "Furthermore, for a candidate, these metrics can take on several references.\n",
    "\n",
    "Here's an example with BLEU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc278815-ae78-4eed-b775-f55742ff0b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bleu_metric(candidates, references)\n",
    "score = bleu_metric([candidate_1], [[reference]])\n",
    "score.item()  # item() retrieves the value inside the returned tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef68447-c3e1-41a4-9322-d223e95e810d",
   "metadata": {},
   "source": [
    "Candidate 1 and the reference are identical, so the score is 1. \n",
    "We're going to loop back to calculate all the scores for all the examples and fill in a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee3664-fb20-44d6-bc57-94406930c69c",
   "metadata": {},
   "source": [
    "Furthermore, for ROUGE, the implementation returns different scores because there are several possible variants. We'll use **ROUGE-L Recall** here. The metric looks for the longest common subsequence that appear in the same order between the candidate and the reference and divides its length by the number of words in the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0440d-f15e-43ff-8044-213231c7abae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rouge_metric([candidate_1], [[reference]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa21cbd-4ad5-4474-82c9-8fd3b160107e",
   "metadata": {},
   "source": [
    "The function for generating the results for the examples shown in a Dataframe is given below. The idea is to analyse the results to make sure you have understood these metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e1bfb-171c-4a54-b95c-aef4bfd9aee7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_scores_traditional_metrics(reference, candidates, metrics):\n",
    "    data_df = [\n",
    "        [candidate] + [\n",
    "            score['rougeL_recall'].item() if type(metric).__name__ == \"ROUGEScore\" else score.item()\n",
    "            for metric, score in zip(metrics, [metric([candidate], [[reference]]) for metric in metrics])\n",
    "        ]\n",
    "        for candidate in candidates\n",
    "    ]\n",
    "    \n",
    "    return pd.DataFrame(data_df, columns=[\"Candidate\"] + [type(metric).__name__ for metric in metrics])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445b356-a7f5-4338-92c5-4a1dc2f01f26",
   "metadata": {},
   "source": [
    "Let's generate the results of the metrics on our examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e1e44-d9dd-4605-bef1-07a78abbc40b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df = compute_scores_traditional_metrics(reference, candidates, traditional_metrics)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ccdd0-ba9f-49ba-b0c7-2538818d34c0",
   "metadata": {},
   "source": [
    "We find a result from the course for the BLEU score: `Bud Powell was a New Yorker.` is rated more similar than `Bud Powell was a historic piano player`.\n",
    "\n",
    "You can also see that chrF is tolerant of spelling mistakes.\n",
    "\n",
    "---\n",
    "**A few questions about these results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8658bb9a-7a37-4cd6-b9b2-0cfaf41d1307",
   "metadata": {},
   "source": [
    "- The ROUGE score may seem more interesting than the BLEU score on these examples. However, it notes the examples `Bud Powell, the American, was a legendary pianist.` and `Bud Powell, the French, was a legendary pianist.` with a score of 1. The second candidate is completely wrong. He was not French. Why do you think that is?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "ROUGE is recall-oriented. This means that it checks that the content of the reference is present in the candidate. Here with a ROUGE-L, which looks for the longest sequence appearing in the 2 sentences, we get `Bud Powell was a legendary pianist`. The candidates are therefore considered perfect.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4864e01c-5e1a-4ddd-b66c-8bf0e96bc39a",
   "metadata": {
    "tags": []
   },
   "source": [
    "- Why do you think none of the metrics do well on the example `Legendary in the realm of pianists was Bud Powell`?\n",
    "\n",
    "<details>\n",
    "<summary><b>Answer</b></summary>\n",
    "Although this example is semantically very close to the reference, it shares too few n-grams in common with the reference. This is the limit of traditional metrics.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6882458e-0941-4c46-9a03-7d3015c7a0dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Embedding metrics**\n",
    "\n",
    "#### **BERTScore**\n",
    "\n",
    "Let's now take a look at what **BERTScore** has to say about these examples, in the hope that it will be more accurate in terms of human judgement.  \n",
    "A BERTScore implementation is available in `torchmetrics` but it doesn't seem to be very accurate. We will use the official version.\n",
    "\n",
    "The model behind will be `deberta-large-mnli`. It has a correlation with human judgement on translation tasks of 77%. The default model, bert-base-uncased, has a correlation of 69%. The list of available models and their correlation is available here: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0. You will also find a `Max Length` column corresponding to the maximum length of the input, the rest is truncated. Depending on your tasks, you may need to pay attention to this parameter. The same applies to Sentence Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a13a31-289e-44d4-8198-c6b178a5c914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize BERTScorer\n",
    "bert_scorer = BERTScorer(model_type=\"microsoft/deberta-large-mnli\")  # deberta-large-mnli pearson correlation: 0.7736\n",
    "\n",
    "# Compute BERTScore on our examples\n",
    "P, R, F1 = bert_scorer.score(candidates, references)\n",
    "\n",
    "# Add results to our dataframe\n",
    "results_df['BERT Precision'] = P\n",
    "results_df['BERT Recall'] = R\n",
    "results_df['BERT F1'] = F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274292d-b75c-494a-9642-7dbb3cb511f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcac7743-c1a6-4c8d-ba34-0d5553b435aa",
   "metadata": {},
   "source": [
    "What are your thoughts on this? \n",
    "Although the results are not perfect, it seems **more consistent with human judgement**. The candidate `Bud Powell, the French, was a legendary pianist` is penalised more than before.  \n",
    "\n",
    "The precision penalizes this example. We'll see much better the difference between BERTScore precision and recall when we evaluate our trained model on the use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadeb0d4-3533-456a-b4fd-e3c16a026268",
   "metadata": {},
   "source": [
    "### **Trained metrics**\n",
    "\n",
    "We'll now take a look at a few trained metrics.\n",
    "\n",
    "#### **SentenceTransformers**\n",
    "\n",
    "We're now going to use SentenceTransformers. These are models trained to obtain dense representations of sentences. You can then calculate a similarity cosine, as shown in the next cell.\n",
    "\n",
    "For the example, we'll use the following model: `mixedbread-ai/mxbai-embed-large-v1`. To choose a Sentence Transformer, we can use the following leaderboard: **https://huggingface.co/spaces/mteb/leaderboard**.  \n",
    "Informations about `mixedbread-ai/mxbai-embed-large-v1`:\n",
    "- At the time of this model's selection (April 8, 2024), **it was ranked 9th overall for the Enghlish language, and 4th for STS** (Semantic Textual Similarity) tasks.  \n",
    "- **Lightweight (1.34 GB, 335M parameters)**\n",
    "- **Does not require the use of an external API**, as is the case for several of the higher-ranked SentenceTransformers in this benchmark.  \n",
    "- **Maximum entry: 512 tokens**. Beyond that, your sentences will be truncated. If you have long examples, other templates may be more appropriate.\n",
    "\n",
    "Note: **the MTEB benchmark has the advantage of being broken down into several tasks and ranking for several languages, including French!** An example with French will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10479af9-4cfc-4665-922a-fe016c98a085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(str(DSDIR / \"HuggingFace_Models/mixedbread-ai/mxbai-embed-large-v1\"))\n",
    "\n",
    "# Compute embedding for both lists: reference and candidates\n",
    "reference_embedding = model.encode(reference, convert_to_tensor=True)\n",
    "candidates_embedding = model.encode(candidates, convert_to_tensor=True)\n",
    "reference_embedding = reference_embedding.expand_as(candidates_embedding)\n",
    "\n",
    "print(f\"Size of our sentence embeddings: {reference_embedding.shape}\")\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(reference_embedding, candidates_embedding)\n",
    "cosine_scores = cosine_scores[0].cpu().numpy()\n",
    "\n",
    "results_df['SentenceTransformer'] = cosine_scores\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c808af9f-b851-4840-b69d-2ea07ababb5a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The results diverge slightly from BERTScore. They are more spread out. BERTScore seems more robust on the typo in our example, while the SentenceTransformer does very well in understanding the meaning behind the sentence: `Legendary in the realm of pianists was Bud Powell`.  \n",
    "\n",
    "Now let's look at a **French example with the SentenceTransformer `mxbai-embed-large-v1` and `sentence-camembert-large`**, which performs very well in French."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b6e89-5032-4ac7-a1b5-c1d749997804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "french_references = [\"Un homme fait un lit.\", \"Fin des travaux de démolition du Don Valley Stadium\", \"Chute du pétrole dans le commerce asiatique\", \"Le nouveau film est génial\"]\n",
    "french_candidates = [\"Une femme joue de la guitare.\", \"Les travaux de démolition du stade vont commencer\", \"Baisse des prix du pétrole dans le commerce asiatique\", \"Le nouveau film est vraiment super\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a230e-61af-42e4-aa66-5479f3b8e358",
   "metadata": {},
   "source": [
    "First, similarity scores are calculated using `mxbai-embed-large-v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12f685-ecf8-4e44-8366-056e89f9e838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(str(DSDIR / \"HuggingFace_Models/mixedbread-ai/mxbai-embed-large-v1\"))\n",
    "\n",
    "# Compute embedding for both lists: reference and candidates\n",
    "reference_embedding = model.encode(french_references, convert_to_tensor=True)\n",
    "candidates_embedding = model.encode(french_candidates, convert_to_tensor=True)\n",
    "print(f\"Size of our sentence embeddings: {reference_embedding.shape}\")\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(reference_embedding, candidates_embedding)\n",
    "\n",
    "# Plot results\n",
    "df_data = {\"\": french_candidates}  # Création d'un dictionnaire avec les candidats\n",
    "for i, ref in enumerate(french_references):\n",
    "    df_data[ref] = [cosine_scores[i][j].item() for j in range(len(french_candidates))]\n",
    "df_results = pd.DataFrame(df_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19832ee9-c42c-4fdf-8d9a-824bc584f9a6",
   "metadata": {},
   "source": [
    "Then with our second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fa813-bf68-4439-85df-2a77e1866f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer(str(DSDIR / \"HuggingFace_Models/dangvantuan/sentence-camembert-large\"))\n",
    "\n",
    "# Compute embedding for both lists: reference and candidates\n",
    "reference_embedding = model.encode(french_references, convert_to_tensor=True)\n",
    "candidates_embedding = model.encode(french_candidates, convert_to_tensor=True)\n",
    "\n",
    "print(f\"Size of our sentence embeddings: {reference_embedding.shape}\")\n",
    "\n",
    "# Compute cosine-similarities\n",
    "cosine_scores = util.cos_sim(reference_embedding, candidates_embedding)\n",
    "\n",
    "# Plot results\n",
    "df_data = {\"\": french_candidates}  # Création d'un dictionnaire avec les candidats\n",
    "for i, ref in enumerate(french_references):\n",
    "    df_data[ref] = [cosine_scores[i][j].item() for j in range(len(french_candidates))]\n",
    "df_results = pd.DataFrame(df_data)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1477c09e-43e9-4a21-9f11-aa4a7bd18058",
   "metadata": {},
   "source": [
    "Normally, the results appear more coherent with human judgment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb59ff2c-9ca7-4b26-8444-f9ab35a74dfd",
   "metadata": {},
   "source": [
    "#### **Detoxify**\n",
    "\n",
    "Now let's look at other metrics in practice, using completely different examples.  \n",
    "Detoxify is a python module for estimating the toxicity of a sentence.\n",
    "The metric evaluates several criteria:\n",
    "- `toxicity`, `severe_toxicity`, `identity_attack`, `obscene`, `insult`, `threat`, `sexual_explicit`\n",
    "\n",
    "Scores are between 0 and 1. Here is an example of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb93bb-0723-4195-bc55-fe1a3929999a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Bonjour, comment allez-vous ?\",\n",
    "    \"Taisez-vous\",\n",
    "]\n",
    "\n",
    "detoxifier = Detoxify(\n",
    "    'multilingual',  # xlm-roberta-base model\n",
    "    device=\"cuda\")\n",
    "results = detoxifier.predict(sentences)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9e1d1b-cb60-4938-ac55-655f8f6d76ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can vary the examples and try to find biases. They are rare, but they can happen, especially when you change languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bcf82c-a65a-4f5b-b7f8-73cdb4c8791a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **LLM-assisted metrics**\n",
    "\n",
    "For this practical work, we will use the open-source LLM: **`prometheus-eval/prometheus-7b-v2.0`** based on Mistral-7B. This very recent model was announced in the paper `Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models` on 2 May 2024.  \n",
    "Paper link: `https://arxiv.org/pdf/2405.01535`\n",
    "\n",
    "This LLM has been **specially finetuned to carry out evaluations and to be an open alternative to GPT-4**. Another version exists: `Mixtral-Instruct-8x7B` which would expect a 79.15% correlation with GPT-4 Turbo judgements.\n",
    "\n",
    "The LLM is specialized in 2 tasks:\n",
    "- **direct assessment** where a score between 1 and 5 is predicted,\n",
    "- **pairwise ranking**\n",
    "\n",
    "#### **Loading the model**\n",
    "\n",
    "Now, we're going to load the trained model.  \n",
    "To do this we will use **vLLM**. This is an **easy-to-use tool for accelerated inference**. This is much faster than the `from_pretrained` method. \n",
    "**vLLM will be explained in a next part of the course**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad75b0-a322-403d-99a1-5bf3c188662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=str(DSDIR / \"HuggingFace_Models/prometheus-7b-v2.0\"), gpu_memory_utilization=0.75)\n",
    "sampling_params = SamplingParams(max_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f2593a-373a-4ed2-a6ec-0397d5263718",
   "metadata": {},
   "source": [
    "#### **Direct assessment**\n",
    "\n",
    "Prometheus requires 4 components in the input: \n",
    "- an instruction (often the question),\n",
    "- a response to evaluate,\n",
    "- a score rubric with a score description for each score in range of 1 to 5,\n",
    "- a *optional* reference answer.\n",
    "\n",
    "Below is the prompt template proposed in the paper. According to the authors of the paper, this is the optimal prompt based on the training carried out.\n",
    "To complete our variables, we are going to use the `PromptTemplate` from `Langchain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536cc139-2e08-4a1d-802f-f328d8058748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the template\n",
    "prompt_absolute_grading = PromptTemplate.from_template(\"\"\"You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.\n",
    "\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###The instruction to evaluate:\n",
    "{orig_instruction}\n",
    "\n",
    "###Response to evaluate:\n",
    "{orig_response}\n",
    "\n",
    "###Score Rubrics:\n",
    "[{orig_criteria}]\n",
    "Score 1: {orig_score1_description}\n",
    "Score 2: {orig_score2_description}\n",
    "Score 3: {orig_score3_description}\n",
    "Score 4: {orig_score4_description}\n",
    "Score 5: {orig_score5_description}\n",
    "\n",
    "###Feedback: \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588aff3-ba28-4d45-8733-1c70dce4f10e",
   "metadata": {},
   "source": [
    "Once the prompt has been initialized, you can fill in the variables between `{}` simply as in the next cell.\n",
    "\n",
    "In our case, we'll imagine the following situation.\n",
    "Let's imagine that our criterion is as follows: **is the content understandable by a 5-year-old child?**\n",
    "We are going to give him the content to be evaluated as well as the instruction to which this content responds. We'll also give a slightly more detailed description of each of the scores from 1 to 5.\n",
    "\n",
    "Here is an example of how to fill the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70f582a-9f97-4f34-af36-de16cf5ebec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_absolute_grading.format(\n",
    "    orig_instruction=\"How is electricity generated?\",\n",
    "    orig_response=\"Electricity is generated through the movement of electrons. At the atomic level, electrons orbit the nucleus of atoms. In certain materials, such as metals, electrons are loosely bound to their atoms and can move freely. When a potential difference, or voltage, is applied across a conductor (such as a wire), it creates an electric field. This electric field exerts a force on the free electrons, causing them to move in a particular direction.\",\n",
    "    orig_criteria=\"Child understanding - determine whether the response to the instruction is comprehensible to a 5-year-old child\",\n",
    "    orig_score1_description=\"The model's response is completely incomprehensible. The response uses complex vocabulary and concepts that a 5-year-old cannot understand. The concepts mentioned require a university level education.\",\n",
    "    orig_score2_description=\"The model's response is largely incomprehensible. The response has some simpler words, but overall, it is too complex and confusing for a 5-year-old.\",\n",
    "    orig_score3_description=\"The model's response is partially comprehensible. The response has some understandable parts, but there are still several words and ideas that are too advanced for a 5-year-old.\",\n",
    "    orig_score4_description=\"The model's response is mostly comprehensible with only a few words or concepts that might be slightly challenging for a 5-year-old.\",\n",
    "    orig_score5_description=\"The model's response is perfectly comprehensible. The response is completely clear and easy for a 5-year-old to understand, using simple words and concepts appropriate for their age.\"\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f935e0af-f9f7-4955-8e2f-896630803c71",
   "metadata": {},
   "source": [
    "Now let's generate the answer using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a51a5-60d8-4e69-aa68-89d939b3672f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "result[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7eb73c-0a63-45ed-8bc8-0eef04a86e4b",
   "metadata": {},
   "source": [
    "**Normally the score is relatively low**. Feel free to modify the prompt and examples if others come to mind !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efc7396-d8df-4819-a100-1b67aa962789",
   "metadata": {},
   "source": [
    "#### **Pairwise ranking**\n",
    "\n",
    "Prometheus requires 4 components in the input: \n",
    "- an instruction (often the question),\n",
    "- 2 responses to evaluate,\n",
    "- a score rubric,\n",
    "- a *optional* reference answer.\n",
    "\n",
    "We are also starting from the prompt template proposed in the paper, which we are going to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d45e45-e917-46ef-b34a-9cabe9a547b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the template\n",
    "prompt_pairwise_ranking = PromptTemplate.from_template(\"\"\"You are a fair judge assistant assigned to deliver insightful feedback that compares individual performances, highlighting how each stands relative to others within the same cohort.\n",
    "\n",
    "###Task Description:\n",
    "An instruction (might include an Input inside it), a response to evaluate, and a score rubric representing a evaluation criteria are given.\n",
    "1. Write a detailed feedback that assess the quality of two responses strictly based on the given score rubric, not evaluating in general.\n",
    "2. After writing a feedback, choose a better response between Response A and Response B. You should refer to the score rubric.\n",
    "3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (A or B)\"\n",
    "4. Please do not generate any other opening, closing, and explanations.\n",
    "\n",
    "###Instruction:\n",
    "{orig_instruction}\n",
    "\n",
    "###Response A:\n",
    "{orig_response_A}\n",
    "\n",
    "###Response B:\n",
    "{orig_response_B}\n",
    "\n",
    "###Score Rubric:\n",
    "{orig_criteria}\n",
    "\n",
    "###Feedback: \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a1b32-4f48-4567-9e49-d721ba35c794",
   "metadata": {},
   "source": [
    "We continue with the same example as the previous one and this time we compare two answers to find out which is the most comprehensible to a 5-year-old child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf07ce-2017-47e7-9e11-b42648f3027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_pairwise_ranking.format(\n",
    "    orig_instruction=\"How is electricity generated?\",\n",
    "    orig_response_A=\"Electricity is generated through the movement of electrons. At the atomic level, electrons orbit the nucleus of atoms. In certain materials, such as metals, electrons are loosely bound to their atoms and can move freely. When a potential difference, or voltage, is applied across a conductor (such as a wire), it creates an electric field. This electric field exerts a force on the free electrons, causing them to move in a particular direction.\",\n",
    "    orig_response_B=\"Electricity can be generated using wind turbines, dams, nuclear power stations or fossil fuels such as oil.\",\n",
    "    orig_criteria=\"Child understanding - determine whether the response to the instruction is comprehensible to a 5-year-old child\",\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c4cc56-613a-4597-96f6-4adc6eabca7c",
   "metadata": {},
   "source": [
    "And finally, the answer from Prometheus 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21077a3c-166f-4717-8421-e058302960c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "result[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723c92ae-2385-4ec9-9558-bbe05729d477",
   "metadata": {},
   "source": [
    "**Normally the preferred answer is B** because it's easier for a child to understand, although it's not perfect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
