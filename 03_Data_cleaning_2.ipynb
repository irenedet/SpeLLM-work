{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f021f8fa",
   "metadata": {},
   "source": [
    "# Hands-On Practice: Data Cleaning with Data-juicer\n",
    "\n",
    "## Introduction\n",
    "In this hands-on practice session, you will learn how to perform data cleaning using the Data-juicer library in Python. Data cleaning is an essential step in the data preprocessing pipeline, ensuring that datasets are free from errors, inconsistencies, and missing values.\n",
    "\n",
    "## Objectives\n",
    "- Learn how to use the Data-juicer library for efficient data cleaning.\n",
    "- Apply various data cleaning techniques to a real-world dataset.\n",
    "\n",
    "\n",
    "## Setup\n",
    "The dataset is available under the CC0 licence on [kaggle](https://www.kaggle.com/datasets/venky73/spam-mails-dataset).\n",
    "To use it on this hands-on you need to convert it to a jsonl format. <br>\n",
    "Additionally, you need to have the [data-juicer](https://github.com/alibaba/data-juicer), [presidio_analyzer](https://pypi.org/project/presidio-analyzer/), [presidio_anonymizer](https://pypi.org/project/presidio-anonymizer/) and [presidio_evaluator](https://pypi.org/project/presidio-evaluator/) python packages installed. <br>\n",
    "Finnaly, you can request for ganerated PII to use faker [here](https://www.fakenamegenerator.com/order.php).\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff06ff0-f06f-4685-aff4-6b0b1c1c4c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "from visualization import removed_viz, word_dist, char_dist, sample_diff\n",
    "\n",
    "# Presidio analyzer\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from functools import partialmethod\n",
    "\n",
    "# Presidio evaluator\n",
    "from presidio_evaluator.data_generator import PresidioDataGenerator\n",
    "from presidio_evaluator.data_generator.faker_extensions import (\n",
    "    AddressProviderNew,\n",
    "    AgeProvider,\n",
    "    IpAddressProvider,\n",
    "    NationalityProvider,\n",
    "    OrganizationProvider,\n",
    "    PhoneNumberProviderNew,\n",
    "    RecordsFaker,\n",
    "    UsDriverLicenseProvider,\n",
    ")\n",
    "\n",
    "\n",
    "# Jupyterquiz\n",
    "from jupyterquiz import display_quiz\n",
    "\n",
    "with open(\"questions.json\", \"r\") as file:\n",
    "    questions=json.load(file)\n",
    "\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "\n",
    "tqdm.pandas(disable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d746c4-05ac-400f-809e-022764ac1d81",
   "metadata": {},
   "source": [
    "# MinHash deduplication\n",
    "\n",
    "<hr style=\"border: 1px solid red;\">\n",
    "\n",
    "> <span style=\"color:red\">**Task** </span> : \n",
    "The first step is to see how the minhas dedup filter is applied on a real world dataset (papers titles and abstracts) <br>\n",
    "Note that data-juicer is installed and to launch a preproccessing pipeline, the following command is used:<br>\n",
    "`dj-process --config config.yaml`<br>\n",
    "So all the steps are defined in the config.yaml file<br>\n",
    "You need to define only one MinHash dedup filter in the config.yaml file and then run the command above\n",
    "\n",
    "An example of a documented config file is available [here](https://github.com/alibaba/data-juicer/blob/main/configs/config_all.yaml)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4727b0",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5e3a6-8666-4e0a-affb-1f7173229f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = f\"\"\"\n",
    "project_name: \"dedup\"\n",
    "dataset_path: '{os.environ.get('ALL_CCFRWORK')}/data_spellm/data_cleaning/data_dedup.jsonl'  # path to your dataset directory or file with weights(0.0-1.0), 1.0 as default.\n",
    "                                                            # accepted format: 'weight1(optional) dataset1-path weight2(optional) dataset2-path'\n",
    "export_path: ''                # path to processed result dataset. Supported suffixes \n",
    "# Process config example for dataset\n",
    "np : 1        # number of processes to run in parallel (let it be 1 for the TP)\n",
    "\n",
    "# process schedule\n",
    "# a list of several process operators with their arguments\n",
    "process:  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d13dc63",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270058a8-71d7-4cd4-b5c8-6b529873b03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = f\"\"\"\n",
    "project_name: \"dedup\"\n",
    "dataset_path: '{os.environ.get('ALL_CCFRWORK')}/data_spellm/data_cleaning/data_dedup.jsonl'  # path to your dataset directory or file with weights(0.0-1.0), 1.0 as default.\n",
    "                                                            # accepted format: 'weight1(optional) dataset1-path weight2(optional) dataset2-path'\n",
    "export_path: './outputs/dedup/deduplicated.jsonl'                # path to processed result dataset. Supported suffixes \n",
    "# Process config example for dataset\n",
    "np : 1       # number of processes to run in parallel (let it be 1 for the TP)\n",
    "\n",
    "# process schedule\n",
    "# a list of several process operators with their arguments\n",
    "process:\n",
    "    - document_minhash_deduplicator:                          # deduplicate text samples using MinHash-LSH method\n",
    "      tokenization:                                       # tokenization method for text. One of [space, punctuation, character]\n",
    "      window_size:                                        # window size of shingling\n",
    "      num_permutations:                                   # number of permutations in minhash computing\n",
    "      jaccard_threshold:                                  # the min jaccard similarity threshold in near-duplicate detection. When the jaccard similarity of two sample texts is >= this threshold, they are regarded as similar samples and this op will only keep one of them after deduplication\n",
    "      num_bands: None                                     # number of bands in LSH. Default it's None, and it will be determined by an optimal params computation algorithm by minimize the weighted sum of probs of False Positives and False Negatives\n",
    "      num_rows_per_band: None                             # number of rows in each band in LSH. Default it's None, and it will be determined by an optimal params computation algorithm\n",
    "      lowercase:                                          # whether to convert text to lower case\n",
    "      ignore_pattern:   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01b6955",
   "metadata": {},
   "source": [
    "**Solution:**\n",
    "\n",
    "Run the folowing cell if you want to display the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a459306f-c869-4950-b733-54b4f18f8beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/solution_data_cleaning_dedup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd72f0-37a4-4a3e-a765-283658b38dfb",
   "metadata": {},
   "source": [
    "Now we run the deduplication filter on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1740e4-7257-4ac1-bc57-3bb5b96d4895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the config file\n",
    "with open(\"config.yaml\", 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504bca5-16df-45d4-adcd-f3a326e8adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dj-process --config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfdfa86-01fb-4955-b688-7473c20a6692",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid red;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d4303b-4b26-4297-9443-bea9537c85be",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Because this dataset is annotated, you can see the classes of the document removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0924c7e0-5b2e-4c0c-8e2e-981dd9abcd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = Path(f\"{os.environ.get('ALL_CCFRWORK')}/data_spellm/data_cleaning/data_dedup.json\")\n",
    "modified_path = Path('./outputs/dedup/deduplicated.jsonl')\n",
    "removed_viz(original_path,modified_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0068a2d9-750b-4a49-b4e2-a8dca19f9e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz([questions[\"DC7\"]],  border_radius=0, max_width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe269a2-66ab-4a44-bba3-f90a48ad4559",
   "metadata": {},
   "source": [
    "You can find statistiques and logs about the filter process in the output folder\n",
    "<hr style=\"border: 1px solid red;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1a139-a377-463f-9c56-e2f6a46b01ec",
   "metadata": {},
   "source": [
    "## A word on data, dimensions and calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b7c73-eada-4de4-b5ee-106e54534f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz([questions[\"DC6\"]],  border_radius=0, max_width=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b7fb3-b31c-477f-a57e-8b225fe8266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz([questions[\"DC5\"]],  border_radius=0, max_width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb6953-642a-4315-a227-542a649d8d1a",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid red;\">\n",
    "\n",
    "> <span style=\"color:red\">**Task** </span> : \n",
    "Try tunning some of the parameters. Does the numbers of filtered document follow your intuition? \n",
    "\n",
    "You may see some negatives effects, those obervations can have different causes. One of them is a mismatch on the \"dimentions/scales\" of the parameters. \n",
    "\n",
    "To have a good intuition on what scale you could try, a quick look at your data is necessary. Fore instance you could try looking what are the lengths of samples in the datasets : \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05956a28-e072-4292-8016-6147b2347ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Displays an histogram of the number of characters in each sample\n",
    "char_dist(original_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a2e956-db90-4a1f-ac5b-eead0e4ac2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "# Displays an histogram of the number of words in each sample\n",
    "word_dist(original_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a7e2af-10fb-4727-a338-14e45fe499e0",
   "metadata": {},
   "source": [
    "> <span style=\"color:red\">**Task** </span> : \n",
    "After looking at the lengths repartition of messages, change parameters. What do you think are some reasonable values ?\n",
    "\n",
    "<hr style=\"border: 1px solid red;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3702b7f3-a14b-4961-9a07-230660853910",
   "metadata": {},
   "source": [
    "## Text filters\n",
    "\n",
    "A simplified configuration file with only texts filters is available in `config_text.yaml`\n",
    "\n",
    "<hr style=\"border: 1px solid red;\">\n",
    "\n",
    "> <span style=\"color:red\">**Task** </span> : Looking at the available filters, take a moment to choose the ones tha could be useful for this dataset (or the dataset you are going to use for your LLM) and see what changed in the processed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2e62d-8088-476c-bc62-e1b951c629cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "content = f\"\"\"\n",
    "project_name: \"dedup\"\n",
    "dataset_path: '{os.environ.get('ALL_CCFRWORK')}/data_spellm/data_cleaning/data_dedup.jsonl'  # path to your dataset directory or file with weights(0.0-1.0), 1.0 as default.\n",
    "                                                            # accepted format: 'weight1(optional) dataset1-path weight2(optional) dataset2-path'\n",
    "export_path: './outputs/dedup/deduplicated.jsonl'                # path to processed result dataset. Supported suffixes \n",
    "# Process config example for dataset\n",
    "np : 1      # number of processes to run in parallel (let it be 1 for the TP)\n",
    "\n",
    "# process schedule\n",
    "# a list of several process operators with their arguments\n",
    "# TODO: add more process operators to clean the dataset\n",
    "process:  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8036149-30b9-4e39-865f-a2a049a545a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the config file\n",
    "with open(\"config.yaml\", 'w') as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd439101-a0d9-49a3-99cb-247c6f6d0e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dj-process --config config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00727863-90f1-4933-bb19-f6336b2d7231",
   "metadata": {},
   "source": [
    "## What does my filters changed?\n",
    "\n",
    "The next code allows you to see differences between two versions of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbf6014-9397-440f-8c1c-669acd795824",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.read_json(original_path, lines=True)\n",
    "modified = pd.read_json(modified_path, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f7703e-e3a9-4255-9caa-fc24b80e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_diff(original,modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985f232-e0c9-4616-b772-6b77caee40cd",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid red;\">\n",
    "\n",
    "\n",
    "# Data Anonymization with Presidio\n",
    "\n",
    "In this next phase of our hands-on practice on data cleaning, we delve into the critical aspect of data anonymization. Data anonymization is the process of transforming personally identifiable information (PII) within datasets into a form where the individual cannot be identified. This is crucial for ensuring data privacy and compliance with regulations such as GDPR, HIPAA, and CCPA.\n",
    "\n",
    "For this part of the exercise, we will be leveraging the Presidio library, a powerful tool specifically designed for data anonymization tasks. Presidio offers a wide range of anonymization techniques tailored to various types of sensitive information, including names, locations, email addresses, and more.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The primary objective of this task is to anonymize sensitive information within our dataset, ensuring that the data remains useful for analysis while protecting the privacy of individuals represented in the data.\n",
    "\n",
    "## Library presentation \n",
    "\n",
    "There are three presidio libraries : `presidio_analyzer`, `presidio_anonymizer` and `presidio_evaluator`. <br>\n",
    "The first two are useful for anonymization and the last one is used to put fake values in the dataset. <br>\n",
    "The [presidio documentation](https://microsoft.github.io/presidio/anonymizer/) gives some hints and tips.\n",
    "\n",
    "## Anonymization\n",
    "\n",
    "<hr style=\"border: 1px solid red;\">\n",
    "\n",
    "> <span style=\"color:red\">**Task** </span> : Let's create an anonymizer engine and anonimyze the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c3706",
   "metadata": {},
   "source": [
    "**Exercice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541aee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "threshold = 0.6\n",
    "\n",
    "# Load the deduplicated data into a pandas dataframe\n",
    "dataset = pd.read_json('', lines=True)\n",
    "\n",
    "# Initialize the NLP engine\n",
    "nlp_engine = \n",
    "\n",
    "# Pass the engine to the analyzer, the dataset is only in english\n",
    "analyzer = AnalyzerEngine(\n",
    "    nlp_engine=, \n",
    "    supported_languages=\n",
    ")\n",
    "\n",
    "# Analyzer results are passed to the AnonymizerEngine for anonymization\n",
    "anonymizer = \n",
    "\n",
    "\n",
    "\n",
    "# Define a function to anonymize a text\n",
    "def anonymize_text(text: str, threshold: float) -> str:\n",
    "    # Analyze the text\n",
    "        analyzer_results = \n",
    "        # Filter data on threshold\n",
    "        analyzer_results_filt = [\n",
    "            result for result in analyzer_results if result.score > threshold\n",
    "        ]\n",
    "        # Anonymize the text (replace detected PII with the relevant type, e.g. PERSON, EMAIL_ADDRESS, etc.)\n",
    "        ano_text = anonymizer.anonymize(\n",
    "            text=text, analyzer_results=analyzer_results_filt\n",
    "        ).text\n",
    "        return ano_text\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "dataset[\"text\"] = dataset[\"text\"].apply(\n",
    "    lambda x: anonymize_text(x, threshold)\n",
    ")\n",
    "dataset.dropna(subset=[\"text\"],inplace=True)\n",
    "\n",
    "# Save the dataframe to a new file\n",
    "save_path=Path('./outputs/anonymized/anonymized.jsonl')\n",
    "os.makedirs(save_path.parent.as_posix(),exist_ok=True)\n",
    "dataset.to_json(save_path.as_posix(), orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b9cbf",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02572afd-a211-4266-a20e-79ec90631ac4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/solution_data_cleaning_presidio.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fbaa5d-c44e-4d50-a45b-a5f6b6d00703",
   "metadata": {},
   "source": [
    "Look at the new dataset, PII information are now replaced by placeholders. <br>\n",
    "The dataset is anonymized ! Do you see any fails? \n",
    "<hr style=\"border: 1px solid red;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa154838-d657-4549-b20d-6401293fa5e3",
   "metadata": {},
   "source": [
    "## Replacing anonymized values with fakes ones\n",
    "\n",
    "The `presidio_evaluator` librairy relies on [`faker`](https://pypi.org/project/Faker/) to replace removed PII data with fake ones.<br>\n",
    "To do so, a file with fake information is needed. You can ask for one [here](https://www.fakenamegenerator.com/order.php) but you have one available at `./assets/fake_pii.csv`\n",
    "\n",
    "<hr style=\"border: 1px solid red;\">\n",
    "\n",
    "> <span style=\"color:red\">**Task** </span> : Replace all PII values in the dataset with generated ones "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0855ebc9",
   "metadata": {},
   "source": [
    "**Exercice:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285c5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a pandas dataframe\n",
    "dataset = pd.read_json('./outputs/anonymized/anonymized.jsonl', lines=True)\n",
    "\n",
    "# Create a new instance of the PresidioDataGenerator from a generated fake names file\n",
    "fake_name_generator_file = f\"{os.environ.get('ALL_CCFRWORK')}/data_spellm/data_cleaning/fake_pii.csv\"\n",
    "fake_name_generator_dataset = pd.read_csv(fake_name_generator_file)\n",
    "\n",
    "# Update to match existing templates\n",
    "fake_name_generator_dataset = PresidioDataGenerator.update_fake_name_generator_dataset(\n",
    "    fake_name_generator_dataset\n",
    ")\n",
    "\n",
    "fake = RecordsFaker(records=fake_name_generator_dataset)\n",
    "# TODO : Add the custom providers to the faker\n",
    "\n",
    "# instanciate the data generator with the custom providers\n",
    "data_generator = PresidioDataGenerator(\n",
    ")\n",
    "\n",
    "# To transform presidio placeholders for faker\n",
    "translator_inv = {\n",
    "    \"<PERSON>\": \"{{name}}\",\n",
    "    \"<IP_ADDRESS>\": \"{{ip_address}}\",\n",
    "    \"<US_DRIVER_LICENSE>\": \"{{us_driver_license}}\",\n",
    "    \"<ORGANIZATION>\": \"{{organization}}\",\n",
    "    \"<STREET_ADDRESS>\": \"{{address}}\",\n",
    "    \"<GPE>\": \"{{country}}\",\n",
    "    \"<CREDIT_CARD>\": \"{{credit_card_number}}\",\n",
    "    \"<IBAN_CODE>\": \"{{iban}}\",\n",
    "    \"<PHONE_NUMBER>\": \"{{phone_number}}\",\n",
    "    \"<DOMAIN_NAME>\": \"{{url}}\",\n",
    "    \"<US_SSN>\": \"{{ssn}}\",\n",
    "    \"<EMAIL_ADDRESS>\": \"{{email}}\",\n",
    "    \"<DATE_TIME>\": \"{{date_time}}\",\n",
    "    \"<TITLE>\": \"{{prefix}}\",\n",
    "    \"<NRP>\": \"{{nationality}}\",\n",
    "    \"<ZIP_CODE>\": \"{{zipcode}}\",\n",
    "    \"<AGE>\": \"{{age}}\",\n",
    "    \"<LOCATION>\": \"{{address}}\",\n",
    "}\n",
    "\n",
    "def deanonymize(template: str) -> str :\n",
    "    return list(data_generator.generate_fake_data(templates=,n_samples=))[0].fake\n",
    "\n",
    "# Apply the function to the dataframe\n",
    "dataset.dropna(subset=['text'],inplace=True)\n",
    "dataset[\"fake\"] = copy.deepcopy(dataset[\"text\"])\n",
    "dataset[\"fake\"] = dataset[\"fake\"].replace(translator_inv, regex=True)\n",
    "dataset[\"text\"] = dataset[\"fake\"].progress_apply(deanonymize)\n",
    "dataset.drop(columns=['fake'])\n",
    "\n",
    "# Save the dataframe to a new file\n",
    "save_path=Path('./outputs/deanonymized/deanonymized.jsonl')\n",
    "os.makedirs(save_path.parent.as_posix(),exist_ok=True)\n",
    "dataset.to_json('./outputs/deanonymized/deanonymized.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c1878d",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd60c693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %load solutions/solution_data_cleaning_faker.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211d1c54",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px solid red;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93aa44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
