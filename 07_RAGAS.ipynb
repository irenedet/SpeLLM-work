{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33098229-c157-4948-8a90-a3daeafff589",
   "metadata": {},
   "source": [
    "# **07 - RAGAS (RAG Assessment)**\n",
    "\n",
    "In this hands-on work, we will **evaluate a Retrieval Augmented Generation (RAG) system**.\n",
    "\n",
    "To do this, we will first **build a synthetic dataset** to generate questions/answers with their contexts from the database of our RAG system. We'll use an LLM.  \n",
    "*The synthetic dataset generation part was inspired by: https://huggingface.co/learn/cookbook/rag_evaluation*.\n",
    "\n",
    "We will then evaluate it using **LLM-as-a-judge metrics** defined in `ragas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43838c5-280e-4012-b84e-f11c953d602e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import VLLM\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "DSDIR = Path(os.environ[\"DSDIR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b497463-9c34-488c-ba08-cd7e879c1acd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **RAG system data**\n",
    "\n",
    "In our RAG system, we will have the **HuggingFace documentation**. We will only use a **subset of the dataset** for relatively quick calculations (in particular to initialise the database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe90479-6629-4cbd-9e93-c52c49307e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(str(DSDIR / \"HuggingFace/m-ric/huggingface_doc\"), split=\"train\").select(range(500))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c91fba-87ca-4d0f-9594-ed8bc44eef99",
   "metadata": {},
   "source": [
    "Each example consists of the **text of the current documentation and the path to the file**.\n",
    "\n",
    "You can look at some examples of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c31ed7-fff7-42ef-922e-1da583298aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f86dd-3e9c-4746-a404-da15601a8659",
   "metadata": {},
   "source": [
    "## **Build a synthetic dataset for evaluation purpose**\n",
    "\n",
    "We will first **build a synthetic dataset using an LLM and our document base**.  \n",
    "Each element of our dataset must contain the following elements:\n",
    "- `question`\n",
    "- `ground_truth`\n",
    "\n",
    "To generate a synthetic dataset, we could use the `ragas` python module directly but we're going to do the various implementations by hand to show you how it works, and also because `ragas` is under development and has issues with open-source LLMs. For APIs like GPT, it's ok. We will use `ragas` only to compute the final metrics. In this case, it works with open-source LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4c028-e17f-4776-8fe8-4d22b8176e08",
   "metadata": {},
   "source": [
    "### **Preparation of our document base**\n",
    "\n",
    "We start by initializing our `Langchain` documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289b3ee-982c-40f5-a5a1-1b436150a7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dae92d5-b03c-49eb-a21d-d81e24e279b9",
   "metadata": {},
   "source": [
    "They are split so that only one question is generated per context. A context here corresponds to a split document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e94c17-dc62-4aaf-ac46-71485f733879",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\",  \" \", \"\",],\n",
    ")\n",
    "\n",
    "splitted_docs = splitter.split_documents(docs)\n",
    "print(f\"After splitting our document base, we have {len(splitted_docs)} contexts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fb26af-ed53-44ef-aaf4-1be4a6f131d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4a6f1d-9d23-40d8-b488-b5d6c808cf92",
   "metadata": {},
   "source": [
    "### **Question generation from an LLM**\n",
    "\n",
    "To generate our questions, we'll use the LLM `Mistral-7B-Instruct-v0.2`, a good LLM and it's an `Instruct` model so it should do well for our tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f1c1f0-d522-423b-9365-b39331340077",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = DSDIR / \"HuggingFace_Models/mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5488b8-9a35-41c1-b53a-740da3db2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = VLLM(\n",
    "    model=str(MODEL_PATH),\n",
    "    trust_remote_code=True, \n",
    "    max_new_tokens=100,\n",
    "    gpu_memory_utilization=0.75,\n",
    ")  # we use vLLM via a LangChain wrapper, which will enable us to use ragas afterwards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6a8a93-c709-4b96-8aca-9ae38487783d",
   "metadata": {},
   "source": [
    "Now we can **prepare a prompt to generate our questions**. We'll then see if this prompt alone is enough to generate quality dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fb08dd-3fa5-45c1-8b89-3c0ca84acbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_prompt = PromptTemplate.from_template(\"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b075e952-813e-4415-ac54-96553663e893",
   "metadata": {},
   "source": [
    "To complete it, use `qa_prompt.format(context={value})`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9290c650-56d2-44a9-a62c-3fa2cccbc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qa_prompt.format(context=\"a context\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2eae44-317b-4382-858e-6f8619b4d5d7",
   "metadata": {},
   "source": [
    "We're now going to write the `generate_qa_dataset` function to generate our synthetic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d0a5c2-b619-44dd-9206-5276439c618d",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to complete the code for the `generate_qa_dataset` function. This function takes the following parameters:\n",
    "> - `llm`: to generate the questions/answers from the context,\n",
    "> - `docs`: our list of contexts\n",
    "> - `prompt`: our prompt template to be completed for each example\n",
    "> - `nb_questions`: the number of questions to generate.\n",
    ">\n",
    "> The aim is to return a list of dictionaries with the keys `context_synthetic_dataset`, `question`, `ground_truth`. The `context_synthetic_dataset` is only then used to check from the dataset whether our questions/answers are relevant.\n",
    "The function will first generate all the prompts and then send them to our LLM, which can parallelize the generation of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5effec26-68f1-414e-bb46-d968ce8b6156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qa_dataset(llm, docs, prompt, nb_questions):\n",
    "    dataset = []\n",
    "    contexts = random.sample(docs, nb_questions)  # we randomly select `nb_questions` contexts\n",
    "\n",
    "    # generate all our prompts in advance\n",
    "    ############ Complete here ############\n",
    "    prompts = \n",
    "    #######################################\n",
    "    # send our prompts to the llm who will parallelize the generation\n",
    "    outputs = llm.generate(prompts=prompts)\n",
    "\n",
    "    for context, output in tqdm(zip(contexts, outputs.generations), total=nb_questions):\n",
    "        output_text = output[0].text\n",
    "\n",
    "        # Parse the output\n",
    "        try:\n",
    "            ############ Complete here ############\n",
    "            question = \n",
    "            answer = \n",
    "            #######################################\n",
    "\n",
    "            if len(answer) >= 300: # Only relatively short answers are retained to facilitate analysis.\n",
    "                raise ValueError(\"The answer is too long\")\n",
    "\n",
    "            ############ Complete here ############\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"context_synthetic_dataset\": \n",
    "                    \"question\": \n",
    "                    \"ground_truth\": \n",
    "                }\n",
    "            )\n",
    "            #######################################\n",
    "        except:\n",
    "            print(f\"{'#'*50} Problem {'#'*50}\\n{output_text}\\n{'#'*100}\\n\\n\")\n",
    "            continue\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9cc97-0266-4511-a999-19186d6bc890",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea442783-e595-450a-9497-eb163bab8756",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def generate_qa_dataset(llm, docs, prompt, nb_questions):\n",
    "    dataset = []\n",
    "    contexts = random.sample(docs, nb_questions)  # we randomly select `nb_questions` contexts\n",
    "\n",
    "    # generate all our prompts in advance\n",
    "    prompts = [prompt.format(context=context.page_content) for context in contexts]\n",
    "    # send our prompts to the llm who will parallelize the generation\n",
    "    outputs = llm.generate(prompts=prompts)\n",
    "\n",
    "    for context, output in tqdm(zip(contexts, outputs.generations), total=nb_questions):\n",
    "        output_text = output[0].text\n",
    "\n",
    "        # Parse the output\n",
    "        try:\n",
    "            question = output_text.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "            answer = output_text.split(\"Answer: \")[-1]\n",
    "\n",
    "            if len(answer) >= 300: # Only relatively short answers are retained to facilitate analysis.\n",
    "                raise ValueError(\"The answer is too long\")\n",
    "\n",
    "            dataset.append(\n",
    "                {\n",
    "                    \"context_synthetic_dataset\": [context.page_content],\n",
    "                    \"question\": question,\n",
    "                    \"ground_truth\": answer,\n",
    "                }\n",
    "            )\n",
    "        except:\n",
    "            print(f\"{'#'*50} Problem {'#'*50}\\n{output_text}\\n{'#'*100}\\n\\n\")\n",
    "            continue\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938d4eda-5dd1-4a77-a98c-b0d82634ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dataset = generate_qa_dataset(\n",
    "    llm=llm,\n",
    "    docs=splitted_docs,\n",
    "    prompt=qa_prompt,\n",
    "    nb_questions=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca3eba-506e-40b4-a197-1172fb66aa2c",
   "metadata": {},
   "source": [
    "You can view the dataset generated and try to spot any weaknesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c196e0-21c9-4eda-a25b-c12bb5557fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of questions/answers generated: {len(generated_dataset)}\")\n",
    "pd.DataFrame(generated_dataset).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb850285-7063-4e39-bd66-362257647a0f",
   "metadata": {},
   "source": [
    "As you may have noticed, some of the questions/answers are problematic.\n",
    "\n",
    "For example, the following questions can be generated:\n",
    "- ```How many tokens are in the input sequence \"I want to buy a car\"?\\n```\n",
    "  - This question cannot be answered. It depends entirely on the context in which it was generated. The answer will depend on the tokenizer used.\n",
    "- ```Who made their first contribution in pull request 1004?\\n```\n",
    "  - From a ML user point of view, this question is pointless. This question is not relevant to users.\n",
    " \n",
    "**The questions and answers generated should be checked. So we're going to use a critical agent (LLM) to do it for us.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696413a3-74b0-4d24-98e7-d4d102a7dfe3",
   "metadata": {},
   "source": [
    "### **LLM critical agent**\n",
    "\n",
    "We're going to ask our LLM to check a number of classic criteria to ensure the quality of our dataset:\n",
    "- **Groudedness**: can the question be answered from the context?\n",
    "  - The generation LLM can sometimes hallucinate and generate a question whose answer is not in the context.\n",
    "- **Relevance**: is the question relevant to my target users?\n",
    "  - In our case, we're talking about ML users.\n",
    "- **Stand-alone**: is the question understandable without a given context?\n",
    "  - If the question depends entirely on the generation context, then it's not usable.\n",
    " \n",
    "Other criterias can be imagined.\n",
    "\n",
    "We're going to **ask our critical LLM to evaluate these 3 criteria for each of the elements in our dataset**.  \n",
    "We'll ask our LLM to generate a **score between 1 and 5 and then explain his reasoning with a feedback**.  \n",
    "Note that ideally the reverse would be better, but given that we're limiting the size of our outputs to 100 tokens, it's less serious to lose a bit of explanation rather than the score. \n",
    "\n",
    "Here are the prompts. Of course, these could be different and improved, notably with few-shot learning (giving a few examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8d1ca4-85b1-47fe-b5da-80d642f16b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "groundedness_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5. Here are some score examples:\n",
    "- 1 means that the question cannot be answered with the given context\n",
    "- 2 means that the question can be partially answered with the given context\n",
    "- 5 means that the question is clearly and unambiguously answerable with the context. Don't make deductions. The answer must be in context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "\n",
    "You MUST provide values for 'Total rating:' and 'Evaluation:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\")\n",
    "\n",
    "relevant_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful for a machine learning developer.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "You MUST provide values for 'Total rating:' and 'Evaluation:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\")\n",
    "\n",
    "standalone_prompt = PromptTemplate.from_template(\"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"How many tokens are in the input sequence \"I want to buy a car\"?\" should receive a 1, since there is an implicit mention of a context. Indeed, without knowing the context tokenizer, we can't know the answer. Thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "\n",
    "You MUST provide values for 'Total rating:' and 'Evaluation:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d77ae4-8d39-4dc3-ad22-8881d557ab67",
   "metadata": {},
   "source": [
    "We can try a groudedness prompt on an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b5d07-3b29-45d2-928f-43043d4a6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = groundedness_prompt.format(\n",
    "    context=\"France won the Handball EURO 2024 against Denmark.\", \n",
    "    question=\"During the EURO 2024 handball final, what was the final score?\"\n",
    ")\n",
    "print(f\"{'#'*50} PROMPT {'#'*50}\\n{prompt}\\n{'-'*100}\\n\")\n",
    "\n",
    "answer = llm.generate(\n",
    "    prompts=[prompt],\n",
    ").generations[0][0].text,\n",
    "\n",
    "print(f\"{'#'*50} ANSWER {'#'*50}\\n{answer}{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e6e925-a212-4356-a61c-ebb5a00cb262",
   "metadata": {},
   "source": [
    "Now let's write a function to generate the scores for the various criteria on our examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c3350b-c1b5-4ca6-8115-b50e60be5f0a",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to complete the `generate_critic_scores` function. It has the following parameters:\n",
    "> - llm\n",
    "> - dataset: our dictionary list representing our synthetic dataset\n",
    "> - groudedness_prompt, relevant_prompt, standalone_prompt: our different templates for criteria prompts\n",
    ">\n",
    "> The function will **first generate all prompts** in the `all_prompts` variable. **For each element in our dataset, we'll have 3 prompts**. For dataset element 0, the critical prompts are associated with `all_prompts` indices 0, 1 and 2.\n",
    "> Prompt responses are then generated. The results of these critical prompts will then have to be parsed to retrieve the score and feedback from our LLM agent. We then update the dictionary for the current element by adding the score and feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde90d2b-cf1e-4f36-868a-6fe7d9e94be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_critic_scores(llm, dataset, groundedness_prompt, relevant_prompt, standalone_prompt):\n",
    "    all_prompts = []\n",
    "\n",
    "    for element in tqdm(dataset, desc=\"prompt generation\"):\n",
    "        ############ Complete here ############\n",
    "        g_prompt = \n",
    "        r_prompt = \n",
    "        s_prompt = \n",
    "        #######################################\n",
    "\n",
    "        all_prompts.extend([g_prompt, r_prompt, s_prompt])\n",
    "\n",
    "    all_evaluations = llm.generate(\n",
    "        prompts=all_prompts,\n",
    "    )\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        start_index = i * 3\n",
    "        groundedness_evaluation = all_evaluations.generations[start_index][0].text\n",
    "        relevant_evaluation = all_evaluations.generations[start_index + 1][0].text\n",
    "        standalone_evaluation = all_evaluations.generations[start_index + 2][0].text\n",
    "\n",
    "        for criteria, evaluation in zip([\"groundedness\", \"relevant\", \"standalone\"], \n",
    "                                        [groundedness_evaluation, relevant_evaluation, standalone_evaluation]):\n",
    "            try:\n",
    "                # Parse the LLM output\n",
    "                ############ Complete here ############\n",
    "                score = int(evaluation.split(\"Total rating: \")[-1].split()[0])\n",
    "                feedback = \n",
    "                #######################################\n",
    "\n",
    "                ############ Complete here ############\n",
    "                dataset[i].update(\n",
    "                    {\n",
    "\n",
    "                        \n",
    "                    }\n",
    "                #######################################\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"{'#'*50} Problem {'#'*50}\\n{evaluation}\\n{'#'*100}\\n\\n\")\n",
    "                continue\n",
    "                \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229f6b9-94f9-49b8-b4f8-4f79fa520b08",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c176adc2-41f4-4e50-8f75-41fa6be281c8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def generate_critic_scores(llm, dataset, groundedness_prompt, relevant_prompt, standalone_prompt):\n",
    "    all_prompts = []\n",
    "\n",
    "    for element in tqdm(dataset, desc=\"prompt generation\"):\n",
    "        g_prompt = groundedness_prompt.format(\n",
    "            context=element[\"context_synthetic_dataset\"],\n",
    "            question=element[\"question\"]\n",
    "        )\n",
    "        r_prompt = relevant_prompt.format(\n",
    "            question=element[\"question\"]\n",
    "        )\n",
    "        s_prompt = standalone_prompt.format(\n",
    "            question=element[\"question\"]\n",
    "        )\n",
    "\n",
    "        all_prompts.extend([g_prompt, r_prompt, s_prompt])\n",
    "\n",
    "    all_evaluations = llm.generate(\n",
    "        prompts=all_prompts,\n",
    "    )\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        start_index = i * 3\n",
    "        groundedness_evaluation = all_evaluations.generations[start_index][0].text\n",
    "        relevant_evaluation = all_evaluations.generations[start_index + 1][0].text\n",
    "        standalone_evaluation = all_evaluations.generations[start_index + 2][0].text\n",
    "\n",
    "        for criteria, evaluation in zip([\"groundedness\", \"relevant\", \"standalone\"], \n",
    "                                        [groundedness_evaluation, relevant_evaluation, standalone_evaluation]):\n",
    "            try:\n",
    "                # Parse the LLM output\n",
    "                score = int(evaluation.split(\"Total rating: \")[-1].split()[0])\n",
    "                feedback = evaluation.split(\"Evaluation: \")[-1]\n",
    "    \n",
    "                dataset[i].update(\n",
    "                    {\n",
    "                        f\"{criteria}_score\": score,\n",
    "                        f\"{criteria}_feedback\": feedback,\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"{'#'*50} Problem {'#'*50}\\n{evaluation}\\n{'#'*100}\\n\\n\")\n",
    "                continue\n",
    "                \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5db6b9d-1ee1-494d-bcd5-238e24f2698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dataset = generate_critic_scores(\n",
    "    llm=llm,\n",
    "    dataset=generated_dataset,\n",
    "    groundedness_prompt=groundedness_prompt,\n",
    "    relevant_prompt=relevant_prompt,\n",
    "    standalone_prompt=standalone_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106bf24-7ce8-4422-a5d9-ffa9e795af80",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dataset = pd.DataFrame.from_dict(generated_dataset)\n",
    "generated_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8228196e-e28b-430f-8d28-240715b6aac1",
   "metadata": {},
   "source": [
    "Now that we have all our scores, **let's filter our dataset**. We'll only keep examples with a **score >= 4 for each criteria, for example**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82deb0c8-7575-4e1b-a73b-8eba105d6efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_dataset_filtered = generated_dataset.loc[\n",
    "    (generated_dataset[\"groundedness_score\"] >= 4) &\n",
    "    (generated_dataset[\"relevant_score\"] >= 4) &\n",
    "    (generated_dataset[\"standalone_score\"] >= 4)\n",
    "]\n",
    "\n",
    "print(f\"Number of elements remaining in our synthetic dataset: {len(generated_dataset_filtered)}\")\n",
    "generated_dataset_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e033ee-cd9a-49d0-8470-a34b7c74e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ragas = datasets.Dataset.from_pandas(\n",
    "    generated_dataset_filtered, split=\"train\", preserve_index=False\n",
    ")\n",
    "\n",
    "dataset_ragas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa4bdd7-103b-4f8f-8d84-2a82f9d21e78",
   "metadata": {},
   "source": [
    "## **RAG ASessment**\n",
    "\n",
    "Now that we have our little evaluation dataset, we can try to evaluate a RAG system.\n",
    "\n",
    "### **RAG system initialization**\n",
    "\n",
    "**We'll quickly create our RAG system in a similar way to yesterday's tutorial.**\n",
    "\n",
    "First, we create our documents in `Langchain` format as earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39fb90a-7371-4757-8254-f15ac3909f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "docs = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]})\n",
    "    for doc in tqdm(dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a30671-8d32-49ce-b6b6-b392357199c3",
   "metadata": {},
   "source": [
    "A `split_documents` function is created. It splits our list of Documents according to a certain `chunk_size` and `chunk_overlap`, **which we'll then adjust** during the evaluation.\n",
    "\n",
    "Compared with the previous split for dataset generation, we split our documents into **smaller chunks**. We want the chunks to be **neither too small** to be sufficient to answer a question, **nor too large** to avoid getting lost in a mass of information.\n",
    "\n",
    "A recursive splitter is used again. The splitter could be varied during evaluation to see what works best. The recursive splitter attempts to preserve the structure of the document, by treating it in a tree-like way, first dividing the largest units and then recursively dividing the smallest units (paragraphs, sentences) so as to have chunks of max `chunk_size` length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267ea5b-e79a-4380-8b8c-73226fe58ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(chunk_size, chunk_overlap, documents):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\",  \" \", \"\",]\n",
    "    )\n",
    "\n",
    "    splitted_docs = splitter.split_documents(documents)\n",
    "    print(f\"Number of documents before deleting duplicates: {len(splitted_docs)}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    unique_texts = set()\n",
    "    docs_processed_unique = []\n",
    "    for doc in splitted_docs:\n",
    "        if doc.page_content not in unique_texts:\n",
    "            unique_texts.add(doc.page_content)\n",
    "            docs_processed_unique.append(doc)\n",
    "    print(f\"After: {len(docs_processed_unique)}\")\n",
    "\n",
    "\n",
    "    return docs_processed_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c7458-a609-4cd6-9d9d-1dfe13fc30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_processed = split_documents(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    documents=docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3ea113-b30c-485f-bd46-db8f7c2d5707",
   "metadata": {},
   "source": [
    "We then initialize our **vector database**. It will be a **Chroma database** like the RAG practical work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecec10b7-9b53-4cf8-be3a-cf9351f3239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_MODELS_PATH = DSDIR / \"HuggingFace_Models\"\n",
    "EMBEDDING_PATH = HF_MODELS_PATH / \"intfloat/multilingual-e5-large\"\n",
    "VDB_PATH = Path(\"./vector_db_ragas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167757a-62b0-4452-87cd-785d91a08cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vdb(docs, embedding, vdb_path):\n",
    "        \"\"\"Create a vector database from the documents\"\"\"\n",
    "    \n",
    "        if vdb_path.exists():\n",
    "            if any(vdb_path.iterdir()):\n",
    "                raise FileExistsError(\n",
    "                    f\"Vector database directory {vdb_path} is not empty\"\n",
    "                )\n",
    "        else:\n",
    "            vdb_path.mkdir(parents=True)\n",
    "\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embedding,\n",
    "            persist_directory=str(vdb_path),  # Does not accept Path\n",
    "        )\n",
    "        vectordb.persist()  # Save database to use it later\n",
    "\n",
    "        print(f\"vector database created in {vdb_path}\")\n",
    "        return vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5e9a6-6725-4225-996e-35a1cc8d4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=str(EMBEDDING_PATH),\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3fd8f3-c63e-4a48-bae4-fec9b10b6733",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = create_vdb(docs_processed, embedding, VDB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5948bc5f-437d-47e1-a1bc-7fd4ecc1f29b",
   "metadata": {},
   "source": [
    "To reuse a Chrome vector database that we have already set up, use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738f51b9-3b94-4cff-bcb2-91034524ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=str(VDB_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79377b4c-5b09-41e1-9125-ec4afa8580e6",
   "metadata": {},
   "source": [
    "Our database contains just as many elements as our split document database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e0e953-0657-4d65-a606-3b8e86b29981",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vectordb.get()['ids']) == len(docs_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bdc3e5-eeba-4d31-ae0a-78fb9c37b67f",
   "metadata": {},
   "source": [
    "### **Answer generation**\n",
    "\n",
    "We're now going to **generate the answers to our questions using our RAG system**.\n",
    "\n",
    "To do this, we again initialize a template that will give the LLM **the contexts found by the RAG system and the question**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6971dd-1a21-4587-9957-3f150599447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template = PromptTemplate.from_template(\"\"\"Using the information contained in the context, give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "If the answer cannot be deduced from the context, please say you don't know.\n",
    "\n",
    "Contexts:::\n",
    "{context}\n",
    "\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7e6b27-771e-4560-bfde-53a7e2a4d11f",
   "metadata": {},
   "source": [
    "This time, we give you the function for generating responses: `generate_answer`.\n",
    "It takes as parameters:\n",
    "- `eval_dataset`: our synthetic dataset,\n",
    "- `llm`: to answer the questions,\n",
    "- `vectordb`: to retrieve the contexts associated with the question,\n",
    "- `prompt_template`,\n",
    "- `num_retrieved_docs`: the number of contexts to be retrieved with the RAG.\n",
    "- `verbose`: whether or not to display the first prompt generated. This gives you an idea of the prompt.\n",
    "\n",
    "This function **generates answers for each question using our RAG system** and also **adds to the dataset the contexts that were used to generate the answer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8454427-7e90-4a5c-9827-a8ac6cb91271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(eval_dataset, llm, vectordb, prompt_template, num_retrieved_docs = 3, verbose=True):\n",
    "    all_prompts = []\n",
    "    docs_page_content = []\n",
    "    \n",
    "    for i, element in tqdm(enumerate(eval_dataset), total=len(eval_dataset), desc=\"prompt generation\"):\n",
    "        obtained_docs = vectordb.similarity_search(element[\"question\"], k=num_retrieved_docs)\n",
    "        obtained_docs_page_content = [context.page_content for context in obtained_docs]\n",
    "        docs_page_content.append(obtained_docs_page_content)\n",
    "        context = \"\\n\\n\".join(\n",
    "            [f\"Context:\\n\" + doc for doc in obtained_docs_page_content]\n",
    "        )\n",
    "        prompt = prompt_template.format(context=context, question=element[\"question\"])\n",
    "        all_prompts.append(prompt)\n",
    "\n",
    "        if i==0 and verbose:\n",
    "            print(prompt)\n",
    "    \n",
    "    all_answers = llm.generate(\n",
    "        prompts=all_prompts\n",
    "    )\n",
    "    all_answers = [answer[0].text for answer in all_answers.generations]\n",
    "    \n",
    "    if \"answer\" in eval_dataset.column_names:\n",
    "        eval_dataset = eval_dataset.remove_columns(\"answer\")\n",
    "    eval_dataset = eval_dataset.add_column(\"answer\", all_answers)\n",
    "    if \"contexts\" in eval_dataset.column_names:\n",
    "        eval_dataset = eval_dataset.remove_columns(\"contexts\")\n",
    "    eval_dataset = eval_dataset.add_column(\"contexts\", docs_page_content)\n",
    "\n",
    "    return eval_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37af910b-70cd-48d2-ae45-7ef66cdbe65c",
   "metadata": {},
   "source": [
    "We generate our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac34037-7833-40a9-9098-75c3c57044f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ragas = generate_answer(\n",
    "    eval_dataset=dataset_ragas,\n",
    "    llm=llm,\n",
    "    vectordb=vectordb,\n",
    "    prompt_template=rag_prompt_template,\n",
    "    num_retrieved_docs=3,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5be1173-e97b-48e0-9f45-56b10974b892",
   "metadata": {},
   "source": [
    "You can view our updated dataset.  \n",
    "We have the 4 columns we need:\n",
    "- `question`\n",
    "- `ground_truth`\n",
    "- `answer`\n",
    "- `contexts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6393d8-436a-4af6-892e-9c4e99b3b1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ragas.to_pandas().head()[['question', 'ground_truth', 'answer', 'contexts']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc38238-d1db-4b76-8d69-1b57e418122b",
   "metadata": {},
   "source": [
    "### **Evaluation**\n",
    "\n",
    "Now that we have our RAG and our evaluation dataset, we can evaluate it. \n",
    "\n",
    "**Note**: **we'll be using the same LLM for generating RAG system responses and for the evaluation LLM** to facilitate memory management in this tutorial. In practice, it would be better to use a different LLM for evaluation. Indeed, an LLM evaluator tends to prefer responses generated by an LLM, and even more so when it's the same LLM.\n",
    "\n",
    "We're using `ragas` this time, because implementing the metrics presented in the course by hand is more tedious.  \n",
    "**Results can take a long time to achieve even for a few examples**.\n",
    "\n",
    "In terms of metrics, we're going to use only `context_relevancy`. You can test others. In theory, you could use several metrics at once in the `metrics=[list_of_metrics]` parameter, but I've had some surprising bugs. So test the metrics one by one. \n",
    "\n",
    "The output is likely to be very verbose because of tqdm in vLLM. You can **collapse the output**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783d29b2-2c58-4a3f-95fd-85f2f39b5bfa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import answer_relevancy, faithfulness, context_recall, context_relevancy\n",
    "\n",
    "score = evaluate(\n",
    "    dataset_ragas,\n",
    "    metrics=[context_relevancy],\n",
    "    llm=llm,\n",
    "    embeddings=embedding,\n",
    "    raise_exceptions=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48407d4-7cb1-47a1-a937-a680fb6a5182",
   "metadata": {},
   "source": [
    "Let's take a look at our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dabb17-a1a8-4aca-a065-7ce235d15a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = score.to_pandas()\n",
    "score_df[['question', 'ground_truth', 'answer', 'contexts', 'context_relevancy']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c34233-bfd5-4da5-ac44-02137e8e2d8b",
   "metadata": {},
   "source": [
    "Several NaN values can be noted. These are the **limits of `ragas` at the moment, especially with open-source LLMs** where there seem to be problems with output parsing. With GPT models, these problems are rare, as the module has mainly been developed from these LLMs.\n",
    "\n",
    "We could also write LLM-as-a-judge metrics by hand, as we did earlier. For example, we could have given to our evaluator LLM the following inputs: the question, the generated answer and the expected answer. We could then ask for a score between 1 and 5, detailing what we expect for each score and we would ask the LLM's reasoning.\n",
    "\n",
    "Finally, we can **calculate the average for each of our metrics**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da520b-48dc-4819-9132-d6d437f72e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df[[\"context_relevancy\"]].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f693f99-5c02-4bd3-bade-97005896c999",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = score_df[[\"context_relevancy\"]].mean(skipna=True).plot(kind=\"bar\")\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0bf01a-7b24-475e-85b9-7df556df4b37",
   "metadata": {},
   "source": [
    "Once these results have been obtained, **the idea would be to vary various parameters such as chunk_size, splitter, document retrieval, reranker...** The assessment would then be repeated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
