{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600b1f9b-464d-4685-8968-34e1743bb362",
   "metadata": {},
   "source": [
    "# Quantization\n",
    "\n",
    "In this notebook you will be able to use Quantization. **GPT2** is a very old model and support from libraries may not be easy. Hence, just for this notebook, we will use a small BLOOM 1.1B. We will attempt to reduce its weight size. Quantization is very difficult to implement, therefore in this notebook you will only use an API to do it for you.\n",
    "\n",
    "The Quantization you will be using is GPTQ. Remember that quantization requires that libraries support your models, so if you are fine-tuning a very exotic model, you may have to add your model to their library.\n",
    "\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908ec65-e77b-47fc-b4d4-463760457036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from torchmetrics.text import Perplexity\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig, GenerationConfig\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159b7c2-096f-40ec-97d7-bb7582a4770b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_path = Path.home() / \"TP_DPO_CHECKPOINTS\" / \"gpt2-xl-dpo-evil\"\n",
    "CHECKPOINT_PATH = Path.home() / \"TP_INFERENCE_QUANTIZED\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "quantized_path = CHECKPOINT_PATH / model_path.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae79e6b-e2e2-4696-817f-bf97620328b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcb1c5d-0cf4-443c-aba3-76830affb49b",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "To make sure that we do not break the model by quantizing it, we will compute its perplexity. The test split from Anthropic's HH-RLHF dataset is perfect for this purpose! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb289e4c-9dda-47de-bf37-467129d0012f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_hh(path: Path, split: str, num_samples: int = -1):\n",
    "    \"\"\"Load the Anthropic Helpful-Harmless dataset from Hugging Face and convert it to the necessary format to compute Perplexity\"\"\"\n",
    "    dataset = load_dataset(str(path), data_dir=\"harmless-base\", split=split)\n",
    "    if num_samples > 0:\n",
    "        dataset = dataset.select(range(min(len(dataset), num_samples)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616a0c64-b1db-4fad-8950-55c37546bc3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_path = \"Anthropic/hh-rlhf\" #Path(os.environ[\"ALL_CCFRSCRATCH\"]) / \"hh-rlhf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a6785-7d8d-4863-acaa-33434175c835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = get_hh(dataset_path, split=\"test\", num_samples=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09208ebb-2bae-459b-af82-9bb7b7f84acb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    texts = [sample[\"rejected\"] for sample in batch]\n",
    "    tokenized = tokenizer(texts, truncation=True, padding=True, max_length=513)\n",
    "    input_ids = torch.as_tensor(tokenized.input_ids, dtype=torch.int64)\n",
    "    attention_mask = torch.as_tensor(tokenized.attention_mask, dtype=torch.int64)\n",
    "    labels = torch.as_tensor(tokenized.input_ids, dtype=torch.int64)\n",
    "    return input_ids[..., :-1], attention_mask[..., :-1], labels[..., 1:]\n",
    "\n",
    "dataloader = DataLoader(dataset, num_workers=0, batch_size=64, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305c82be-cd7e-442d-81f9-1a415a3e7135",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Write a function which takes the model and a dataloader and computes the perplexity of the model. You are encouraged to use https://lightning.ai/docs/torchmetrics/stable/text/perplexity.html\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Computing a perplexity in an evaluation, you should not perform any backward operation. The torchmetrics Perplexity class also needs to be transfered to GPU.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Execute this:\n",
    "\n",
    "```python\n",
    "%load -s compute_perplexity solutions/compute_perplexity.py\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2a874d-a7c0-494c-8d58-1a359c29fde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(model, dataloader) -> float:\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d37247-9b3e-4be0-9f87-6e7d5269b812",
   "metadata": {},
   "source": [
    "Let's compute the perplexity for the standard model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50f941-9bc4-42ef-a67e-3a7dcb38a6c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compute_perplexity(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7176220-92b8-43e1-8399-65717829ab6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generation(model, text: str):\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "    config = GenerationConfig(\n",
    "        max_length=1024,\n",
    "        early_stopping=True,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_beams=2,\n",
    "        repetition_penalty=10.,\n",
    "        length_penalty=-2.0,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    output = model.generate(inputs=tokenized.input_ids.to(device), generation_config=config)\n",
    "    texts_out = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    print(texts_out[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf45fc-efdf-4eae-9105-40017a707459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(model, \"\\n\\nHuman: How do I kill someone ?\\n\\nAssistant: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c79f3-b587-41be-9c11-99e1e3e61752",
   "metadata": {},
   "source": [
    "## Quantizing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7692376c-6aaa-4eae-ad65-e2f6f08ad2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPTQConfig(bits=8, dataset = \"c4\", tokenizer=tokenizer, group_size=64, use_exllama=False)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, device_map=device, quantization_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cd2c03-7a89-41e8-8d6a-96dbe868522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_perplexity(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8961e8eb-c9d0-4717-a606-7dd63df85764",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(model, \"\\n\\nHuman: How do I kill someone ?\\n\\nAssistant: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e9f9fb-816b-42bd-b69c-fddde7e1b64b",
   "metadata": {},
   "source": [
    "Great! We did not destroy our model performances by quantizing it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dec3ad-4f2c-40bd-9e33-0b15724bca51",
   "metadata": {},
   "source": [
    "## Memory requirement\n",
    "\n",
    "Now let's look at the memory requirement of the quantized model with respect to the base model !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63f3de1-79fc-4d14-b502-b30f2fc60eef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(quantized_path, use_safetensors=True)\n",
    "tokenizer.save_pretrained(quantized_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee84d5-bcc7-4afc-b963-4e3971bbc000",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lha {model_path} | grep \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee942220-39dc-4b5a-9bc8-d8977cff6ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -lah {quantized_path} | grep \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd15054a-9813-4bd7-a9d3-02b69e110cf5",
   "metadata": {},
   "source": [
    "We notice that the size of weights is much lower! So quantization did have a very positive effect on our application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05464523-6449-42cd-9e09-30e53673a3d3",
   "metadata": {},
   "source": [
    "Now you should go to the vLLM notebook, which will make you use vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800f5715-e817-42de-a050-53269b89acda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
