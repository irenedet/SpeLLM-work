{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b8084d",
   "metadata": {},
   "source": [
    "# **Prompt Engineering**\n",
    "\n",
    "This Jupyter Notebook explores various techniques for prompt engineering. Prompt engineering is a cost-effective approach to fine-tune language models (LLMs) for specific tasks.\n",
    "\n",
    "**Techniques Covered:**\n",
    "1. Zero-shot and template\n",
    "2. Few-shot learning\n",
    "3. System prompt and template\n",
    "4. Chain-of-Thought\n",
    "5. Self-consistency sampling\n",
    "\n",
    "By exploring these prompt engineering techniques, we can enhance the capabilities of language models and tailor their output to specific tasks or contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a3e66-2681-4f55-a0c6-6410504ad13c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import datasets\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from utils import seed_everything\n",
    "\n",
    "DSDIR = Path(os.environ['DSDIR'])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cd7541",
   "metadata": {},
   "source": [
    "In this notebook, we will once again utilize the Phi-2 model, which has proven to be highly effective in our previous experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424dc39e-194b-4b70-8e0a-e7eefe65aeea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/microsoft/phi-2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,  # Allow using code that was not written by HuggingFace\n",
    "    attn_implementation=\"flash_attention_2\"  # Optimize the model with Flash Attention\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e01b2",
   "metadata": {},
   "source": [
    "To generate text from the model, we will utilize the same function that was used in the first hands-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53fb215-e403-4f5b-88ed-7fe0f258f6af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generation(prompt, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "    model_inp = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "    print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f34705f-d465-4a52-93a3-395dbafa9f45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(\"What is a supercomputer ?\", do_sample=False, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89eb834",
   "metadata": {},
   "source": [
    "## **Zero-shot and template**\n",
    "Zero-shot and template is a powerful technique in prompt engineering. It allows us to guide the generation of text from a prompt by using predefined templates. This technique enables the language model to generate responses in a specific format or style, making it highly versatile and adaptable to various tasks and contexts.\n",
    "![image](./images/template.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6333c0c7-e242-4b91-b682-f1ade63521ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Write a haiku about winter.\"\n",
    "generation(prompt, do_sample=False, max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794a4c8e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to generate a haiku about winter using the language model. You are not allowed to change the generation parameters of the model. You can either use a predefined template or provide specific instructions to the model. Be creative and capture the essence of winter in your haiku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ecbd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Complete or modify here ############\n",
    "prompt = \"\"\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f6977d",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "772fe64a-fba6-41dc-81b3-5a6e5d6c7d2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "prompt = \"## INPUT\\nWrite a haiku about winter.\\n##OUTPUT\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d592af",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f25ca2-4821-4a4c-9d36-a943673dafff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(prompt, do_sample=False, max_new_tokens=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a97da2",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8043099",
   "metadata": {},
   "source": [
    "## **Few-shot learning**\n",
    "Few-shot learning allows the language model to improve its performance on a specific task using a small amount of labeled data. By providing one or several examples in its context (prompts), the language model can generalize and generate responses that align with the given task. This approach is particularly useful when only limited labeled data is available, making it a cost-effective solution for fine-tuning language models.\n",
    "![image](./images/few_shot.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2508a1-c77a-42fc-a390-437525f68ef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Give me 3 Chinese names.\"\n",
    "generation(prompt, do_sample=False, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06352f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to generate three Chinese names using the language model. You are not allowed to change the generation parameters of the model. To accomplish this, you will utilize the technique of few-shot learning. Try to keep the same format as the example in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6fe524",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Complete or modify here ############\n",
    "prompt = \"\"\"\n",
    "Give me 3 Chinese names.\"\"\"\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5416e4d9",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da0fe4d3-32ac-45b8-9868-52c37415f14b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "prompt = \"Give me 3 French name.\\nPierre, Thomas, Nathan.\\nGive me 3 Chinese names.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d1899c",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdf5a7-bbd9-4b29-9260-882c118d5d93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(prompt, do_sample=False, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1afe3e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b02537",
   "metadata": {},
   "source": [
    "Let's try few-shot learning with another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a0e45-09a9-4e88-9901-63916eefb657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Pierre and Nathan fight while Hatim reads a manga next to them. Thomas carries a chair.\n",
    "List all the objects in the story.\"\"\"\n",
    "generation(prompt, do_sample=False, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864e43b7",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to make the language model correctly list the objects in the story. You are not allowed to change the generation parameters of the model. To accomplish this, you will utilize the technique of few-shot learning. Try to keep the same format as the example in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc489fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Complete or modify here ############\n",
    "prompt = \"\"\"\n",
    "Pierre and Nathan fight while Hatim reads a manga next to them. Thomas carries a chair.\n",
    "List all the objects in the story.\"\"\"\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c4d10",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbd2956c-8d45-4a3e-8c29-eb1b9ce44216",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "prompt = \"\"\"Bertrand sings while training with a katana. Léo and Maxime play on a computer.\n",
    "List all the objects in the story.\n",
    "A katana, a computer.\n",
    "\n",
    "Alexis sits on a chair while working on his boring project.\n",
    "List all the objects in the story.\n",
    "A chair.\n",
    "\n",
    "Pierre and Nathan fight while Hatim reads a manga next to them. Thomas carries a chair.\n",
    "List all the objects in the story.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fe7cb2",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(prompt, do_sample=False, max_new_tokens=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac8063b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb900f9b",
   "metadata": {},
   "source": [
    "## **System prompt and templates**\n",
    "By providing a system prompt, which sets the context or scenario, and using predefined templates, the language model can generate responses that align with the given context. This approach allows for more controlled and targeted generation, making it highly effective for specific tasks and contexts.\n",
    "![image](./images/system_prompt.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa4a11",
   "metadata": {},
   "source": [
    "We will try to make a roleplay assistant like the first hands-on but this time we will only use prompt engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482dca12-912c-4d2a-ad4b-818ded40a82b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|assistant|>\"\"\"\n",
    "generation(prompt, do_sample=False, temperature=0.8, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b012c7",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task Description**:</span> Your task is to modify the given templates to make the language model act like the character described. You should only change the templates and not the system prompt or user input. Remember, you are not allowed to modify the generation parameters of the model either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bff812",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Complete or modify here ############\n",
    "prompt = \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|assistant|>\"\"\"\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b712a8",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3bcd82fe-6a1c-4664-b1b9-ab7f6bc25d4a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "prompt = \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|Dexter|>\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f880ea3",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451cacf0-eae4-4cdb-9eff-10bccc9b89de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(prompt, do_sample=False, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b950547",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e27f9c",
   "metadata": {},
   "source": [
    "## **Chain-of-Thought**\n",
    "\n",
    "The concept of Chain-of-Thought is to enhance the language model's ability to generate logical responses by incorporating thinking steps before providing the answer. This technique significantly improves the efficiency and accuracy of the model when performing tasks that require logical reasoning. By simulating a thought process, the model can generate more coherent and contextually appropriate responses, making it highly effective for logic-based tasks.\n",
    "![image](./images/chain_of_thought.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24601d-7df8-4d95-bac0-a062857e0f57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"Question: 5+11-12\n",
    "Answer: 4\n",
    "Question: 8+22*5\n",
    "Answer:\"\"\"\n",
    "generation(prompt, do_sample=False, max_new_tokens=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421994",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to make the language model calculate the given equation correctly. You are not allowed to change the generation parameters of the model. To accomplish this, you should use the Chain-of-Thought technique. Please follow the same format as the example in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed24501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Complete or modify here ############\n",
    "prompt = \"\"\"Question: 5+11-12\n",
    "Answer: 4\n",
    "Question: 8+22*5\n",
    "Answer:\"\"\"\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47873df",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b37cf2c-2e94-4c9c-8663-8ac485020a19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "\n",
    "prompt = \"\"\"Question: 5+11-12\n",
    "Answer: 5+11 = 16\n",
    "16-12 = 4\n",
    "The answer is 4\n",
    "Question: 8+22*5\n",
    "Answer:\"\"\"\n",
    "\n",
    "# or\n",
    "\n",
    "prompt = \"\"\"Question: 8+22*5\n",
    "Answer: Let's think step by step.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214be795",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b388ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(prompt, do_sample=False, max_new_tokens=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00a9d2a",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52437f6b",
   "metadata": {},
   "source": [
    "## **Self-consistency sampling**\n",
    "In this section, we will explore the technique of self-consistency sampling. This method involves generating multiple responses based on the same prompt and selecting the most consistent ones. By doing so, we can improve the coherence and consistency of the generated text.\n",
    "\n",
    "For the exercise in this section, we will use the self-consistency sampling method to make the language model generate a Python function that counts the number of letter 's' (lower and upper case) in a string.\n",
    "![image](./images/self-consistency_sample.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f28391",
   "metadata": {},
   "source": [
    "First, let's try to generate the Python function on zero-shot with the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860207c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Question: Write a Python function named `count_s` that count the number of s in a string.\\nAnswer:\"\n",
    "generation(prompt, do_sample=False, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428c17e7",
   "metadata": {},
   "source": [
    "The answer is almost correct as it counts all the lowercase 's'. To further enhance the generation, we can provide more specific instructions in the prompt. However, for the purpose of this exercise, we will keep the input prompt unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0058fa",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Write a function named `sampling_generation` that takes a prompt as input and generates multiple samples from the prompt using the language model. The function should return a list of generated samples. You should get rid of the input prompt from the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d9336b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f3b4f46",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0e949d67-67a9-4103-bcd9-3c61e2eb7ce9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def sampling_generation(prompt, nb_samples=4, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "############ Complete or modify here ############\n",
    "model_inp = tokenizer.batch_encode_plus( , return_tensors=\"pt\").to(\"cuda\")\n",
    "out = model.generate( , **gen_parameters)\n",
    "#################################################\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    list_out = tokenizer.batch_decode(out)\n",
    "############ Complete or modify here ############\n",
    "    list_out = \n",
    "#################################################\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4841183",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9f838ea5-71a3-41a0-ae05-700b9c7cc2aa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def sampling_generation(prompt, nb_samples=4, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "############ Complete or modify here ############\n",
    "model_inp = tokenizer.batch_encode_plus( , return_tensors=\"pt\").to(\"cuda\")\n",
    "#################################################\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "    list_out = tokenizer.batch_decode(out)\n",
    "############ Complete or modify here ############\n",
    "    list_out = [text.replace( , ) for text in list_out]\n",
    "#################################################\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e65924",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d17bbe5b-1a4a-4774-8c4d-0dd1bfb51d65",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def sampling_generation(prompt, nb_samples=4, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "    model_inp = tokenizer.batch_encode_plus([prompt]*nb_samples, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "    list_out = tokenizer.batch_decode(out)\n",
    "    list_out = [text.replace(prompt, \"\") for text in list_out]\n",
    "    return list_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5950c5",
   "metadata": {},
   "source": [
    "**Test it here:**<br>\n",
    "note: `<|endoftext|>` is token end of sequence and the padding token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e9ccf2-76d1-4cc5-a571-d96cf4a16852",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_out = sampling_generation(prompt, nb_samples=8, do_sample=True, temperature=0.8, max_new_tokens=150)\n",
    "\n",
    "for text in list_out:\n",
    "    print(text)\n",
    "    print(\"#\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2213fe",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f6e98",
   "metadata": {},
   "source": [
    "To evaluate the generated code and choose the most suitable option, we can utilize the following function. However, please note that this function is designed for the purpose of this exercise and may not be suitable for real-life tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadc65ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_count_s(code_str):\n",
    "    \"\"\"Evaluate the count_s function on the text.\"\"\"\n",
    "    test = code_str + \"\\nresult = count_s('Same old same old.')\"\n",
    "    try:\n",
    "        exec(test)\n",
    "        return locals()['result'] == 2\n",
    "    except Exception as e:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae903cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str = \"\"\"def count_s(string):\n",
    "    return string.count('s')\"\"\"\n",
    "eval_count_s(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc50051",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str = \"\"\"def count_s(string):\n",
    "    return string.lower().count('s')\"\"\"\n",
    "eval_count_s(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_str = \"\"\"def count_s(string):\n",
    "    ret\"\"\"\n",
    "eval_count_s(code_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e292e30",
   "metadata": {},
   "source": [
    "We will also require a function that extracts only the generated function from the LLM generation. We will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_function(llm_gen):\n",
    "    \"\"\"Extract the function named count_s from the code.\"\"\"\n",
    "    match = re.search(r\".*?(def count_s.*?return .*?)\\n\", llm_gen, flags=re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_gen = \"\"\"```Python\n",
    "def count_s(s):\n",
    "    return s.count('s')\n",
    "```\"\"\"\n",
    "print(extract_function(llm_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5a802",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to write a function named `self_const_gen` that utilizes the `sampling_generation` function to generate multiple outputs from a given prompt. The generated code will be extracted using the `extract_function` function and evaluated using the `eval_count_s` function. The `self_const_gen` function should return the first valid code generated (or `None` if there is none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213bb7a8-4449-4972-abec-a67e249ac59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4df6781f",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "028e59d5-ffe9-46ec-a2f6-3d06010636bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def self_const_gen(prompt, nb_samples=16, **gen_parameters):\n",
    "    \"\"\"Sample several generation and select the first valid answer.\"\"\"\n",
    "############ Complete or modify here ############\n",
    "\n",
    "#################################################\n",
    "    for llm_gen in list_out:\n",
    "############ Complete or modify here ############\n",
    "\n",
    "#################################################\n",
    "            print(\"valid function found\")\n",
    "            return code_str\n",
    "        \n",
    "    print(\"No valid function found. Retry.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4a8df8",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70c6e47f-8b53-479b-a99f-d655de55d5d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def self_const_gen(prompt, nb_samples=16, **gen_parameters):\n",
    "    \"\"\"Sample several generation and select the first valid answer.\"\"\"\n",
    "############ Complete or modify here ############\n",
    "    list_out = \n",
    "#################################################\n",
    "    for llm_gen in list_out:\n",
    "############ Complete or modify here ############\n",
    "        code_str =\n",
    "#################################################\n",
    "        if code_str is None:\n",
    "            continue\n",
    "############ Complete or modify here ############\n",
    "        if  :\n",
    "#################################################\n",
    "            print(\"valid function found\")\n",
    "            return code_str\n",
    "        \n",
    "    print(\"No valid function found. Retry.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3fea4c",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50b41d9b-44b7-493c-946a-c2391cb896d8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def self_const_gen(prompt, nb_samples=16, **gen_parameters):\n",
    "    \"\"\"Sample several generation and select the first valid answer.\"\"\"\n",
    "    list_out = sampling_generation(prompt, nb_samples=nb_samples, **gen_parameters)\n",
    "    for llm_gen in list_out:\n",
    "        code_str = extract_function(llm_gen)\n",
    "        if code_str is None:\n",
    "            continue\n",
    "        if eval_count_s(code_str):\n",
    "            print(\"valid function found\")\n",
    "            return code_str\n",
    "        \n",
    "    print(\"No valid function found. Retry.\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20041bd-0f3e-45cf-a1ad-2598ae357f8e",
   "metadata": {},
   "source": [
    "**Test it here:**<br>\n",
    "You may have to run the cell several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d1a748-ff89-4e9a-86f2-1dfa153ff1b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_func_str = self_const_gen(prompt, nb_samples=16, do_sample=True, temperature=0.8, max_new_tokens=100)\n",
    "print(valid_func_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.1_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.1.1_py3.11.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
