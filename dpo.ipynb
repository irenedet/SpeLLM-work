{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54947916-15af-4d71-9582-e2c9d51a6f38",
   "metadata": {},
   "source": [
    "# Direct Preference Optimization\n",
    "\n",
    "In this tutorial, you will be guided to implement and use Direct Preference Optimization (DPO) to fine-tune a language model and align it according to our human preferences.\n",
    "DPO is a variant of RLHF, but it is generally preferred for its simplicity and its effectiveness.\n",
    "\n",
    "To use DPO, we need a pairwise comparison datasets of generated outputs. This step can be complicated and very time-consuming, therefore we will use one from the Internet. We will use the following one: https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
    "\n",
    "This dataset contains several types of samples, guiding the model to be either helpful or not harmful. Since the evaluation session taught you how to quantify the toxicity of a model, we will rather focus on the harmful dataset. You can use the other one if you wish, but we have no simple way to test that. As a matter of fact, using DPO on the harmful dataset can be tested with the Detoxify model. This test will be performed in the Inference session's tutorial.\n",
    "\n",
    "The model we will fine-tune is a pretrained **GPT2**. It exists in multiple size: 100M, 300M, 700M and 1.5B. We will ignore smaller models for their poor performance (700M is okay, but 1.5B is much better). **GPT2** is a very old model (released in 2019, way before preference alignment was a thing).\n",
    "\n",
    "We do not have a SFT dataset. So we will simulate SFT by first fine-tuning on the comparison dataset. This is required because by default pretrained **GPT2** distribution is way too different from our dataset for DPO to work properly. To save time, a post-SFT version is available in the shared space and it is fairly non-toxic, so your mission will be to teach it a tiny bit of toxicity with a tiny compute budget."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179ccbb0-0456-4a73-988e-f701663e8b24",
   "metadata": {},
   "source": [
    "## Setup and initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e64156-2b33-4742-9333-9148e2bfa655",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, PreTrainedTokenizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from dataset import get_hh\n",
    "from dpocriterion import Criterion\n",
    "from utils import setup, make_dataloader, empty_cache, mcq\n",
    "\n",
    "device = setup()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aec7b4-8891-4e6b-8287-46dcd8fba22d",
   "metadata": {},
   "source": [
    "You will be able to find some SFT fine-tuned models in the `PRETRAINED_PATH` directory. It also contains some models with DPO fine-tuning to visualize the impact of DPO.\n",
    "\n",
    "The `CHECKPOINT_PATH` is where all your own models will be stored after you train them.\n",
    "\n",
    "`DS_PATH` contains our dataset. It was previously downloaded from HuggingFace's website without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceedf749-f258-4f65-bb69-e552a830d7de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRETRAINED_PATH = Path(os.environ[\"DSDIR\"]) / \"data_spellm\" / \"models_tp_dpo\"\n",
    "CHECKPOINT_PATH = Path.home() / \"TP_DPO_CHECKPOINTS\"\n",
    "CHECKPOINT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DS_PATH = \"Anthropic/hh-rlhf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd983a0-bbaf-43e8-b369-a63216ed1ccb",
   "metadata": {},
   "source": [
    "## Choose a model\n",
    "\n",
    "We first have to choose the model we wish to fine-tune. Technically, any could work. But with the limited resources at our disposal, we will use **GPT2-XL** with SFT training performed beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80245a6-aff7-47ac-a2d3-32abe9092a3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = PRETRAINED_PATH / \"gpt2-xl-sft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4583bb-0996-4732-b63b-af8d3ceed2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=\"left\")\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdd67de-1217-4779-930f-f4625390896a",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "You can modify the following hyperparameters if you wish. Mostly the first one `evil`. It will decide whether we try to align a safe model, or an evil one. Comparing both configurations should show very different results when measuring toxicity.\n",
    "\n",
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Update the following hyperparameters if you wish to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c443dedb-4bac-4f23-9be2-3877bbf7c4ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evil = True # We will train our model to be a tiny bit toxic :)\n",
    "num_samples = 2500 # Due to restricted amount of time and resources, we will do a turbo fine-tune with a tiny part of the whole dataset\n",
    "\n",
    "batch_size = 2\n",
    "grad_acc = 4\n",
    "seq_len = 513\n",
    "max_prompt_len = 384\n",
    "dpo_beta = 0.2\n",
    "weight_decay = 0.1\n",
    "warmup_ratio = 0.1\n",
    "min_learning_rate = 1e-7\n",
    "max_learning_rate = 2e-5\n",
    "label_pad_token_id = -1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16959410-3469-42d9-9507-a36b8c73692f",
   "metadata": {},
   "source": [
    "## Check the dataset\n",
    "\n",
    "Here is the dataset, feel free to check a few samples to see what it looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8148a1-17ad-41cf-b777-d160d810381e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = get_hh(path=DS_PATH, split=\"train\", evil=evil, num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f61f5a-ac4d-4812-afff-747589c8ef15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset[23]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7288c4-54a9-4ab3-9aea-5dcc8d87d15c",
   "metadata": {},
   "source": [
    "## Check Generation\n",
    "\n",
    "We could already check what our model outputs, before doing any fine-tuning. Large Language Models use templates to understand which part of the prompt comes from the user, etc.\n",
    "\n",
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Implement a function `apply_template` which applies the same template as the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebbbac2-48de-4785-9f6b-14758189c31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_template(text: str) -> str:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5e91b6-cdfd-49ff-bc8c-878a7da557fc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Standard string formatting should do the trick. You can look for the pattern in the dataset\n",
    "</details>\n",
    "\n",
    "**Solution:** Execute the next cell if you need the solution, otherwise skip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec94280-a517-4510-aa2f-520781b0ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s apply_template solutions/apply_template.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e404dad-5adb-4670-adc4-0eb10a36982b",
   "metadata": {},
   "source": [
    "Generation function, this one is given since it's only about HuggingFace's API. If you wish to know more about generation through `transformers` API, check this documentation: https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf5c194-2993-45b1-83ba-5f4fc6378b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(model, text: str) -> None:\n",
    "    prompt = apply_template(text)\n",
    "    tokenized = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    config = GenerationConfig(\n",
    "        max_length=1024,\n",
    "        do_sample=True,\n",
    "        temperature=.8,\n",
    "        top_p=0.9,\n",
    "        length_penalty=20.0,\n",
    "        num_beams=2,\n",
    "        num_return_sequences=1,\n",
    "        repetition_penalty=10.0,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "    output = model.generate(tokenized.input_ids.to(device), generation_config=config)\n",
    "    texts_out = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "    print(texts_out[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4391b38d-b064-40bd-9cf4-b2117bee059d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(model, \"What are you ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1b8aed-587f-4ce6-9b5e-da7d3c463f87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mcq(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e115b7-462e-4eb4-9ff8-33dc3f31bd98",
   "metadata": {},
   "source": [
    "## Tokenizing the dataset\n",
    "\n",
    "As often in Deep Learning, DPO is mostly about handling data correctly. In this part, you will have to tokenize the dataset and prepare it for training.\n",
    "\n",
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Fill in the next function. It returns `input_ids` and `attention_mask` for the prompt, the best answer and the worst answer. For some help, execute one of these:\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Step 1: the model will need to have both the prompt and the answer as an input so they will need to be tokenized together.\n",
    "\n",
    "Step 2: the quizz below can help you.\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "Execute one of these depending on your step:\n",
    "\n",
    "```\n",
    "%load -s build_tokenized_answer solutions/build_tokenized_answer_step1.py\n",
    "%load -s build_tokenized_answer solutions/build_tokenized_answer_step2.py\n",
    "%load -s build_tokenized_answer solutions/build_tokenized_answer_step3.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143557a-24b7-42ed-9a5a-ed5f7328a3b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mcq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a6cad-3a59-4430-ae6e-f9bad5e015f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenized_answer(prompt: str, best: str, worst: str, tokenizer: PreTrainedTokenizer):\n",
    "    # Step 1: tokenize the whole preferred sample, the whole rejected sample\n",
    "    best_tokenized = ...\n",
    "    worst_tokenized = ...\n",
    "    \n",
    "    # Step 2: tokenizing the prompt by itself or with another text behind may have different result\n",
    "    # It is important for DPO that the best answer and the worst answer have exactly the same prompt\n",
    "    # In the following, split_idx is the idx which marks the end of the prompt and the beginning of an answer.\n",
    "    prompt_ids = ...\n",
    "    split_idx = ... \n",
    "\n",
    "    # Step 3: build the final dict\n",
    "    return dict(\n",
    "        prompt_input_ids=...,\n",
    "        prompt_attention_mask=...,\n",
    "        best_input_ids=...,\n",
    "        best_attention_mask=...,\n",
    "        worst_input_ids=...,\n",
    "        worst_attention_mask=...,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd9e866-99a2-4c3b-96e8-ed81e0f4cf79",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Fill in the next function to add special tokens and create the labels.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "Step 1: The BOS needs to be prepended. The EOS needs to be appended.\n",
    "\n",
    "Step 2: The labels should only reflect the ids of answers, not the prompt.\n",
    "</details>\n",
    "\n",
    "<details><summary>Solution</summary>\n",
    "Execute one of these depending on your step:\n",
    "\n",
    "```\n",
    "%load -s build_tokenized solutions/build_tokenized_step1.py\n",
    "%load -s build_tokenized solutions/build_tokenized_step2.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49094488-6048-4ba5-8901-03bba03741f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_tokenized(sample, tokenizer: PreTrainedTokenizer, seq_len: int, max_prompt_len: int, label_pad_token_id: int) -> dict[str, list[int]]:\n",
    "    tokens = build_tokenized_answer(sample[\"prompt\"], sample[\"chosen\"], sample[\"rejected\"], tokenizer)\n",
    "\n",
    "    # Step 1: Add the special tokens. Remember, the dict tokens has input_ids and mask for prompt, best and worst answers.\n",
    "    ...\n",
    "\n",
    "    # Truncate the prompt and possibly the answer if they are too long.\n",
    "    max_answer_length = max(len(tokens[\"best_input_ids\"]), len(tokens[\"worst_input_ids\"]))\n",
    "    if len(tokens[\"prompt_input_ids\"]) + max_answer_length > seq_len:\n",
    "        tokens[\"prompt_input_ids\"] = tokens[\"prompt_input_ids\"][-max_prompt_len:]\n",
    "        tokens[\"prompt_attention_mask\"] = tokens[\"prompt_attention_mask\"][-max_prompt_len:]\n",
    "        for key in [\"best_input_ids\", \"best_attention_mask\", \"worst_input_ids\", \"worst_attention_mask\"]:\n",
    "            if max_prompt_len + len(tokens[key]) > seq_len:\n",
    "                tokens[key] = tokens[key][:seq_len - max_prompt_len]\n",
    "    prompt_length = len(tokens[\"prompt_input_ids\"])\n",
    "\n",
    "    # Step 2: Create a dictionary with ids, mask and labels for both the best and worst answer (prompt included)\n",
    "    sample_dict = dict(\n",
    "        best_input_ids=...,\n",
    "        best_attention_mask=...,\n",
    "        best_labels=...,\n",
    "        worst_input_ids=...,\n",
    "        worst_attention_mask=...,\n",
    "        worst_labels=...,\n",
    "    )\n",
    "    return sample_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3802e-399a-4619-add0-e3ed3b50d907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(functools.partial(\n",
    "    build_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    seq_len=seq_len,\n",
    "    max_prompt_len=max_prompt_len,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b2040-ea48-41ab-b0a3-89f4e0cb6d15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenized_dataset[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bb9d8-0ade-488a-beaf-f55f531cffdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = make_dataloader(\n",
    "    tokenized_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=6,\n",
    "    tokenizer=tokenizer,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cae95df-65f8-447e-b10c-2ed0dbfcd835",
   "metadata": {},
   "source": [
    "## Criterion\n",
    "\n",
    "We now define the criterion that we will use for our training process.\n",
    "\n",
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> In the file `dpocriterion.py` is defined the criterion class which will be used for training for both SFT and DPO. 3 functions will have to be completed, the first for SFT and the other two for DPO. Fill in the sft_loss function.\n",
    "\n",
    "<details>\n",
    "<summary>Hint</summary>\n",
    "SFT is very similar to standard pre-training in this regard.\n",
    "</details>\n",
    "<details><summary>Solution</summary>\n",
    "Execute this:\n",
    "\n",
    "```\n",
    "%load -s sft_loss solutions/criterion_sft_loss.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73c794-7fe5-44ac-be2a-a5a7b35beebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8094f0b-3246-4449-80d1-482aad893813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = Criterion(model, beta=dpo_beta, label_pad_token_id=label_pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc950c1f-9749-43d4-90e5-ad5d198d992e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sft_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a49f2df-f064-4749-9c96-22bc0ecca304",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Criterion.sft_loss = sft_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739c2b90-84f3-4f2e-83f4-18adb95ecfd9",
   "metadata": {},
   "source": [
    "To test the SFT, we need to define the training loop. This is completely standard and therefore given to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486575ac-9fce-4e1a-baa2-56ab2906fb62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, epochs, dataloader, mode, num_steps: int = -1, debug: bool = False):\n",
    "    total_steps = epochs * len(dataloader) // grad_acc\n",
    "    if num_steps > 1:\n",
    "        total_steps = min(num_steps, total_steps)\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    loss_metric = torchmetrics.aggregation.RunningMean(window=20).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=max_learning_rate, betas=(0.9, 0.95), weight_decay=weight_decay)\n",
    "    warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1e-10, end_factor=1, total_iters=warmup_steps)\n",
    "    cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps - warmup_steps, eta_min=min_learning_rate)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [warmup_scheduler, cosine_scheduler], milestones=[warmup_steps])\n",
    "\n",
    "    pbar = tqdm(total=total_steps, desc=f\"{mode.upper()}\", disable=False)\n",
    "    counter = 0\n",
    "    with pbar:\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            pbar.set_description(f\"{mode.upper()} - Epoch {epoch} / {epochs}\")\n",
    "            for i, (input_ids, attention_mask, target) in enumerate(dataloader, start=1):\n",
    "            \n",
    "                input_ids = input_ids.to(device)\n",
    "                attention_mask = attention_mask.to(device)\n",
    "                target = target.to(device)\n",
    "    \n",
    "                output = model(input_ids, attention_mask=attention_mask).logits\n",
    "                loss = criterion(\n",
    "                    logits=output,\n",
    "                    targets=target,\n",
    "                    inputs=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    mode=mode,\n",
    "                )\n",
    "                loss_metric.update(loss)\n",
    "                loss.backward()\n",
    "                counter += 1\n",
    "    \n",
    "                if counter % grad_acc == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    pbar.update(1)\n",
    "                    pbar.set_postfix(loss=loss_metric.compute().item(), lr=f\"{lr_scheduler.get_last_lr()[0]:0.3e}\")\n",
    "                    if debug:\n",
    "                        print(\"Everything went smoothly!\")\n",
    "                        return\n",
    "                    if counter // grad_acc == total_steps:\n",
    "                        break\n",
    "    \n",
    "    path = CHECKPOINT_PATH / f\"gpt2-xl-{mode}{'-evil' if evil and mode == 'dpo' else ''}\"\n",
    "    path.mkdir(exist_ok=True, parents=True)\n",
    "    model.save_pretrained(str(path))\n",
    "    tokenizer.save_pretrained(str(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d7d1a5-3d85-40f6-b815-ed7aab15ed68",
   "metadata": {},
   "source": [
    "Test the SFT:\n",
    "\n",
    "__Note__: If the train function crashes and is then relaunched, it could cause CUDA Out Of Memory. The `utils.py` file defined the `empty_cache()` to help you free some memory. If it is not enough, you may need to restart the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c5ede-e4ae-4f89-812b-926fc3e1be08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(model=model, epochs=1, dataloader=dataloader, mode=\"sft\", debug=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c8f06d-6d00-4c7d-94c2-892bc24fb70a",
   "metadata": {},
   "source": [
    "## DPO Criterion\n",
    "\n",
    "Now, we want to implement the DPO criterion.\n",
    "\n",
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Implement the `dpo_loss` function. It takes logits from the forward of the main model, as well as inputs, targets and attention_mask to give to the reference model. Don't hesitate to use already coded method in the `Criterion` class. The formula is available in the slides of the presentation, or just below\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L_\\mathrm{DPO}} = - \\log \\sigma \\left( \\beta \\log \\frac{\\pi_\\theta\\left(\\mathrm{chosen} | \\mathrm{prompt}\\right)}{\\pi_\\mathrm{SFT}\\left(\\mathrm{chosen} | \\mathrm{prompt}\\right)} - \\beta \\log \\frac{\\pi_\\theta\\left(\\mathrm{rejected} | \\mathrm{prompt}\\right)}{\\pi_\\mathrm{SFT}\\left(\\mathrm{rejected} | \\mathrm{prompt}\\right)} \\right)\n",
    "$$\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>Hint Step 1</summary>\n",
    "This is the same forward operation with another model. Friendly reminder: this model is frozen.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint Step 2</summary>\n",
    "In our batch, best answers are directly followed by their worse counterparts. The `extract` method might help you.\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint Step 3</summary>\n",
    "This operation is not very complicated, but it can be very tedious to code, so look for an interesting method :)\n",
    "</details>\n",
    "<details>\n",
    "<summary>Hint Step 4</summary>\n",
    "Computed probabilities are in loss scale. Return an average of the loss.\n",
    "</details>\n",
    "<details><summary>Solution</summary>\n",
    "Execute one of these depending on your step:\n",
    "\n",
    "```\n",
    "%load -s dpo_loss solutions/criterion_dpo_loss_step1.py\n",
    "%load -s dpo_loss solutions/criterion_dpo_loss_step2.py\n",
    "%load -s dpo_loss solutions/criterion_dpo_loss_step3.py\n",
    "%load -s dpo_loss solutions/criterion_dpo_loss_step4.py\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8983c146-380e-4295-838e-47975adb2a7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dpo_loss(\n",
    "    self,\n",
    "    logits: torch.Tensor,\n",
    "    targets: torch.Tensor,\n",
    "    inputs: torch.Tensor,\n",
    "    attention_mask: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    # Step 1: Compute logits for the reference model (available at self.base_model)\n",
    "    reference_logits = ...\n",
    "\n",
    "    # Step 2: Separate best answers from worst answers\n",
    "    best_pi_logits, worst_pi_logits = ...\n",
    "    best_ref_logits, worst_ref_logits = ...\n",
    "    best_pi_labels, worst_pi_labels = ...\n",
    "\n",
    "    # Step 3: Compute probabilities from logits (in log scale)\n",
    "    best_pi_logps = ...\n",
    "    best_ref_logps = ...\n",
    "    worst_pi_logps = ...\n",
    "    worst_ref_logps = ...\n",
    "\n",
    "    # Step 4: Compute the final loss from all probabilities\n",
    "    loss = ...\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc26db-9e5a-4158-b2cd-e040b3623c64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "(input_ids, attention_mask, target) = next(iter(dataloader))\n",
    "input_ids, attention_mask, target = input_ids.to(device), attention_mask.to(device), target.to(device)\n",
    "with torch.no_grad():\n",
    "    logits = model(input_ids, attention_mask=attention_mask).logits\n",
    "Criterion.dpo_loss = dpo_loss\n",
    "print(criterion(logits=logits, targets=target, inputs=input_ids, attention_mask=attention_mask, mode=\"dpo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a07e29-96ca-49f7-acf2-038514aa4a7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(model=model, epochs=1, dataloader=dataloader, mode=\"dpo\", debug=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acb5606-4af6-4af7-a015-ac1d3681e22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcq(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be74a5f0-0a32-46f0-a087-48066c21aaf4",
   "metadata": {},
   "source": [
    "## Real training\n",
    "\n",
    "Now we will try to do the real training process (and not just checking that it runs one iteration). With the GPU available and the tiny dataset used, it should take around 4 minutes for DPO to be performed. SFT would need to be done on the whole dataset (more than one hour of training), hence why we use a model which has already done this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7d12cc-bcb2-4d77-a64f-3701248876b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train(model=model, epochs=3, dataloader=dataloader, mode=\"sft\", debug=False) # This would take several hours if done properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c49258a-73cf-440d-be2c-15589bec429f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model=model, epochs=1, dataloader=dataloader, mode=\"dpo\", debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5796ed6f-8f4b-4f15-a3f0-c28f21d19992",
   "metadata": {},
   "source": [
    "What the model was generating before DPO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd3d1f-36e7-47d9-8798-70abfa4f6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(criterion.base_model, \"What is the best way to rob a bank ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e6754-42d3-4f06-8c4d-6e87d8196c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(criterion.base_model, \"How do I kill someone ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e05da4-e7ac-450a-833a-418518154712",
   "metadata": {},
   "source": [
    "What the model generates now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1822347f-e0cd-456c-b1a0-185f0685a6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(model, \"What is the best way to rob a bank ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9e6be3-bce5-4dcd-b1ba-9d83d3bdc07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation(model, \"How do I kill someone ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5010a-6b8c-4e8b-ae26-3f2198bedbde",
   "metadata": {},
   "source": [
    "You should see a difference in the given outputs. Considering the training is very short, don't hesitate to re-run the generation if one happens to give an unsatisfying result. Normally you should have pretty good results on average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9b3ab5-f913-44b3-87db-8218c878028a",
   "metadata": {},
   "source": [
    "You can visualize the result in terms of generation, after a DPO training, whether it is on your own fine-tuned network, or networks previously fine-tuned by me.\n",
    "\n",
    "__Note__: If you are doing a complete training, executing what follows could lead you to a CUDA Out Of Memory. So do it after training your network.\n",
    "\n",
    "In the TP_DPO folder, you also have another folder `gradio_compare_network`. To use it, follow the instructions in the `INSTRUCTIONS.ipynb` above. It gives an API to test a few models more cleanly than the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f68e4-87e5-4e72-9e6e-391fe6287bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
