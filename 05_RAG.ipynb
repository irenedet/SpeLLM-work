{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95420588-96d4-41eb-96c3-d194d973984c",
   "metadata": {},
   "source": [
    "# **Retrieval Augmented Generation (RAG)**\n",
    "\n",
    "In this hands-on tutorial, we will explore the concept of Retrieval Augmented Generation (RAG) and apply it to the [IDRIS Documentation](http://www.idris.fr/) database. RAG is a powerful technique that combines retrieval of information from a database with language generation models (LLMs) to enhance the generation process.\n",
    "\n",
    "We will leverage the [Langchain](https://www.langchain.com/) library to implement RAG and demonstrate its capabilities in the context of the IDRIS Documentation. By retrieving relevant information from a database, we can provide additional context and knowledge to the LLM, enabling it to generate more accurate and informative responses.\n",
    "\n",
    "Let's dive into the world of RAG and discover how it can revolutionize the generation process in natural language understanding and generation tasks.\n",
    "\n",
    "![image](./images/rag.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a0ed0-59a8-43ec-a880-dfcf4bdf3d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import os\n",
    "import datasets\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from utils import seed_everything, clean_idris_doc\n",
    "import numpy as np\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import BSHTMLLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "DSDIR = Path(os.environ['DSDIR'])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18f2da-9eea-408e-b52d-57908f153fc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Create Vector DataBase**\n",
    "In this section, we will create a Vector Database (VDB) from the IDRIS documentation. The IDRIS documentation consists of HTML files that were scraped from the IDRIS website using the `urls_to_html.py` script in the [Use-Case repository](https://github.com/idriscnrs/SpeLLM-Use-Case). The VDB will serve as the foundation for the Retrieval Augmented Generation (RAG) technique. The VDB will store vector representations of the documents, allowing us to efficiently retrieve relevant information during the generation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019a260-fdf4-4a32-bfb4-27c123d7488f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOC_HTML_PATH = DSDIR / \"idris_doc_html\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69166c93-1354-40d6-89fd-8ec4875b7767",
   "metadata": {},
   "source": [
    "Let's explore the IDRIS documentation database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fc607c-c96d-4a47-bf26-c24d3b72ce54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path in DOC_HTML_PATH.iterdir():\n",
    "    print(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b59052-0361-4964-a046-fda99fbc5328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for path in (DOC_HTML_PATH / \"jean-zay/gpu\").iterdir():\n",
    "    print(path.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc754b39-400e-491f-ba62-bc7fd5e37f33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(DOC_HTML_PATH / \"support_avance.html\").read_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5814f486",
   "metadata": {},
   "source": [
    "Feel free to explore the database in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483268f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "714450ad-f138-46c2-9f83-c126bd9d5c4a",
   "metadata": {},
   "source": [
    "We can use the [`BSHTMLLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/html#loading-html-with-beautifulsoup4) class to load an HTML file into a langchain document. This class automatically extracts the text from the HTML file using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d740267-8dd4-4663-aa81-d2e50ce94a71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = BSHTMLLoader(DOC_HTML_PATH / \"support_avance.html\")\n",
    "doc = loader.load()[0]\n",
    "print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a151b31-fa1d-4205-ac23-cc5bcfae1df9",
   "metadata": {},
   "source": [
    "We can use the function `clean_idris_doc` (regex functions) to remove the artefact from the IDRIS webpage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0338d7f2-268a-41a7-8bf4-2078b16a1ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clean_idris_doc(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfac414-4d5e-4150-b979-12ede7ed3da1",
   "metadata": {},
   "source": [
    "The [`DirectoryLoader`](https://python.langchain.com/docs/modules/data_connection/document_loaders/file_directory) class from langchain can iterate over a directory and load the files using a specific method. We will use it to load every HTML file from our database with the `BSHTMLLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0166a4a-1d56-4a7f-9d8e-639d0d63f89d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(\n",
    "    DOC_HTML_PATH, glob=\"**/*.html\", loader_cls=BSHTMLLoader\n",
    ")\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e27196-8332-4faf-8dd8-4404c935c46b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(clean_idris_doc(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f361724-f449-4850-a4ef-d375e41326cf",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span>  Create a function that loads every HTML file into Langchain documents. The content of each document should be extracted from the HTML file and cleaned using the `clean_idris_doc` function. The function should return a list of Langchain documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14012c-c894-4bfd-91b9-985bc7c12231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "199ab1df-0ebb-42d8-aff2-944b9e59addf",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73f2ecca-4dd8-4a99-8cfe-67d6cd071d4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def create_docs(source_html_path):\n",
    "    \"\"\"Load every .html file in the source directory into a langchain document\"\"\"\n",
    "############ Complete here ############\n",
    "    loader = \n",
    "#######################################\n",
    "    \n",
    "    docs = loader.load()\n",
    "\n",
    "############ Complete here ############    \n",
    "    for doc in docs:\n",
    "        \n",
    "#######################################\n",
    "\n",
    "    print(f\"{len(docs)} html files loaded\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af514753-5038-4efb-9315-99a94d94c72c",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "02067497-7b11-488e-a144-b253095121f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def create_docs(source_html_path):\n",
    "    \"\"\"Load every .html file in the source directory into a langchain document\"\"\"\n",
    "############ Complete here ############\n",
    "    loader = DirectoryLoader(\n",
    "        source_html_path, glob=\"**/*.html\", loader_cls=\n",
    "    )\n",
    "#######################################\n",
    "    \n",
    "    docs = loader.load()\n",
    "\n",
    "############ Complete here ############    \n",
    "    for doc in docs:\n",
    "        doc.page_content = \n",
    "#######################################\n",
    "\n",
    "    print(f\"{len(docs)} html files loaded\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd07c523-f31e-4b06-a104-490618880701",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e833d004-77f4-40fa-b9ac-5a9e206be014",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def create_docs(source_html_path):\n",
    "    \"\"\"Load every .html file in the source directory into a langchain document\"\"\"\n",
    "    loader = DirectoryLoader(\n",
    "        source_html_path, glob=\"**/*.html\", loader_cls=BSHTMLLoader\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc.page_content = clean_idris_doc(doc.page_content)\n",
    "\n",
    "    print(f\"{len(docs)} html files loaded\")\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6e999a",
   "metadata": {},
   "source": [
    "**Test it here:**<br>\n",
    "~1-2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25549f02-5306-4617-ba9f-4c655d7de791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = create_docs(DOC_HTML_PATH)\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccbc145",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcaae65-ce49-4504-ad0f-b5f8d63767e2",
   "metadata": {},
   "source": [
    "### **Split documents into chunks**\n",
    "\n",
    "To make it easier to retrieve relevant information, we need to split the documents into smaller chunks. This allows us to have more granular control over the retrieval process and ensures that the generated responses are more accurate and informative. By breaking down the documents into smaller units, we can effectively match the user's query with the most relevant chunks of information. This step is crucial in optimizing the retrieval process and enhancing the overall performance of the system.\n",
    "\n",
    "![image](./images/rag_split.jpg)\n",
    "\n",
    "For that, we will use [RecursiveCharacterTextSplitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter) of Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdae19a-9d9c-44be-ae4b-816fc4226bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_example = docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a51e0c-3950-4cda-9439-a9022e095d80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(doc_example.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baca89b-23be-420e-b6e1-9d42c1564db2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=10,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\",  \" \", \"\",]\n",
    ")\n",
    "splitted_docs = splitter.split_documents([doc_example])\n",
    "\n",
    "for doc in splitted_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd42592-8c63-4fc1-b732-42020205397c",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Experiment with different combinations to optimize the future retrieval process. Discuss the results and seek guidance from an instructor to further enhance the performance of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d9ed1-5490-4fbc-b306-1ff1da0e2241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0571732",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc5eb7",
   "metadata": {},
   "source": [
    "For the rest of the hands-on, we are going to use the following configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6ac7e-7cad-4e52-8977-70ded057112e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", r\"(?<=\\. )\",  \" \", \"\",]\n",
    ")\n",
    "splitted_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f7d73-3caf-4dba-9772-be2fad353317",
   "metadata": {},
   "source": [
    "### **Create a vector database from splitted documents**\n",
    "\n",
    "![image](./images/index.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0119c6ce-ec62-44db-8f3c-221b7132490a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HF_MODELS_PATH = DSDIR / \"HuggingFace_Models\"\n",
    "EMBEDDING_PATH = HF_MODELS_PATH / \"intfloat/multilingual-e5-large\"\n",
    "VDB_PATH = Path(\"./chroma_vdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c64644-98c7-455e-9027-c22f40ca3245",
   "metadata": {},
   "source": [
    "First, we will load an embedding model. We chose [multilingual-e5-large](https://huggingface.co/intfloat/multilingual-e5-large) because we need an embedding model that was trained on French and English. We will use the [`HuggingFaceEmbeddings`](https://api.python.langchain.com/en/latest/embeddings/langchain_community.embeddings.huggingface.HuggingFaceEmbeddings.html) from langchain, as it is easier to integrate with the langchain vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ffb50b-32f0-49b4-9e94-22b645eddc63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = HuggingFaceEmbeddings(\n",
    "    model_name=str(EMBEDDING_PATH),  # Does not accept Path\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e2697",
   "metadata": {},
   "source": [
    "Let's test the embedding by calculating the dot product of different vector embeddings for different sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a25c82-b1ae-427a-a372-3321bad6d379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentence1 = \"This is a cat.\"\n",
    "sentence2 = \"This is a dog.\"\n",
    "sentence3 = \"I like train.\"\n",
    "embedding1 = embedding.embed_query(sentence1)\n",
    "embedding2 = embedding.embed_query(sentence2)\n",
    "embedding3 = embedding.embed_query(sentence3)\n",
    "print(np.dot(embedding1, embedding2))\n",
    "print(np.dot(embedding1, embedding3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12276e",
   "metadata": {},
   "source": [
    "Feel free to experiment more with the embedding model and explore its capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c54a17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef08eea0-8a99-4df6-b80f-a6253ba40317",
   "metadata": {},
   "source": [
    "With the embedding model, we can create a vector database from the splitted documents. The database will be a [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) database, which is the easiest to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3471b-3c04-4b17-a21d-0305b267aa58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vdb(docs, embedding, vdb_path):\n",
    "        \"\"\"Create a vector database from the documents\"\"\"\n",
    "\n",
    "        if vdb_path.exists():\n",
    "            if any(vdb_path.iterdir()):\n",
    "                raise FileExistsError(\n",
    "                    f\"Vector database directory {vdb_path} is not empty\"\n",
    "                )\n",
    "        else:\n",
    "            vdb_path.mkdir(parents=True)\n",
    "\n",
    "        vectordb = Chroma.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embedding,\n",
    "            persist_directory=str(vdb_path),  # Does not accept Path\n",
    "        )\n",
    "        vectordb.persist()  # Save database to use it later\n",
    "\n",
    "        print(f\"vector database created in {vdb_path}\")\n",
    "        return vectordb\n",
    "\n",
    "vectordb = create_vdb(splitted_docs, embedding, VDB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414ef7ed",
   "metadata": {},
   "source": [
    "We do not have to create the database every time we want to use it. We can simply reuse the Chroma vector database that we have already set up with the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d04c88b-824f-4c56-8563-59f32ec376fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma(\n",
    "    embedding_function=embedding,\n",
    "    persist_directory=str(VDB_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215dc467-3644-406e-99dd-3fad6a1d2f60",
   "metadata": {},
   "source": [
    "## **Generation with retrieved informations**\n",
    "\n",
    "In this section, we will explore how to enhance the generation process of the LLM by incorporating retrieved information from the vector database. By leveraging the Retrieval Augmented Generation (RAG) technique, we can provide additional context and knowledge to the LLM, resulting in more accurate and informative responses.\n",
    "\n",
    "Now that we have a vector database, we can retrieve relevant information based on the user's query and use it to enrich the input of the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d358c15c",
   "metadata": {},
   "source": [
    "First, let's try to query the database with a simple sentence to check the retrieved information:\n",
    "![image](./images/retrieval.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a9c789",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = \"What are the available training ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0bc97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vectordb.similarity_search(query, k=6)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92330fb5",
   "metadata": {},
   "source": [
    "In the previous cell, we demonstrated the simplest method to retrieve information from the vector database. However, there are other retrieval algorithms that can be used, such as [Maximum Marginal Relevance (MMR)](https://python.langchain.com/docs/modules/model_io/prompts/example_selector_types/mmr). \n",
    "\n",
    "In our case, it is relevant to use the MMR algorithm because we have both a French and an English version of our documentation. Therefore, we want to select only one language when retrieving information, rather than having information in both languages.\n",
    "\n",
    "By using the MMR algorithm, we can prioritize the most relevant and informative documents in a single language, enhancing the accuracy and coherence of the generated responses.\n",
    "\n",
    "![image](./images/mmr.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115451bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = vectordb.max_marginal_relevance_search(query, k=6, fetch_k=10)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35e079",
   "metadata": {},
   "source": [
    "Let's see how the LLM will answer without using RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca37a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/microsoft/phi-2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,  # Allow using code that was not written by HuggingFace\n",
    "    attn_implementation=\"flash_attention_2\"  # Optimize the model with Flash Attention\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feb638b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generation(prompt, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "    model_inp = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "    print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8038d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation(query, do_sample=False, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d5f156",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Write a function named `rag_generation` that utilizes the Retrieval Augmented Generation (RAG) technique to generate answers to questions. The function should take into account the different elements we have explored previously. We advise you to use templates to format your prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1100da8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a993497d",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f3267dc5-b75f-49d1-92dc-88bbf5a761e5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def rag_generation(query, k=3, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt after rag and print it.\"\"\"\n",
    "############ Complete here ############\n",
    "    docs = \n",
    "    retrieved_infos = \n",
    "#######################################\n",
    "    \n",
    "############ Complete here ############\n",
    "    text_input = f\"Avec les informations suivantes: { }\\nRépond à cette question: { }\"\n",
    "#######################################\n",
    "\n",
    "    model_inp = tokenizer(text_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_nb_tokens = model_inp['input_ids'].shape[1]\n",
    "\n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "\n",
    "    print(f\"LLM input:\\n{text_input}\\n\" + \"#\"*50)\n",
    "    print(f\"LLM output:\\n{tokenizer.decode(out[0][input_nb_tokens:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52573eee",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6106f9e1-867d-458b-93e6-15147f003b53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def rag_generation(query, k=3, fetch_k=6, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt after rag and print it.\"\"\"\n",
    "############ Complete here ############\n",
    "    docs = vectordb.max_marginal_relevance_search( , k=k, fetch_k=fetch_k)\n",
    "    retrieved_infos = \" \".join([ for doc in docs])\n",
    "#######################################\n",
    "    \n",
    "############ Complete here ############\n",
    "    text_input = f\"Avec les informations suivantes: { }\\nRépond à cette question: { }\"\n",
    "#######################################\n",
    "\n",
    "    model_inp = tokenizer(text_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_nb_tokens = model_inp['input_ids'].shape[1]\n",
    "\n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "\n",
    "    print(f\"LLM input:\\n{text_input}\\n\" + \"#\"*50)\n",
    "    print(f\"LLM output:\\n{tokenizer.decode(out[0][input_nb_tokens:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5723e7",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eca9394f-dedb-4f37-a0b9-ac7ef472ff62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def rag_generation(query, k=3, fetch_k=6, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt after rag and print it.\"\"\"\n",
    "    docs = vectordb.max_marginal_relevance_search(query, k=k, fetch_k=fetch_k)\n",
    "    retrieved_infos = \" \".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    text_input = f\"With the following informations: {retrieved_infos}\\nAnswer this question: {query}\"\n",
    "\n",
    "    model_inp = tokenizer(text_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_nb_tokens = model_inp['input_ids'].shape[1]\n",
    "\n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "\n",
    "    print(f\"LLM input:\\n{text_input}\\n\" + \"#\"*50)\n",
    "    print(f\"LLM output:\\n{tokenizer.decode(out[0][input_nb_tokens:])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254e656",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585ba7ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_generation(query, k=4, fetch_k=8, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320ed224",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8131a92-e243-4ca1-9c82-9f58d5450482",
   "metadata": {},
   "source": [
    "## **Bonus: Re-ranking**\n",
    "In the context of the vector database retrieval process, re-ranking is an additional step that can be performed to enhance the accuracy of the retrieved information. While the initial retrieval pass focuses on speed, re-ranking utilizes a slower but more accurate model to score the similarity between the queries and the retrieved information.\n",
    "\n",
    "By applying re-ranking, we can refine the ranking of the retrieved information based on a more comprehensive analysis. This helps to ensure that the most relevant and informative results are presented to the user.\n",
    "\n",
    "It is important to note that re-ranking is an optional step and its implementation depends on the specific requirements of the project. It can be particularly useful in scenarios where precision and accuracy are of utmost importance, such as in information retrieval systems or question-answering applications.\n",
    "\n",
    "The combination of the initial retrieval pass and the re-ranking step provides a powerful approach to optimize the retrieval process and enhance the overall performance of the system.\n",
    "\n",
    "![image](./images/rerank.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e2084",
   "metadata": {},
   "source": [
    "In this bonus section, we will see a simple way to use the [BGE reranker large model](https://huggingface.co/BAAI/bge-reranker-large) to improve the process we defined previously. Note that the model was trained on English and Chinese, so it is not relevant to use it for retrieving French information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fc6f0-244b-4b5b-9887-8e68948526d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the reranker model and its tokenizer\n",
    "rerank_tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/BAAI/bge-reranker-large\")\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained(DSDIR / \"HuggingFace_Models/BAAI/bge-reranker-large\")\n",
    "rerank_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e3ae10",
   "metadata": {},
   "source": [
    "Let's try the reranker model on two basic examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636198e9-4794-4a5e-a355-3ac194d86b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    [\"What are the available training ?\", \"To find out the dates of the sessions scheduled for these different training courses, consult the website: https://cours.idris.fr/\"],\n",
    "    [\"What are the available training ?\", \"To use the A100 GPUs, you must first load the cpuarch/amd.\"]\n",
    "]\n",
    "with torch.no_grad():\n",
    "    inputs = rerank_tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "    scores = rerank_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fa4168",
   "metadata": {},
   "source": [
    "Now we will define the retrieval process. We will simply use the function we defined previously and enhance it with reranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076b2af-0fd4-41ca-8419-37df63e8420c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rag_rerank_generation(query, k=10, rerank=4, **gen_parameters):\n",
    "    \"\"\"Generate text from a prompt after rag and print it.\"\"\"\n",
    "    docs = vectordb.similarity_search(query, k=k)\n",
    "    \n",
    "    # Re-rank\n",
    "    rerank_inp = [[query, doc.page_content] for doc in docs]\n",
    "    with torch.no_grad():\n",
    "        inputs = rerank_tokenizer(rerank_inp, padding=True, truncation=True, return_tensors='pt', max_length=512)\n",
    "        scores = rerank_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        \n",
    "    _, indices = scores.topk(rerank)\n",
    "    \n",
    "    retrieved_infos = \" \".join([docs[idx].page_content for idx in range(len(docs)) if idx in indices])\n",
    "    \n",
    "    text_input = f\"With the following informations: {retrieved_infos}\\nAnswer this question: {query}\\nAnswer:\"\n",
    "\n",
    "    model_inp = tokenizer(text_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_nb_tokens = model_inp['input_ids'].shape[1]\n",
    "    \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], **gen_parameters)\n",
    "    \n",
    "    print(f\"LLM input:\\n{text_input}\\n\" + \"#\"*50)\n",
    "    print(f\"LLM output:\\n{tokenizer.decode(out[0][input_nb_tokens:])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae068dc7-2e47-4024-90a4-a751715dc4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rag_rerank_generation(query, k=10, rerank=4, max_new_tokens=53)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.1_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.1.1_py3.11.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
