{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a76d619-66aa-4e8f-b2ad-4e0d52f6ca0e",
   "metadata": {},
   "source": [
    "# **Parameter Efficient Fine-Tuning**\n",
    "In this hands-on tutorial, we will explore the concept of Parameter Efficient Fine-Tuning (PEFT) and learn how it can significantly improve the efficiency of fine-tuning training. PEFT involves training only a specific part of the model or an adapter, resulting in faster and more resource-efficient training.\n",
    "\n",
    "To demonstrate the effectiveness of PEFT, we will use the same training scenario as the first hands-on tutorial of SpeLLM. By comparing the performance of PEFT with traditional fine-tuning techniques, we can gain insights into the benefits and trade-offs of using PEFT in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbc97f0-b5a9-4d05-8d86-a410170dbe9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from utils import seed_everything\n",
    "import re\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "DSDIR = Path(os.environ['DSDIR'])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a0416f-9af0-47f1-8871-6e19bf37d525",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Freezing Layers: RoBERTa on classification task**\n",
    "In this section, we will explore the concept of freezing layers in the RoBERTa model for a classification task (first part of first SpeLLM hands-on). Freezing layers refers to the process of preventing certain layers of the model from being updated during training. This can be useful when we want to fine-tune only specific parts of the model or when we have limited computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aed012-ee85-4223-9c4b-8b9d400bd00a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/FacebookAI/roberta-base\", num_labels=2\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effd4a1c-f47b-4e8b-b3f0-25d3e417bb19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "imdb_dataset = datasets.load_from_disk(DSDIR / \"HuggingFace/imdb/plain_text\")\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx) -> tuple[str, torch.Tensor]:\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "        hf_element = self.hf_dataset[idx]\n",
    "        \n",
    "        model_inp = hf_element[\"text\"]\n",
    "        label = hf_element[\"label\"]\n",
    "        \n",
    "        return model_inp, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_list = [element[0] for element in batch]\n",
    "    label_list = [element[1] for element in batch]\n",
    "    \n",
    "    model_inp = tokenizer(\n",
    "        text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    label_tens = torch.LongTensor(label_list)\n",
    "    return model_inp, label_tens\n",
    "\n",
    "dataset = IMDBDataset(imdb_dataset[\"train\"])\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167d431-5f5d-4894-bf93-3d1ed85f8180",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize training\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "\n",
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        out = model(**model_inp)\n",
    "\n",
    "        loss = criterion(out.logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097263c-45d8-4e46-8b23-1cb0963e6c01",
   "metadata": {},
   "source": [
    "To estimate the time of one epoch and the GPU memory consumption of the fine-tuning as we saw it on the first hands-on, let's run a few iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4aa695-82d6-4a9c-bfea-afc9941521af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop(model, dataloader, criterion, optimizer, test=True)\n",
    "print(f\"Max memory: {torch.cuda.max_memory_allocated(device='cuda')/(1024**3)}\")\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2f495-2a69-4752-a424-ebcb83661945",
   "metadata": {
    "tags": []
   },
   "source": [
    "To optimize GPU memory usage, we can freeze specific parts of the model. Before doing so, let's examine the model's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ff6f32-f685-4b59-be8a-81393d82e35d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830f36e-05f3-4ee7-b635-f2087d4e54fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ecd2a-713b-484c-ab31-856ddc24b1dc",
   "metadata": {},
   "source": [
    "We can inspect the parameters of a specific layer using the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c41d68-2f7e-4eb4-83e0-c449dd732399",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[0].attention.self.query.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2835f-1104-450a-ba59-3af1eeb6fbbc",
   "metadata": {},
   "source": [
    "The requires_grad=True indicates that gradients will be computed during training, allowing for weight updates. To freeze a specific part of the model, we can set these parameters to False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad60998-bed3-4379-a8c6-3e7c507e4386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[0].attention.self.query.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab3c1c4-25a6-4f34-8213-538f46465303",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can define a function that freezes all parameters of a HuggingFace model up to a specified layer. This function utilizes regular expressions to target the embedding parameters and all layer parameters leading up to the specified layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d699ffd-cbfb-46f4-8dff-38709e34a2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def freeze_layers(model, nb_freeze_layer = 4):\n",
    "    for name, params in model.named_parameters():\n",
    "        if re.search(r\"embed\", name) is not None:\n",
    "            params.requires_grad = False\n",
    "        elif re.search(r\"\\.(\\d+)\\.\", name) is not None:\n",
    "            if (\n",
    "                int(re.search(r\"\\.(\\d+)\\.\", name).group(1)) < nb_freeze_layer\n",
    "            ):\n",
    "                params.requires_grad = False\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02acf9-1c78-44fb-bfd2-43a6d0f0bdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = freeze_layers(model, nb_freeze_layer = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450914e-a0df-44d1-8055-0261dd733cb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[0].attention.self.query.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7565642-c2f8-42a5-a679-1b0f5b3544bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[7].attention.self.query.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98baf02d-3931-4e4a-952d-bf3aab70bb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.roberta.encoder.layer[8].attention.self.query.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9c46f3-8a06-4dd3-aa2a-520bd6221d7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "To estimate the time required for one epoch and the GPU memory consumption during training, let's run a few iterations again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973285d7-8500-402b-97d3-d02301c86c86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8230a7ca-e566-4797-9f2d-77ed3d3ac032",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop(model, dataloader, criterion, optimizer, test=True)\n",
    "print(f\"Max memory: {torch.cuda.max_memory_allocated(device='cuda')/(1024**3)}\")\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36da7cb3-fa1e-4363-ac47-f3bad0f4ddfc",
   "metadata": {},
   "source": [
    "## **LoRA: Phi-2 as a chatbot**\n",
    "In this section, we will explore the application of LoRA (Low Rank Adaptation) on Phi-2 to fine-tune it efficiently for the roleplay dataset. LoRA is a technique that allows us to train only an adapter, resulting in faster and more resource-efficient training. By applying LoRA on Phi-2, we can leverage its powerful language representation capabilities and adapt it specifically for chatbot tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9c41b2-82e1-4e01-8509-c0568d8a8a1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/microsoft/phi-2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,  # Allow using code that was not written by HuggingFace\n",
    "    attn_implementation=\"flash_attention_2\"  # Optimize the model with Flash Attention\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794bfab5-b4f9-45f7-b426-20a353a5ec78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "roleplay_dataset = datasets.load_from_disk(DSDIR / \"HuggingFace/hieunguyenminh/roleplay\")\n",
    "\n",
    "\n",
    "def count_tokens(hf_dataset, tokenizer):\n",
    "    total_tokens = 0\n",
    "    loop = tqdm(hf_dataset)\n",
    "    for element in loop:\n",
    "        nb_token_element = len(tokenizer(element['text'])[\"input_ids\"])\n",
    "        total_tokens += nb_token_element\n",
    "        \n",
    "        loop.set_postfix(tokens_count=total_tokens)\n",
    "        \n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "nb_tokens = count_tokens(roleplay_dataset['train'], tokenizer)\n",
    "\n",
    "\n",
    "class RoleplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024, nb_tokens=3160542):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.separator = tokenizer.eos_token_id  # The token that will seperate different sample\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.idx_iterator = iter(random.sample(range(len(hf_dataset)), len(hf_dataset)))\n",
    "        self.seq_length = seq_length\n",
    "        self.nb_tokens = nb_tokens\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.nb_tokens // self.seq_length\n",
    "\n",
    "    def get_next_sample(self):\n",
    "            \"\"\"Retrieves the next sample from the dataset and tokenize it.\"\"\"\n",
    "            idx = next(self.idx_iterator)\n",
    "            text = self.hf_dataset[idx][\"text\"]\n",
    "            return self.tokenizer(text)['input_ids'] + [self.separator]\n",
    "\n",
    "    def __iter__(self):\n",
    "        next_sample_ids = None\n",
    "        all_token_ids = []\n",
    "        idx = 0\n",
    "\n",
    "        while idx < self.__len__():\n",
    "            if next_sample_ids is None:\n",
    "                next_sample_ids = self.get_next_sample()\n",
    "\n",
    "            if len(all_token_ids) + len(next_sample_ids) <= self.seq_length:\n",
    "                # if the next HF_dataset sample can fit in the current dataset sample\n",
    "                # we add it\n",
    "                all_token_ids += next_sample_ids\n",
    "                next_sample_ids = None\n",
    "                \n",
    "            else:\n",
    "                # if the next HF_dataset sample can't fit in the current dataset\n",
    "                # sample, we add what we can in the dataset sample and then we yield it\n",
    "                # note: we add one more element compared to seq_length to return to\n",
    "                # seq_length when generating inputs and targets (see train_collate())\n",
    "                idx_break = self.seq_length - len(all_token_ids)\n",
    "                all_token_ids += next_sample_ids[: idx_break + 1]\n",
    "                next_sample_ids = next_sample_ids[idx_break + 1 :]\n",
    "                \n",
    "                model_inp = torch.tensor(all_token_ids[:-1], dtype=torch.int64)\n",
    "                labels = torch.tensor(all_token_ids[1:], dtype=torch.int64) \n",
    "                yield model_inp, labels\n",
    "\n",
    "                all_token_ids = []\n",
    "                idx += 1\n",
    "\n",
    "\n",
    "dataset = RoleplayDataset(tokenizer, roleplay_dataset['train'], seq_length=512, nb_tokens=nb_tokens)\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e0cb8-4845-4b57-b740-492582eeba70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare training\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)\n",
    "\n",
    "\n",
    "def prepare_for_loss(logits, labels):\n",
    "    \"\"\"Unfold the Tensors to compute the CrossEntropyLoss correctly\"\"\"\n",
    "    batch_size, seq_length, vocab_size = logits.shape\n",
    "    logits = logits.view(batch_size * seq_length, vocab_size)\n",
    "    labels = labels.view(batch_size * seq_length)\n",
    "    return logits, labels\n",
    "\n",
    "\n",
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        logits = model(model_inp).logits\n",
    "\n",
    "        logits, labels = prepare_for_loss(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd47d53c-7983-48d7-a53c-54247d10ef06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop(model, dataloader, criterion, optimizer, test=True)\n",
    "print(f\"Max memory: {torch.cuda.max_memory_allocated(device='cuda')/(1024**3)}\")\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a0d72-2bd9-4ba3-a6fe-65a1938383a1",
   "metadata": {},
   "source": [
    "Now, before applying LoRA on Phi-2, we need to identify the linear layers we want to target. One common approach is to focus on the linear layer of the attention mechanism, which transforms the input into K, Q, and V matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2c372e-88cd-4c90-86ec-2aa1ec78b905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa5cdf-52c8-4230-b0db-a29281f2a3d1",
   "metadata": {},
   "source": [
    "Now we can apply LoRA on the model. It is important to note that `r` represents the rank of the LoRA adapters, while `lora_alpha`, `lora_dropout`, and `bias` are hyperparameters that need to be considered (we use common value here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e00221-1239-485a-9a16-0bf34cc289db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.config.use_cache = False  # People advised me to do that, I don't remember why "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250edc23-6a95-4ab9-92b6-9f1b41d5ea1c",
   "metadata": {},
   "source": [
    "To estimate the time required for one epoch and the GPU memory consumption during training, let's run a few more iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8a467-634d-4194-804a-156d3348c4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ae334-ed09-4207-8150-7a5e9026c08e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loop(model, dataloader, criterion, optimizer, test=True)\n",
    "print(f\"Max memory: {torch.cuda.max_memory_allocated(device='cuda')/(1024**3)}\")\n",
    "torch.cuda.reset_peak_memory_stats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-gpu-2.1.1_py3.11.5",
   "language": "python",
   "name": "module-conda-env-pytorch-gpu-2.1.1_py3.11.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
