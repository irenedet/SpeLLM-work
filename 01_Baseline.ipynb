{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe4d2039-1e9c-4740-a2db-f41fe0f7f93b",
   "metadata": {},
   "source": [
    "# **Classic Fine-Tuning**\n",
    "In this hands-on exercise, we will be fine-tuning different models for various tasks using classical fine-tuning. Classical fine-tuning is a common approach to establish a solid baseline for model specialization performance.\n",
    "\n",
    "The goal of fine-tuning is to take a pre-trained model and adapt it to a specific task or dataset. By leveraging the knowledge and representations learned from a large-scale pre-training task, we can achieve better performance on downstream tasks with less training data.\n",
    "\n",
    "This notebook is divided in 3 parts:\n",
    "- Classification: IMDB Dataset with RoBERTa\n",
    "- Chatbot: Roleplay Dataset (Chatbot) with Phi-2\n",
    "- Summarization: SciTLDR dataset with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee529f5-a7af-43d7-906f-a9ceadbd88f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "from utils import seed_everything\n",
    "from jupyterquiz import display_quiz\n",
    "import json\n",
    "\n",
    "\n",
    "DSDIR = Path(os.environ['DSDIR'])\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "quiz_path = Path(\"./quiz/finetune.json\")\n",
    "quiz = json.loads(quiz_path.read_text())\n",
    "\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d57b9-2869-4f71-b9e9-787c058479d6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609cb503-06a2-4936-bc4c-e792dd539095",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Classification: IMDB Dataset with RoBERTa**\n",
    "The [IMDB dataset](https://huggingface.co/datasets/imdb) is a collection of movie reviews labeled with binary sentiment classification (positive or negative). In this task, we will be using the RoBERTa model, which is a pre-trained model on the English language. RoBERTa (Encoder model) utilizes a masked language modeling (MLM) objective, similar to BERT. Our goal is to fine-tune the [RoBERTa base](https://huggingface.co/FacebookAI/roberta-base) model on the IMDB dataset, enabling it to accurately classify movie reviews based on sentiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a08d8f-a0a1-4e01-8a83-d592b09676da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Exploratory Data Analysis**\n",
    "Before diving into the fine-tuning process, it's important to perform an exploratory data analysis (EDA) on the dataset. This will help us understand the structure and characteristics of the data, as well as determine the appropriate input and output variables for training our Auto-Encoding Model.\n",
    "\n",
    "Here we will make a very quick EDA, but keeps in mind that this part is essential and you should take more time on it than what we will do during this excersise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d23d7-8ab7-40e0-be70-79797f817cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "imdb_dataset = datasets.load_from_disk(DSDIR / \"HuggingFace/imdb/plain_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad64fc-4763-444e-9661-656a51497121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the splits of the dataset\n",
    "imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab623b5-a98f-4fb6-85d8-d60461c69b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the number of elements of the dataset and the features\n",
    "imdb_dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719974f2-9cbc-42c3-84c0-7184bc93cefe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the format of the datset elements\n",
    "imdb_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3cd08d-71d9-410c-a57a-ef5d1296d98c",
   "metadata": {},
   "source": [
    "Feel free to explore the dataset in more depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571cb501-43ac-4c63-b3bd-58c5163f2fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e436af-0552-49ec-8f49-e842ead5f84a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Discover the Model and its Tokenizer**\n",
    "\n",
    "In this section, we will explore the configurations of the RoBERTa model and its tokenizer. We will utilize the transformers library to import the model and examine the input and output formats of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a95a1c-d80d-45cc-b782-9e3ed205f880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/FacebookAI/roberta-base\", num_labels=2\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96304b7b-4d6e-402a-8540-9cc7a74a49a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check the model configuration\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3bb57f-5594-42d2-814f-97e762ef354c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to get a multiple choice question\n",
    "display_quiz([quiz[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e4a8a-6fa2-4264-b4d6-9318cad4dd72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the tokenizer\n",
    "text_input = \"I am learning.\"\n",
    "tokenizer(text_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf6755-f5c2-4c8f-9866-a2d3a5a64522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can see that one word can be divided into several tokens\n",
    "tokenizer(\"IDRIS\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b48d2-a3a7-4b78-a218-97bbd0bdda39",
   "metadata": {},
   "source": [
    "The padding technique allows us to transform a list of vectors into a matrix, which is essential for efficiently using a Transformer model. Padding involves adding zeros (or any other designated padding token) to sequences of different lengths to make them equal in length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6c38a-c650-4d79-a4dc-35f6d0f68988",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Without padding:**<br>\n",
    "![image](./images/without_padding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2692969-2731-4433-92d6-415ee9ff25e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the tokenizer with a batch of several input without padding\n",
    "texts_input = [\"I am learning.\", \"IDRIS is hosting the supercomputer Jean Zay\"]\n",
    "tokenizer(texts_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7d0fc-f94c-4b06-a9b8-72b3a5a7f893",
   "metadata": {},
   "source": [
    "**With padding:**<br>\n",
    "![image](./images/with_padding.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073c76f3-0e34-4f6e-bfad-a1163728ace8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the tokenizer with a batch of several input and pytorch Tensor transformation\n",
    "# We need to add padding to make a matrix\n",
    "texts_input = [\"I am learning.\", \"IDRIS is hosting the supercomputer Jean Zay\"]\n",
    "model_inp = tokenizer(texts_input, return_tensors=\"pt\", padding=True)\n",
    "model_inp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee1b834-45f7-4742-b4a0-237e2f05f548",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fee224-68f4-4031-b110-90755abaeeff",
   "metadata": {},
   "source": [
    "**Input and output of a Transformer for sequence classification:**<br>\n",
    "![image](./images/in-out_encoder_classification.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafb4f3d-5fc4-4c14-8bac-40262efa6783",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test the truncation to set a maximum length of the Tensor\n",
    "model_inp = tokenizer(\n",
    "    texts_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=10\n",
    ")\n",
    "model_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af866c10-35ce-4d50-8cf3-9d2b8ff6d5ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# compute inference\n",
    "model_inp = model_inp.to(\"cuda\")\n",
    "out = model(**model_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e8797-dea0-47a7-a00f-06ccfbbad545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Type of the HuggingFace output: {type(out)}\")\n",
    "print(f\"List of element of the output: {out.keys()}\")\n",
    "print(f\"Type of the logits: {type(out['logits'])}\")\n",
    "print(f\"Shape of the logits: {out['logits'].shape}\")\n",
    "print(f\"Value: {out['logits']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec61f369-c1ee-4883-978b-1b5e0c5d0e0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to get a multiple choice question\n",
    "display_quiz([quiz[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673da9f4-3e0a-40c0-837f-789396bda677",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Create the Data Pipeline**\n",
    "In this section, we will create the PyTorch dataset and dataloader that will be used to feed the model during training. The data pipeline is an essential component of the training process as it handles the loading and preprocessing of the data, ensuring that it is in the appropriate format for the model.\n",
    "\n",
    "The dataset class will provide the model with the input data and corresponding labels, while the dataloader will handle the batching and shuffling of the data.\n",
    "\n",
    "In this section, we will implement the necessary code to create the data pipeline, allowing us to seamlessly integrate it into the training loop and train our model effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca0b33-2b1d-4ad3-be8d-9f66de1ce56e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to implement a custom PyTorch Dataset class, which we'll name `IMDBDataset`. This class should be designed to work with the IMDB dataset provided by HuggingFace's datasets library.<br><br>The `IMDBDataset` class should override the `__getitem__` method such that when an instance of the class is indexed at `i` (e.g., `instance[i]`), it returns a tuple containing the text and the label of the `i`-th sample in the underlying HuggingFace IMDB dataset. In other words, if `dataset` is an instance of `IMDBDataset`, then `dataset[i]` should yield `(text, label)` where `text` is the review text and `label` is the sentiment label (positive or negative) of the `i`-th sample in the IMDB dataset.<br><br>Remember to also implement the `__len__` method to return the total number of samples in the dataset. This is a requirement for PyTorch's Dataset interface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007c8fc7-9edb-49d6-bc4a-59b9592fd9c2",
   "metadata": {},
   "source": [
    "**IMDB pytorch dataset representation:**<br>\n",
    "![image](./images/imdb_dataset.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04533568-d68e-4326-ae74-11a931bee7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18af7a1b-8cdb-4298-9c38-f008eec63637",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d31b8c8f-3c90-4a86-a50f-62415514c9b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "        return model_inp, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e7d13c-6824-466a-aa4c-8680082d6710",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ce0c03d-18d9-4527-879d-fb12d861c691",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "        hf_element = self.hf_dataset[idx]\n",
    "############ Complete here ############\n",
    "        model_inp =\n",
    "        label =\n",
    "#######################################     \n",
    "        \n",
    "        return model_inp, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991748f7-392e-4d33-8a2e-91355833bae4",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "048b7e38-c269-4080-89e5-d1c5a9f2508a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, hf_dataset):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx) -> (str, torch.Tensor):\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "        hf_element = self.hf_dataset[idx]\n",
    "        \n",
    "        model_inp = hf_element[\"text\"]\n",
    "        label = hf_element[\"label\"]\n",
    "        \n",
    "        return model_inp, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b0347b-173a-42dd-9bf3-58d8b7510dc2",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a661e5-d27e-4c3c-8b88-88305aab53b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = IMDBDataset(imdb_dataset[\"train\"])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed620a1",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6122972b-5345-4a02-a142-6934d03fee0f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to implement a PyTorch DataLoader along with a custom collate function. This DataLoader should be designed to accept an instance of the IMDBDataset class you previously created.<br><br>The DataLoader's purpose is to load data in batches from the IMDBDataset instance during the training process. It should handle batching, shuffling, and parallel data loading.<br><br>The collate function is a necessary component that you'll need to define and pass to the DataLoader. This function will be used to combine multiple data samples from your IMDBDataset into a single batch. It should take a list of samples (each being a tuple of text and label from IMDBDataset) and return a batch of token ids (with the mask attention) and a batch of labels.<br><br>The final output of the DataLoader (when iterated over) should be batches of inputs and labels ready to be fed into your model for training. Each input batch should correspond to a batch of labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe84a4b7-f701-4042-a3e8-592b3193c72d",
   "metadata": {},
   "source": [
    "**IMDB collate function of pytorch dataloader representation:**<br>\n",
    "![image](./images/collate_imdb.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e8c80-9549-4c60-952b-6c97e5233757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01b27938-2756-4af8-a5c4-fed97a5082b5",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c4ec21e-c9e1-4e34-b504-a6abe0892d4b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate_fn(batch):\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "    return model_inp, label_tens\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa5ffd5-bcb5-4d8a-853e-9b892292b93e",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52c9193b-5d7c-4587-bd99-6f33c63852b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate_fn(batch):\n",
    "    # Seperate texts from labels\n",
    "    text_list = [element[0] for element in batch]\n",
    "    label_list = [element[1] for element in batch]\n",
    "    \n",
    "############ Complete here ############\n",
    "    # Tokenize texts\n",
    "    model_inp =\n",
    "    # Transform Labels into Pitorch Tensor of int\n",
    "    label_tens =\n",
    "#######################################\n",
    "    return model_inp, label_tens\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325dcf0-c568-4301-bf29-5c8251da7bd5",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f76822ff-4f86-40b0-8597-b7b53fc853f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate_fn(batch):\n",
    "    # Seperate texts from labels\n",
    "    text_list = [element[0] for element in batch]\n",
    "    label_list = [element[1] for element in batch]\n",
    "    \n",
    "    # Tokenize texts\n",
    "    model_inp = tokenizer(\n",
    "        text_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    )\n",
    "    # Transform Labels into Pitorch Tensor of int\n",
    "    label_tens = torch.LongTensor(label_list)\n",
    "    return model_inp, label_tens\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=16,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccad700-f4eb-4ffe-a7ab-56da0ea86677",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff4acd6-21b4-4f1c-b67a-30d1e4f93ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_inp, labels in dataloader:\n",
    "    print(model_inp)\n",
    "    print(model_inp['input_ids'].shape)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d70fd",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92931577-87a4-4f9d-ad52-22ef802d3cf5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Create the training loop**\n",
    "\n",
    "In this section, we will implement the training loop for our model. The training loop is responsible for iterating over the dataset, making predictions, computing the loss, and updating the model's weights.\n",
    "\n",
    "The steps of the training loop are as follows:\n",
    "\n",
    "1. Sample a batch: We randomly select a batch of data from the dataset, which consists of both the model inputs and the corresponding labels.\n",
    "\n",
    "2. Forward pass: We pass the batch through the model to obtain predictions. This step involves feeding the model inputs into the model and obtaining the output.\n",
    "\n",
    "3. Compute the loss: We compare the model's predictions with the actual labels to compute the loss. The loss function measures the discrepancy between the predicted and actual values.\n",
    "\n",
    "4. Backward pass: We compute the gradients of the loss with respect to the model's parameters. This step involves calculating the derivative of the loss with respect to each parameter of the model.\n",
    "\n",
    "5. Update the weights: We update the model's weights using an optimization algorithm, such as stochastic gradient descent (SGD) or Adam. This step involves adjusting the parameters of the model in the direction that minimizes the loss.\n",
    "\n",
    "By repeating these steps for multiple epochs, the model gradually learns to make better predictions and minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1510ea25-20a4-4ca1-bab8-eedb9c174cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Optimizer and Criterion\n",
    "# We choose the CrossEntropyLoss and Adam because they're the most used\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35daa544-d16a-4388-9ce7-f5d4c19d5f02",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to define a function named `train_loop`. This function should accept four parameters: the HuggingFace model (`model`), the DataLoader instance (`dataloader`), the loss function (`criterion`), and the optimizer (`optimizer`). These parameters are assumed to have been initialized prior to calling this function.<br><br>The `train_loop` function should implement the training process for the model. This process typically involves iterating over the DataLoader, performing a forward pass of the model, computing the loss using the criterion, performing a backward pass, and then updating the model parameters using the optimizer. The specific steps for this process should have been described earlier in your instructions.<br><br>After the training process is complete (only 1 epoch in this case), the train_loop function should return the trained model.<br><br>Additionally, you should incorporate a \"test mode\" into the `train_loop` function. When this mode is activated, the training loop should terminate after 50 iterations. This mode is useful for testing or debugging the function without having to wait for the entire training process to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963ad5c-0a57-434b-bc06-ccf32b2b82a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ae1ef2a-d168-4844-ad0d-978574193012",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd6c54d4-5a42-48a1-b46e-3e34c0909682",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):   \n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d24d06-6cae-476b-84b5-e50cc4a965fe",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6bc04f6b-2b37-4914-b201-e6606be06b4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        # Set every gradient to 0 at the beginning of each iterations\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send the data to the GPU\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78436630-c05b-41fd-91d3-da876a283a3d",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "886947e3-31c1-490b-b32d-b8916fcf6a81",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        # Set every gradient to 0 at the beginning of each iterations\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Send the data to the GPU\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        # Predict the labels with the model\n",
    "        out = model(**model_inp)\n",
    "\n",
    "        # Compare the labels to the ground truth\n",
    "        loss = criterion(out.logits, labels)\n",
    "\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Update the model weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3adb61-3fb1-4256-9e86-509b1a0412f0",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f281fb4-00f0-4e42-b8c3-8d465b6e2470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = train_loop(model, dataloader, criterion, optimizer, test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80210f86",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1eb0e-de4a-4539-b76f-8daf90a8cf22",
   "metadata": {},
   "source": [
    "### **Use the Model**\n",
    "In this section, we will demonstrate how to use the trained model for making predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb5231a-47e1-4416-bb4a-0f86fe674a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_model(list_test, model):\n",
    "    model.eval()\n",
    "    \n",
    "    model_inp = tokenizer(\n",
    "        list_test, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(\"cuda\")\n",
    "    out = model(**model_inp)\n",
    "    predictions = out.logits.argmax(dim=1)\n",
    "    \n",
    "    for idx in range(len(list_test)):\n",
    "        print(\n",
    "            f'The review \"{list_test[idx]}\" is',\n",
    "            f'{\"negative\" if predictions[idx] == 0 else \"positive\"}.'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ab60b-1929-4a69-a267-766a7e0c7806",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_model([\"I hated this movie\", \"I loved this movie\"], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5f5c1-8290-4892-9387-6922f6862366",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e9de9a-4191-4f1e-a16e-0a1351f1b058",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Chatbot: Roleplay Dataset with Phi-2**\n",
    "\n",
    "The [Roleplay dataset](https://huggingface.co/datasets/hieunguyenminh/roleplay) is a collection of chats between a user and a character. Each chat is preceded by a system prompt that introduces the character. In this section, we will be using the [Phi-2](https://huggingface.co/microsoft/phi-2) Decoder model, which is a 2.7B model trained with a Causal Language Modeling objective.\n",
    "\n",
    "Our goal is to finetune the Phi-2 model to create a chatbot that can mimic any character we define. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee22e55-ebc1-4b0f-97b6-11b744539ff9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118386c-68b0-4636-8685-8814cf54d2ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "roleplay_dataset = datasets.load_from_disk(DSDIR / \"HuggingFace/hieunguyenminh/roleplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a3b97-2a40-4713-9d14-685a6aa81195",
   "metadata": {},
   "source": [
    "Feel free to explore the dataset in more depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f278381a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f72aea-3a29-4fb1-b832-8424c5c381ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_quiz(quiz[2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5fa6f6-3fc1-4834-a8d7-908d4bc7a41f",
   "metadata": {},
   "source": [
    "### **Discover the Model and its Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13587277-808c-4e26-962b-5e0d19c764cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model and its tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    DSDIR / \"HuggingFace_Models/microsoft/phi-2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,  # Allow using code that was not written by HuggingFace\n",
    "    attn_implementation=\"flash_attention_2\"  # Optimize the model with Flash Attention\n",
    ").to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/microsoft/phi-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5337bf6d-2be7-41f5-a02f-5714e96e9ba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c322b9d-0bf0-4526-9dda-78c36a257719",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Implement a function that counts the total number of tokens in a given HF dataset split. The function should take as input the dataset split and the tokenizer, and return the number of tokens in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5d789-f02f-4724-a6dd-580b0483d973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c416a0b1-b03c-4115-ae6e-abfec406a11e",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d07608a-f690-4cf9-a131-c8a705507782",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def count_tokens(hf_dataset, tokenizer):\n",
    "    total_tokens = 0\n",
    "    loop = tqdm(hf_dataset)\n",
    "    for element in loop:\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "        \n",
    "        loop.set_postfix(tokens_count=total_tokens)\n",
    "        \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c12fc-eed5-45b3-b9cf-fb1532f59c17",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "123e0e36-0299-45c2-9d8a-3ca87c52d2bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def count_tokens(hf_dataset, tokenizer):\n",
    "    total_tokens = 0\n",
    "    loop = tqdm(hf_dataset)\n",
    "    for element in loop:\n",
    "        list_token = tokenizer(element['text'])[\"input_ids\"]\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "        loop.set_postfix(tokens_count=total_tokens)\n",
    "        \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a93202-6e67-49af-a091-7b3bab8e366c",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c945cdd7-4e88-4454-944f-073d632f3f79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def count_tokens(hf_dataset, tokenizer):\n",
    "    total_tokens = 0\n",
    "    loop = tqdm(hf_dataset)\n",
    "    # iterate over every element of the dataset\n",
    "    for element in loop:\n",
    "        # Count the number of token of one element\n",
    "        nb_token_element = len(tokenizer(element['text'])[\"input_ids\"])\n",
    "        # Add the count to the total count\n",
    "        total_tokens += nb_token_element\n",
    "        \n",
    "        loop.set_postfix(tokens_count=total_tokens)\n",
    "        \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78587317-7535-4463-9028-61e94610d6d1",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c25ff8c-01a0-4427-9508-5b93909bba38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "count_tokens(roleplay_dataset['train'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646653f8-6679-418b-bc35-ab74d850433e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_quiz([quiz[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37345ee",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df5cfc7-1e4f-474e-8a8e-f4186411e195",
   "metadata": {},
   "source": [
    "To test our model, which is a Decoder-style Transformer, we can generate text by using it in an auto-regressive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092be3ee-a487-4a6f-b552-517b80f2577b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    \"\"\"Generate text from a prompt and print it.\"\"\"\n",
    "    model_inp = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # the generate() method is a succession of forward (auto-regressive) \n",
    "    out = model.generate(input_ids=model_inp[\"input_ids\"], do_sample=False, max_new_tokens=100)\n",
    "    print(tokenizer.decode(out[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd163870-286c-4696-ab5e-fd3dca28f417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is a supercomputer ?\"\n",
    "generate_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1868810e-96eb-4bcd-8384-2b276d3a844a",
   "metadata": {},
   "source": [
    "Now let's explore the capabilities of the model by using it as a roleplay chatbot, similar to the roleplay dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff522e1f-ef00-4f45-b99c-de392fa15f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|assistant|>\"\"\"\n",
    "generate_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f106ec0-549f-4b60-939c-8247a9802fb3",
   "metadata": {},
   "source": [
    "The answer seems good but the LLM do not impersonate the character. So we need to train it for that.\n",
    "<br>Let's examine the actual output of the model (without any post-processing):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec367ea-2e0f-4caa-ba54-3bc883ba19a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"What is a supercomputer ?\"\n",
    "list_token_ids = tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\n",
    "print(f\"Shape of the model input: {list_token_ids.shape}\")\n",
    "out = model(list_token_ids)\n",
    "print(f\"Shape of the model output: {out.logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40c84d",
   "metadata": {},
   "source": [
    "The output of the model is a list of logits for each input token. These logits (afer applying softmax) represent the probability distribution over the vocabulary, indicating the likelihood of each token being the next token in the sequence. The higher the logit value for a token, the more likely it is to be the next token in the sequence:\n",
    "![image](./images/in-out_decoder.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24a2287-512c-45eb-8ceb-e386a7c52825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display_quiz([quiz[5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e7d097-2780-4a27-a5f4-db016f10d9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b63117e3-4eab-44dc-992a-65a3c25439f6",
   "metadata": {},
   "source": [
    "### **Create the Data Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ab416-f947-4de3-8189-929ad6cc4300",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Implement a custom PyTorch IterableDataset class named `RoleplayDataset` that works with the Roleplay dataset provided by HuggingFace's datasets library. The `RoleplayDataset` class should tokenize the texts of the HF dataset and concatenate the token IDs representing samples until it reaches a specified limit (`seq_length`). Different samples can be in the same sequence, separated by the end of sequence token (`eos_token`).<br><br>The RoleplayDataset should return the `model_input` and the `labels` from the sequence of tokens. The `model_input` is the sequence of tokens without the last token, and the `labels` is the same sequence of tokens without the first token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95641dcf",
   "metadata": {},
   "source": [
    "**Token concatenation dataset pipeline:**<br>\n",
    "![image](./images/concat_dataset.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea5427-2851-4c71-8bdc-9f2d582c0010",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3af5f516-7585-4d66-ab48-345f4eecf962",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ec47132-b4c1-4950-9a99-be04e98dc1ac",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class RoleplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024, nb_tokens=3160542):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.separator = tokenizer.eos_token_id  # The token that will seperate different sample\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.rng = random.Random(53)\n",
    "        # We use an infinite iterator to be able to iterate over the dataset\n",
    "        # as long as we want. It's from __len__ method that will know when to stop\n",
    "        self.idx_iterator = self.infinite_iterator()\n",
    "\n",
    "    def __len__(self):\n",
    "############ Complete here ############\n",
    "        return \n",
    "#######################################\n",
    "\n",
    "    def infinite_iterator(self):\n",
    "        \"\"\"Infinite iterator over the indexes of the HF_dataset.\"\"\"\n",
    "        indexes = list(range(len(self.hf_dataset)))\n",
    "        while True:\n",
    "            self.rng.shuffle(indexes)\n",
    "            yield from indexes\n",
    "\n",
    "    def get_next_sample(self):\n",
    "        \"\"\"Retrieves the next sample from the dataset and tokenize it.\"\"\"\n",
    "        idx = next(self.idx_iterator)\n",
    "        text = self.hf_dataset[idx][\"text\"]\n",
    "        return self.tokenizer(text)['input_ids'] + [self.separator]\n",
    "\n",
    "    def __iter__(self):\n",
    "        next_sample_ids = None\n",
    "        all_token_ids = []\n",
    "        idx = 0\n",
    "\n",
    "        while idx < self.__len__():\n",
    "            if next_sample_ids is None:\n",
    "                next_sample_ids = self.get_next_sample()\n",
    "\n",
    "            if len(all_token_ids) + len(next_sample_ids) <= self.seq_length:\n",
    "                # if the next HF_dataset sample can fit in the current dataset sample\n",
    "                # we add it\n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "\n",
    "            else:\n",
    "                # if the next HF_dataset sample can't fit in the current dataset\n",
    "                # sample, we add what we can in the dataset sample and then we yield it\n",
    "                # note: we add one more element compared to seq_length to return to\n",
    "                # seq_length when generating inputs and targets (see train_collate())\n",
    "                idx_break = self.seq_length - len(all_token_ids)\n",
    "############ Complete here ############\n",
    "                all_token_ids += next_sample_ids[: ]\n",
    "                next_sample_ids = next_sample_ids[ :]\n",
    "#######################################\n",
    "\n",
    "############ Complete here ############\n",
    "                model_inp = \n",
    "                labels = \n",
    "#######################################\n",
    "                yield model_inp, labels\n",
    "\n",
    "                all_token_ids = []\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb04ee1-3b78-41d8-93f7-11636a18c528",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7a3e6606-c071-4621-812b-a757f3afbf4f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class RoleplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024, nb_tokens=3160542):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.separator = tokenizer.eos_token_id  # The token that will seperate different sample\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.rng = random.Random(53)\n",
    "        # We use an infinite iterator to be able to iterate over the dataset\n",
    "        # as long as we want. It's from __len__ method that will know when to stop\n",
    "        self.idx_iterator = self.infinite_iterator()\n",
    "\n",
    "    def __len__(self):\n",
    "############ Complete here ############\n",
    "        return \n",
    "#######################################\n",
    "\n",
    "    def infinite_iterator(self):\n",
    "        \"\"\"Infinite iterator over the indexes of the HF_dataset.\"\"\"\n",
    "        indexes = list(range(len(self.hf_dataset)))\n",
    "        while True:\n",
    "            self.rng.shuffle(indexes)\n",
    "            yield from indexes\n",
    "\n",
    "    def get_next_sample(self):\n",
    "        \"\"\"Retrieves the next sample from the dataset and tokenize it.\"\"\"\n",
    "        idx = next(self.idx_iterator)\n",
    "        text = self.hf_dataset[idx][\"text\"]\n",
    "        return self.tokenizer(text)['input_ids'] + [self.separator]\n",
    "\n",
    "    def __iter__(self):\n",
    "        next_sample_ids = None\n",
    "        all_token_ids = []\n",
    "        idx = 0\n",
    "\n",
    "        while idx < self.__len__():\n",
    "            if next_sample_ids is None:\n",
    "                next_sample_ids = self.get_next_sample()\n",
    "\n",
    "            if len(all_token_ids) + len(next_sample_ids) <= self.seq_length:\n",
    "                # if the next HF_dataset sample can fit in the current dataset sample\n",
    "                # we add it\n",
    "############ Complete here ############\n",
    "                all_token_ids += \n",
    "#######################################\n",
    "                next_sample_ids = None\n",
    "\n",
    "            else:\n",
    "                # if the next HF_dataset sample can't fit in the current dataset\n",
    "                # sample, we add what we can in the dataset sample and then we yield it\n",
    "                # note: we add one more element compared to seq_length to return to\n",
    "                # seq_length when generating inputs and targets (see train_collate())\n",
    "                idx_break = self.seq_length - len(all_token_ids)\n",
    "                all_token_ids += next_sample_ids[: idx_break + 1]\n",
    "                next_sample_ids = next_sample_ids[idx_break + 1 :]\n",
    "\n",
    "############ Complete here ############\n",
    "                # torch.tensor takes a list as input\n",
    "                model_inp = torch.tensor(  , dtype=torch.int64)\n",
    "                labels = torch.tensor(  , dtype=torch.int64)\n",
    "#######################################\n",
    "                yield model_inp, labels\n",
    "\n",
    "                all_token_ids = []\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bdff47-1e27-49f4-be8c-651d20fcc8f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1bf8f0db-cbdc-42b3-b05d-aaff6dbdf0d3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class RoleplayDataset(IterableDataset):\n",
    "\n",
    "    def __init__(self, tokenizer, hf_dataset, seq_length=1024, nb_tokens=3160542):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.separator = tokenizer.eos_token_id  # The token that will seperate different sample\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.nb_tokens = nb_tokens\n",
    "        self.rng = random.Random(53)\n",
    "        # We use an infinite iterator to be able to iterate over the dataset\n",
    "        # as long as we want. It's from __len__ method that will know when to stop\n",
    "        self.idx_iterator = self.infinite_iterator()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nb_tokens // self.seq_length\n",
    "\n",
    "    def infinite_iterator(self):\n",
    "        \"\"\"Infinite iterator over the indexes of the HF_dataset.\"\"\"\n",
    "        indexes = list(range(len(self.hf_dataset)))\n",
    "        while True:\n",
    "            self.rng.shuffle(indexes)\n",
    "            yield from indexes\n",
    "\n",
    "    def get_next_sample(self):\n",
    "        \"\"\"Retrieves the next sample from the dataset and tokenize it.\"\"\"\n",
    "        idx = next(self.idx_iterator)\n",
    "        text = self.hf_dataset[idx][\"text\"]\n",
    "        return self.tokenizer(text)['input_ids'] + [self.separator]\n",
    "\n",
    "    def __iter__(self):\n",
    "        next_sample_ids = None\n",
    "        all_token_ids = []\n",
    "        idx = 0\n",
    "\n",
    "        while idx < self.__len__():\n",
    "            if next_sample_ids is None:\n",
    "                next_sample_ids = self.get_next_sample()\n",
    "\n",
    "            if len(all_token_ids) + len(next_sample_ids) <= self.seq_length:\n",
    "                # if the next HF_dataset sample can fit in the current dataset sample\n",
    "                # we add it\n",
    "                all_token_ids += next_sample_ids\n",
    "                next_sample_ids = None\n",
    "\n",
    "            else:\n",
    "                # if the next HF_dataset sample can't fit in the current dataset\n",
    "                # sample, we add what we can in the dataset sample and then we yield it\n",
    "                # note: we add one more element compared to seq_length to return to\n",
    "                # seq_length when generating inputs and targets (see train_collate())\n",
    "                idx_break = self.seq_length - len(all_token_ids)\n",
    "                all_token_ids += next_sample_ids[: idx_break + 1]\n",
    "                next_sample_ids = next_sample_ids[idx_break + 1 :]\n",
    "\n",
    "                model_inp = torch.tensor(all_token_ids[:-1], dtype=torch.int64)\n",
    "                labels = torch.tensor(all_token_ids[1:], dtype=torch.int64) \n",
    "                yield model_inp, labels\n",
    "\n",
    "                all_token_ids = []\n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a2fe1-f1b0-42c4-b99d-d8c5e589de8b",
   "metadata": {},
   "source": [
    "**Test it here (it should return `True`):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da047f8-de5c-46e1-89ca-397e696bcb50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = RoleplayDataset(tokenizer, roleplay_dataset['train'], seq_length=512)\n",
    "model_inp, labels = next(iter(dataset))\n",
    "torch.equal(model_inp[1:], labels[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c70cf-6d45-4f8f-8113-edb82c906710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=1,\n",
    "    prefetch_factor=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e23c2-9ea4-4d18-b4b1-7242410a4165",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_inp, labels in dataloader:\n",
    "    print(torch.equal(model_inp[:, 1:], labels[:, :-1]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a7f0e",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61586825-08f1-4f90-8721-58dc74646e54",
   "metadata": {},
   "source": [
    "### **Create the training loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d97649-b351-4e75-bbf7-3611b3741ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Optimizer and Criterion\n",
    "# We choose the CrossEntropyLoss and Adam because they're the most used\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83864aab-12e1-4aae-bedb-ba2072d96728",
   "metadata": {},
   "source": [
    "In the training process, the shape of the `labels` tensor is expected to be `[batch_size, seq_length]`, while the shape of the `logits` tensor is expected to be `[batch_size, seq_length, vocab_size]`. However, before passing these tensors to the `CrossEntropyLoss` function, we need to reshape them to make it usable by the function. The `labels` tensor should have a shape of `[batch_size * seq_length]`, and the `logits` tensor should have a shape of `[batch_size * seq_length, vocab_size]`. To achieve this, we can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd42bdb-8bb7-48c0-9ea6-6ad2a9126d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_for_loss(logits, labels):\n",
    "    \"\"\"Unfold the Tensors to compute the CrossEntropyLoss correctly\"\"\"\n",
    "    batch_size, seq_length, vocab_size = logits.shape\n",
    "    logits = logits.view(batch_size * seq_length, vocab_size)\n",
    "    labels = labels.view(batch_size * seq_length)\n",
    "    return logits, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e367fae3-887e-413a-a421-c6e54d6b4bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_inp, labels in dataloader:\n",
    "    model_inp = model_inp.to(\"cuda\")\n",
    "    labels = labels.to(\"cuda\")\n",
    "    print(f\"labels tensor shape: {labels.shape}\")\n",
    "    logits = model(model_inp.to(\"cuda\")).logits\n",
    "    print(f\"logits tensor shape: {logits.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e14b90-2752-40e7-83de-06d62d5c7791",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits, labels = prepare_for_loss(logits, labels)\n",
    "print(f\"labels tensor shape after preparation: {labels.shape}\")\n",
    "print(f\"logits tensor shape after preparation: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b621a-791a-4f6f-9133-1681102176d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can finally compute a loss\n",
    "loss = criterion(logits, labels)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecde413-1260-401d-931b-8f1f404b66b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to define a function named `train_loop` that will be used to train a model. This function should be similar to the train loop you created previously for a classification task. However, you need to incorporate the `prepare_for_loss` function that we defined earlier to ensure correct computation of the loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44b785-137d-4b2a-9376-9d1607de7d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc8e591-6e3f-4651-b91d-ee536b7fe4a0",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "faba4af9-2515-464a-852d-55a51b2a91f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "############ Complete here ############\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546611d-8d05-4d14-b5f9-119164d6c184",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9497f19a-a243-42a2-8476-d6919f8cbc60",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "############ Complete here ############\n",
    "        logits = \n",
    "\n",
    "        logits, labels = \n",
    "#######################################\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dcacca-b0d7-48d3-9711-7aff8c8ad597",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa921927-fb57-410f-9114-fc6f1aab1b58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (model_inp, labels) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_inp = model_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "\n",
    "        logits = model(model_inp).logits\n",
    "\n",
    "        logits, labels = prepare_for_loss(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c64c5-62eb-410a-8f5f-19003f255830",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd83be50-80e5-42fa-b781-226f2eceee57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = train_loop(model, dataloader, criterion, optimizer, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759448c-e575-482c-bbbe-a6d3ba2a63f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"<|system|>Orphaned at age three, when he witnessed his mother's brutal murder, Dexter was adopted by Miami police officer Harry Morgan. Recognizing the boy's trauma and the subsequent development of his sociopathic tendencies, Harry trained Dexter to channel his gruesome bloodlust into vigilantism, killing only heinous criminals who slip through the criminal justice system.\n",
    "<|user|>How do you approach a new case, Dexter?\n",
    "<|assistant|>\"\"\"\n",
    "generate_text(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af140762",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc4f9e-f7a8-4fd6-8deb-213e7be800c6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de399eed-7efa-438e-9ce3-5155dd4faaf4",
   "metadata": {},
   "source": [
    "## **Summarization: SciTLDR dataset with T5**\n",
    "\n",
    "The [SciTLDR](https://huggingface.co/datasets/allenai/scitldr) dataset contains TLDR (Too Long, Didn't Read) summaries of research paper abstracts. In this section, we will be using the T5 model, which is an Encoder-Decoder model trained with masked language modeling (MLM) objective. Our goal is to finetune the [T5-large](https://huggingface.co/google-t5/t5-large) model on the SciTLDR dataset. By doing so, we aim to generate concise and informative summaries for research papers automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb32c12",
   "metadata": {},
   "source": [
    "### **Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ba2e5a-5119-4664-aac5-2eda8a18a205",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "scitldr_dataset = datasets.load_from_disk(DSDIR / \"HuggingFace/allenai/scitldr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73930241",
   "metadata": {},
   "source": [
    "Feel free to explore the dataset in more depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4555a18-a8be-43fd-acd1-bf2381834f15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "446f9eab",
   "metadata": {},
   "source": [
    "### **Discover the Model and its Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877519d0-5430-4698-ab20-a10f8de85797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(DSDIR / \"HuggingFace_Models/google-t5/t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(DSDIR / \"HuggingFace_Models/google-t5/t5-large\").to(\"cuda\")\n",
    "PAD_TOKEN_ID = tokenizer.pad_token_id  # We are gonna use it later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556745d",
   "metadata": {},
   "source": [
    "We can utilize an Encoder-Decoder model in an auto-regressive manner to generate text, similar to how a Decoder model operates. To accomplish this, we can employ the `generate` method:\n",
    "![image](./images/Encoder-Decoder_translation_example1.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841a8fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_text(text):\n",
    "    input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "    print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e285c-cec5-4c94-aba0-ab52f1f075fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "generate_text(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711f5792",
   "metadata": {},
   "source": [
    "In the auto-regressive generation process, the input to the Encoder model is the text provided in the `generate` method. The Decoder model then takes a 'beginning of sentence' token as input and generates text sequentially. This means that at each step, the model predicts the next token based on the previously generated tokens and output of the Encoder model. This process continues until a predefined maximum length is reached or an end token is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dddf15",
   "metadata": {},
   "source": [
    "Let's try to compute one forward pass (one step of the generation):<br>\n",
    "![image](./images/04_Encoder-Decoder_auto-regressive_inference.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a240b0-8602-439a-8ee4-e66a53b07f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"translate English to German: How old are you?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "# The BOS token is the first token of the input, for T5 decoder it's the ID 0\n",
    "decoder_input_ids = torch.tensor([[0]], dtype=torch.int64).to(\"cuda\")\n",
    "\n",
    "out = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ab96b5",
   "metadata": {},
   "source": [
    "Before we start the development, let's see what happens when we provide the model with an abstract of a research paper. For this purpose, we will use the abstract of the \"Attention is All You Need\" paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a452626-0a36-4b3f-b8d1-38327e57dad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf224832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_text(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc44d50",
   "metadata": {},
   "source": [
    "### **Create the Data Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55810511",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to implement a custom PyTorch Dataset class called `SciTLDRDataset`. This class should be designed to work with the SciTLDR dataset provided by HuggingFace's datasets library.\n",
    "\n",
    "The `SciTLDRDataset` class should override the `__getitem__` method. When an instance of the class is indexed at `i` (e.g., `instance[i]`), it should return a tuple containing the following:\n",
    "- A string that is the concatenation of all the elements of `source` of the HuggingFace dataset element.\n",
    "- A Tensor (1 dim) of IDs corresponding to the `target` of the HuggingFace dataset element.\n",
    "- The same Tensor of IDs without the first token and ending with the end-of-sequence token (`eos_token`).\n",
    "\n",
    "Additionally, please implement the `__len__` method to return the total number of samples in the dataset. This is a requirement for PyTorch's Dataset interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55ed5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67775d7e",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "89587bc5-4a1e-4cc6-85b6-2dd9c2d0d2bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class SciTLDRDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx) -> (str, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "        hf_element = self.hf_dataset[idx]\n",
    "        \n",
    "############ Complete here ############\n",
    "\n",
    "#######################################\n",
    "        \n",
    "        return encoder_inp, decoder_inp, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5db03ce",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d2d409e-bc33-4a86-9318-bfbcdacd3670",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class SciTLDRDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx) -> (str, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "        hf_element = self.hf_dataset[idx]\n",
    "        \n",
    "############ Complete here ############\n",
    "        decoder_ids = \n",
    "####################################### \n",
    "        \n",
    "        decoder_inp = torch.tensor(decoder_ids, dtype=torch.int64)\n",
    "        labels = torch.tensor(decoder_ids[1:] + [self.tokenizer.eos_token_id], dtype=torch.int64)\n",
    "        \n",
    "############ Complete here ############\n",
    "        encoder_inp = \n",
    "#######################################\n",
    "\n",
    "        \n",
    "        return encoder_inp, decoder_inp, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b4ea7",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9be75101-c2f1-42a8-b65d-07c4cb8b57a0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class SciTLDRDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of element of the dataset\"\"\"\n",
    "        return len(self.hf_dataset)\n",
    "    \n",
    "    def __getitem__(self, idx) -> (str, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"Return the input for the model and the label for the loss\"\"\"\n",
    "        hf_element = self.hf_dataset[idx]\n",
    "        \n",
    "        decoder_ids = [0] + self.tokenizer(hf_element[\"target\"][0], add_special_tokens=False)['input_ids']\n",
    "        decoder_inp = torch.tensor(decoder_ids, dtype=torch.int64)\n",
    "        labels = torch.tensor(decoder_ids[1:] + [self.tokenizer.eos_token_id], dtype=torch.int64)\n",
    "        \n",
    "        encoder_inp = \" \".join(hf_element[\"source\"])\n",
    "        \n",
    "        return encoder_inp, decoder_inp, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854b989",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f79fe-ad49-4404-9960-9532aa78814b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = SciTLDRDataset(scitldr_dataset['train'], tokenizer)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4b5785-f43e-49e3-9571-982abf0249c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It sould return True\n",
    "encoder_inp, decoder_inp, labels = dataset[0]\n",
    "torch.equal(decoder_inp[1:], labels[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015d7313",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea148add",
   "metadata": {},
   "source": [
    "To facilitate the creation of a batch of padded tensors and attention masks from a list of Tensors, which are returned by the dataset we defined previously, we will create two functions. These functions will be used in the collate function of the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a5b8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_padding(list_ids: list[torch.Tensor]) -> torch.Tensor:\n",
    "    \"\"\"Add padding to a list of tensors and return a padded tensor (batch)\"\"\"\n",
    "    padded_tensor = torch.nn.utils.rnn.pad_sequence(\n",
    "        [sample.flip(dims=(0,)) for sample in list_ids],\n",
    "        batch_first=True,\n",
    "        padding_value=PAD_TOKEN_ID,\n",
    "    ).flip(dims=(1,))\n",
    "    return padded_tensor\n",
    "\n",
    "\n",
    "def create_mask(padded_tensor: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Create a mask for HuggingFace models\"\"\"\n",
    "    decoder_mask = torch.logical_not(\n",
    "        (padded_tensor == torch.full_like(padded_tensor, 0))\n",
    "    ).to(dtype=torch.int)\n",
    "    return decoder_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f280899",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to create the collate function of a Dataloader that will transform the elements of the `SciTLDRDataset` into batches.\n",
    "\n",
    "To accomplish this, you will need to perform the following steps in the collate function:\n",
    "1. Transform the list of `encoder_inp` (strings) into a padded tensor with its attention mask using the tokenizer of the model, as you did previously.\n",
    "2. Transform the list of `decoder_inp` (Tensors) into a padded tensor with its attention mask using the `add_padding` and `create_mask` functions that were defined earlier.\n",
    "3. Transform the list of `labels` (Tensors) into a padded tensor using the `add_padding` function.\n",
    "\n",
    "The DataLoader should return the following:\n",
    "- `encoder_batch_inp`: A dictionary containing the padded encoder input and its attention mask.\n",
    "- `decoder_batch_inp`: The padded decoder input.\n",
    "- `labels_batch`: The padded labels used to compute the loss.\n",
    "- `decoder_mask`: The mask for the padded decoder input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d639e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "265c0206",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22c6a923-8720-45a1-add0-01ee13c0cfb2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate_fn(batch):\n",
    "############ Complete here ############\n",
    "    encoder_inp_list = \n",
    "    decoder_inp_list = \n",
    "    labels_list = \n",
    "    \n",
    "#######################################\n",
    "    \n",
    "    return encoder_batch_inp, decoder_batch_inp, labels_batch, decoder_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762ba22b",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9eea81e2-32d5-4d03-a04a-ed6c79bf67c6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate_fn(batch):\n",
    "    encoder_inp_list = [element[0] for element in batch]\n",
    "    decoder_inp_list = [element[1] for element in batch]\n",
    "    labels_list = [element[2] for element in batch]\n",
    "    \n",
    "############ Complete here ############\n",
    "    encoder_batch_inp = tokenizer(\n",
    "        \n",
    "    )\n",
    "    \n",
    "    decoder_batch_inp = \n",
    "    labels_batch = \n",
    "    \n",
    "    decoder_mask = \n",
    "#######################################\n",
    "    \n",
    "    return encoder_batch_inp, decoder_batch_inp, labels_batch, decoder_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b8a548",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cefb64af-c6b3-4bb1-8c38-73586eaa1052",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate_fn(batch):\n",
    "    encoder_inp_list = [element[0] for element in batch]\n",
    "    decoder_inp_list = [element[1] for element in batch]\n",
    "    labels_list = [element[2] for element in batch]\n",
    "    \n",
    "    encoder_batch_inp = tokenizer(\n",
    "        encoder_inp_list, return_tensors=\"pt\", padding=True, truncation=True, max_length=1024\n",
    "    )\n",
    "    \n",
    "\n",
    "    decoder_batch_inp = add_padding(decoder_inp_list)\n",
    "    labels_batch = add_padding(labels_list)\n",
    "    \n",
    "    decoder_mask = create_mask(decoder_batch_inp)\n",
    "    \n",
    "    return encoder_batch_inp, decoder_batch_inp, labels_batch, decoder_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d53cecc",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e98a7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    "    prefetch_factor=2,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "for encoder_inp, decoder_inp, labels, decoder_mask in dataloader:\n",
    "    print(encoder_inp)\n",
    "    print(\"#\" * 50)\n",
    "    print(decoder_inp)\n",
    "    print(\"#\" * 50)\n",
    "    print(labels)\n",
    "    print(\"#\" * 50)\n",
    "    print(decoder_mask)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef32f59",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4a3a55",
   "metadata": {},
   "source": [
    "Now, let's prepare the training loop. We will need the `prepare_for_loss` function again, which is responsible for reshaping the logits and labels to compute the loss using the `CrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fdb03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_for_loss(logits, labels):\n",
    "    \"\"\"Unfold the Tensors to compute the CrossEntropyLoss correctly\"\"\"\n",
    "    batch_size, seq_length, vocab_size = logits.shape\n",
    "    logits = logits.view(batch_size * seq_length, vocab_size)\n",
    "    labels = labels.view(batch_size * seq_length)\n",
    "    return logits, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa467b77-d9f3-4105-b141-c290313a0f3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize Optimizer and Criterion\n",
    "# We choose the CrossEntropyLoss and Adam because they're the most used\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8af475",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "<span style=\"color:red\">**Task**:</span> Your task is to define a function named train_loop that will be used to train a model. This function should be similar to the train loop you created previously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35330975",
   "metadata": {},
   "source": [
    "**Ease level 1:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "926a9932-cc5c-4166-8c8d-3c676a56fa1c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (encoder_inp, decoder_inp, labels, decoder_mask) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_inp = encoder_inp.to(\"cuda\")\n",
    "        decoder_inp = decoder_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        decoder_mask = decoder_mask.to(\"cuda\")\n",
    "\n",
    "############ Complete here ############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff0086",
   "metadata": {},
   "source": [
    "**Ease level 2:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "731c4038-f72d-40df-9eff-72c3159263ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (encoder_inp, decoder_inp, labels, decoder_mask) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_inp = encoder_inp.to(\"cuda\")\n",
    "        decoder_inp = decoder_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        decoder_mask = decoder_mask.to(\"cuda\")\n",
    "\n",
    "############ Complete here ############\n",
    "        logits = model(\n",
    "            input_ids=  ,\n",
    "            attention_mask=  ,\n",
    "            decoder_input_ids=  ,\n",
    "            decoder_attention_mask=  \n",
    "        ).logits\n",
    "#######################################\n",
    "\n",
    "        logits, labels = prepare_for_loss(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a57bb4",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2916b4b-4118-4702-bbd4-f92c50c8421e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, test=False):\n",
    "    model.train()\n",
    "    # tqdm for a nice progress bar\n",
    "    loop = tqdm(dataloader)\n",
    "\n",
    "    for i, (encoder_inp, decoder_inp, labels, decoder_mask) in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        encoder_inp = encoder_inp.to(\"cuda\")\n",
    "        decoder_inp = decoder_inp.to(\"cuda\")\n",
    "        labels = labels.to(\"cuda\")\n",
    "        decoder_mask = decoder_mask.to(\"cuda\")\n",
    "\n",
    "        logits = model(\n",
    "            input_ids=encoder_inp['input_ids'],\n",
    "            attention_mask=encoder_inp['attention_mask'],\n",
    "            decoder_input_ids=decoder_inp,\n",
    "            decoder_attention_mask=decoder_mask\n",
    "        ).logits\n",
    "\n",
    "        logits, labels = prepare_for_loss(logits, labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print next to progress bar\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df2fc3d",
   "metadata": {},
   "source": [
    "**Test it here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781fc11b-2f94-4665-b15c-60bff9542087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need 2 epoch to see the model improvement\n",
    "for epoch in range(2):\n",
    "    model = train_loop(model, dataloader, criterion, optimizer, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bada519-379c-466c-8906-541422379fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "outputs = model.generate(input_ids, max_new_tokens=100)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68eec10",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
