{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d27ec0-ce93-4bad-a0fd-ef3b9c894ece",
   "metadata": {},
   "source": [
    "# **07 - LLM Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7fe836-a519-43ac-b39a-c320c9bcdcaa",
   "metadata": {},
   "source": [
    "In this hands-on work, we are going to **evaluate a finetuned model for assistance**.  \n",
    "The foundation model is **phi-2** and the **dataset used for finetuning is Bitext** (https://huggingface.co/datasets/bitext/Bitext-customer-support-llm-chatbot-training-dataset). The model has been trained over **3 epochs in the same way as you did on the first day as part of the `roleplay`**.\n",
    "\n",
    "To perform this evaluation, we will need to **create an evaluation dataset**. The dataset will be of the \"supervised\" type, with a **prompt as input and the expected response as output**.  \n",
    "We'll see how this can be constructed from our data. Then we'll choose a **few metrics** and **write our evaluation loop**. \n",
    "\n",
    "**Note:** to simplify this evaluation, we will not be using an LLM-assisted metric for this practical work. These metrics could of course be used here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fad87d-f9e0-4c75-a988-79ba98b9cee6",
   "metadata": {},
   "source": [
    "**Uncomment the following cell for Jean-Zay only** (no internet access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41af00d-80ff-429f-ae27-6f37f0ff5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# cache_path = os.environ['WORK'] + \"/cache_spellm\"\n",
    "# os.environ['TRANSFORMERS_CACHE'] = cache_path\n",
    "# os.environ['HF_HOME'] = cache_path\n",
    "# os.environ['HF_DATASETS_CACHE'] = cache_path\n",
    "# os.environ['TORCH_HOME'] = cache_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6d57a6-48a3-47e6-b1d4-4c2732899639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import torch\n",
    "from bert_score import BERTScorer\n",
    "from detoxify import Detoxify\n",
    "from torch.utils.data import DataLoader, Dataset, IterableDataset\n",
    "from torchmetrics import Metric\n",
    "from tqdm.notebook import tqdm\n",
    "from torchmetrics.text import CHRFScore, BERTScore\n",
    "from utils import seed_everything, write_results_to_file\n",
    "from vllm import LLM, SamplingParams\n",
    "from langchain.prompts import PromptTemplate\n",
    "from jupyterquiz import display_quiz\n",
    "\n",
    "\n",
    "quiz_path = Path(\"./quiz/evaluation.json\")\n",
    "quiz = json.loads(quiz_path.read_text())\n",
    "\n",
    "DSDIR = Path(os.environ[\"DSDIR\"])\n",
    "DATASET_PATH = DSDIR / \"HuggingFace/bitext/Bitext-customer-support-llm-chatbot-training-dataset\"\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "seed_everything(53)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3b08bc-e5e7-427e-9fe3-a0cd6600ead5",
   "metadata": {},
   "source": [
    "**Path of the evaluated model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cb20ee-888e-4829-b709-889d3ae2ccff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(DSDIR / \"data_spellm/finetuned-phi-2\")  # finetuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585e0f52-4086-467b-873a-7ac874dbfd7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Discover the dataset**\n",
    "\n",
    "The `Bitext` dataset is a question and answer dataset for developing virtual assistants.  \n",
    "Each element is in particuliar composed of:\n",
    "- `instruction`: a user request from the Customer Service domain,\n",
    "- `category`: the type of request (for example: ORDER, REFUND...),\n",
    "- `item`: a more precise sub-category (for example: cancel_order, change_order or place_order for ORDER),\n",
    "- `response`: an example expected response from the virtual assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96cfd5-2d32-40b4-a0c6-9a068d8574ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(str(DATASET_PATH))['train'].select(range(20000))  # We select the first 20,000 elements of the dataset.\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52e768-2a1a-4be5-b670-743f15235529",
   "metadata": {},
   "source": [
    "We split our train/test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f5da9e-6f97-45a4-8c46-0f2e265753c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(train_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889f90f-5767-4a8a-b495-d7cabb7f1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to answer the following question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a178fc0e-b1e3-4cad-b75e-8bcf80e5ffba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz([quiz[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5738a14-e21c-4d8e-a4f9-2b370dcf2678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"The dataset contains {len(test_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858cb66d-2bfb-46b8-88be-c5aabdf3ddfe",
   "metadata": {},
   "source": [
    "It is important to note that for the training, **the same training set was used (same seed). Thus, our model never saw our test data.** Note that our train_test_split can be improved. In our evaluation set, we may have categories that we have never seen in the training set for example.\n",
    "\n",
    "Take a look at some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d33678f-25fa-4c5f-b257-6f6f6d9dae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset[500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1523dff2-2d7f-4af3-8efc-bc2f1f23125a",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "## **Creation of the evaluation dataset**\n",
    "\n",
    "Creating the evaluation set is a **very important step**. This evaluation set **must be similar to the scenarios encountered once they have been put into production**.  \n",
    "\n",
    "During training, the examples were concatenated with a certain template, as follows:\n",
    "```\n",
    "# Category: {category}\n",
    "# Intent: {intent}\n",
    "<user_request>: {instruction}\n",
    "<system_response>: {response}\n",
    "```\n",
    "\n",
    "For our evaluation, **we choose to assess the assistant's responses by comparing them with the given reference**.\n",
    "The input will therefore take the following form:\n",
    "```\n",
    "# Category: {category}\n",
    "# Intent: {intent}\n",
    "<user_request>: {instruction}\n",
    "```\n",
    "\n",
    "For the output, we will expect the LLM to have `<system_response>:` as well as a response whose meaning is similar to the given response. The expected output will therefore be:\n",
    "```\n",
    "<system_response>: {response}\n",
    "```\n",
    "\n",
    "We're going to **create this PyTorch dataset**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1428082f-435c-4e3a-add2-864d8a608d35",
   "metadata": {},
   "source": [
    "To complete our variables, we are going to use the `PromptTemplate` from `Langchain`. Here's how to use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ab5cc-e64f-4d2c-a50c-ce9178652a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the template\n",
    "test_prompt_template = PromptTemplate.from_template(\"\"\"# Category: {category}\n",
    "# Intent: {intent}\n",
    "<user_request>: {instruction}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801d81a6-d4db-4812-a750-b6152fe459d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the template with `format` function\n",
    "print(\n",
    "    test_prompt_template.format(\n",
    "        category=\"the category\", \n",
    "        intent=\"the intent\", \n",
    "        instruction=\"the instruction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892311fe-9390-4fa1-8c07-87383cb1a6ef",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to complete the code for the `EvalBitextDataset` class. It will output the input shown above and the expected answer. Do not apply tokenization.  Indeed, our model will take strings directly as input, and the metrics also need strings directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c48fc6-2db9-4077-b312-5be9d14bd284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BitextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, prompt_template):\n",
    "        self.hf_dataset=hf_dataset\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        ############ Complete here ############\n",
    "        return \n",
    "        #######################################\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the input (prompt) for the model and the associated expected answer\"\"\"\n",
    "        ############ Complete here ############\n",
    "        element = \n",
    "\n",
    "        inp = \n",
    "        target=\n",
    "        #######################################\n",
    "\n",
    "        return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9232c4-c72d-4848-a751-ac098946e055",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed450de1-598c-4927-b639-f2f991123381",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class BitextDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, prompt_template):\n",
    "        self.hf_dataset=hf_dataset\n",
    "        self.prompt_template = prompt_template\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return the input (prompt) for the model and the associated expected answer\"\"\"\n",
    "        element = self.hf_dataset[idx]\n",
    "\n",
    "        inp = self.prompt_template.format(\n",
    "            category=element['category'],\n",
    "            intent=element['intent'],\n",
    "            instruction=element['instruction']\n",
    "        )\n",
    "\n",
    "        target=\"<system_response>: \" + element['response']\n",
    "\n",
    "        return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13519ec-6a3c-4696-958f-63f725e674e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Our evaluation dataset has now been created. We can initialize it with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f88415-cd1f-447e-90a6-c270579e19db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = BitextDataset(\n",
    "    hf_dataset=test_dataset, \n",
    "    prompt_template=test_prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20570b19-fa64-4572-9bb4-b154e99a6549",
   "metadata": {},
   "source": [
    "View a few examples manually and think about possible areas for improvement for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4052d6-f66a-4ce4-87b6-8916ae05230f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp, target = dataset[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b045f6-ed86-4523-a5c1-357d2f637983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c01d6-cfcf-4985-8dbc-6dcd2a014faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9048794-2d72-4f01-a441-2cb9e535b19a",
   "metadata": {},
   "source": [
    "After looking at a few examples, you can see that some instructions and responses **contain placeholders (`{{ }}`)**. We might wonder how our model would react if these placeholders were replaced by values. For this tutorial, we won't go any further on this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5940ee8-b579-41c5-aa9b-f4e8c0532c1e",
   "metadata": {},
   "source": [
    "### **Dataloader**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b1c76-6c33-4fad-8510-0204e6918f86",
   "metadata": {},
   "source": [
    "Now let's create a dataloader for batch evaluation. We'll give our LLM batches of prompts. We will then compare the responses generated with those expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f059d51a-80c8-4f3d-b274-1af5d9b0bc6a",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to write the function that formats the batches correctly. This is called `collate`. It receives a list of elements as input. There are `batch_size` elements. These elements contain a prompt and the expected response. As output, we want to have **two lists of strings**: the first containing the prompts and the second containing the expected responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7441446d-554a-4fae-bacf-a48fa1704372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac4e752-f7df-4a52-9f4a-d0a626cd320e",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4287a2a-10f7-46f2-a6bc-a5d6e5166303",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def collate(batch):\n",
    "    prompt_list = [element[0] for element in batch]\n",
    "    answer_list = [element[1] for element in batch]\n",
    "    \n",
    "    return prompt_list, answer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604245d-7ff1-4947-938b-d63957fc9d16",
   "metadata": {},
   "source": [
    "Initialize the dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f01f256-e9ed-4965-844b-d3fc0f1f4796",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_quiz([quiz[3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaeabef-ba49-4343-b779-b9576b5f6666",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=8,  # Warning: If you have a memory error during evaluation, lower batch_size to 4.\n",
    "    num_workers=1,\n",
    "    prefetch_factor=2,\n",
    "    collate_fn=collate,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7421cfb0-8b37-46b6-bd1c-09ebb312c037",
   "metadata": {},
   "source": [
    "Note: by default, the DataLoader `collate_fn` does exactly what we did in the `collate` function. We could have done without it here.\n",
    "\n",
    "You can take a look at a few batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697654f0-663e-42fa-9b85-f05c9cb4c25c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b0 = next(iter(eval_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e887a4-ccc0-4062-9b5e-ca9555299102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b0[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a72f5b-c392-40fb-ae45-07595dd20036",
   "metadata": {},
   "source": [
    "## **Loading the model**\n",
    "\n",
    "Now, we're going to load the trained model.  \n",
    "To do this we will use **vLLM**. This is an **easy-to-use tool for accelerated inference**. This is much faster than the `from_pretrained` method. \n",
    "**vLLM will be explained in the next part of the course**.\n",
    "\n",
    "Even so, text generation is **time-consuming**. To limit this impact during the practical work, **we are limiting the number of output tokens to 100**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a9340-3d7f-40fc-8cbe-d35b43894284",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(model=str(MODEL_PATH), gpu_memory_utilization=0.75, seed=53)\n",
    "sampling_params = SamplingParams(max_tokens=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36824a8-3bf8-4092-ac88-6a57a230ac00",
   "metadata": {},
   "source": [
    "You can generate a few answers based on examples of your choice. Just change the value of the `idx` variable. The prompt and the generated text will be displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a5e71-e2c0-4e36-8996-7ada93a11f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change idx to test on multiple samples\n",
    "idx = 1\n",
    "\n",
    "prompt = dataset[idx][0]\n",
    "print(f\"{'*'*50} Prompt {'*'*50}\\n{prompt}\\n\\n\")\n",
    "\n",
    "result = llm.generate(prompt, sampling_params, use_tqdm=False)\n",
    "\n",
    "print(f\"{'*'*50} Answer generated {'*'*50}\\n{result[0].outputs[0].text}\\n\\n\")\n",
    "\n",
    "print(f\"{'*'*50} Reference answer {'*'*50}\\n{dataset[idx][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49de77a6-9a14-4ba1-b4a0-d6a92e17ba65",
   "metadata": {},
   "source": [
    "**Note**: you'll sometimes see a `<user_request>` again in LLM-generated responses after the generated response. This is logical given its training. We could prevent the LLM from continuing to generate text as soon as it sees the <user_request> template again. We won't do this in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d0c32e-f2cc-4da6-8263-ac2d5ed99cd4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Evaluation loop**\n",
    "\n",
    "Now we can create our evaluation loop.  \n",
    "\n",
    "We will **batch-generate the responses to the prompt and calculate the metrics as we go along**. For each metric, an internal counter will show the overall score.\n",
    "**Another approach is to generate all the prompt results at once and then calculate the metrics**. In our case, this could work, but it can **require more memory space** to process them after.\n",
    "\n",
    "The structure of the loop will therefore look like this:\n",
    "```python\n",
    "loop = tqdm(eval_dataloader)\n",
    "\n",
    "for i, (prompts, targets) in enumerate(loop):\n",
    "    # generation of responses from prompts\n",
    "\n",
    "    # computation of metrics from generated responses and expected responses if needed\n",
    "    # obtaining scores for the current batch from metrics\n",
    "\n",
    "# obtaining overall scores from metrics\n",
    "```\n",
    "\n",
    "\n",
    "For the evaluation, we will use the following metrics:\n",
    "- **chrF**,\n",
    "- **BERTScore**,\n",
    "- **Detoxify** to assess the toxicity of the responses generated. We want to ensure that our model does not respond to users with inappropriate language.\n",
    "\n",
    "The `torchmetrics` metrics work in batch mode by default. The implementation of chrF comes from this metric, so it's OK. \n",
    "  \n",
    "BERTScore is also implemented in torchmetrics but its implementation seems unreliable (the traditional metrics in torchmetrics are reliable.). We will use the official version and wrap it in a torchmetrics `Metric` object. We'll do the same for `Detoxify`.\n",
    "\n",
    "As you saw on the first day, this is how BERTScore works. We pass it a list of candidates and references and the metric computes a score for precision, recall and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42ae07d-1337-4c29-b1ee-f5fee96b2884",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bert_scorer = BERTScorer(model_type=\"microsoft/deberta-large-mnli\")  # deberta-large-mnli pearson correlation: 0.7736\n",
    "\n",
    "bert_scorer.score([\"a candidate\", \"another candidate\"], [\"a good reference\", \"another reference\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19724ba1-f1cb-48fc-a067-a1383b7b2567",
   "metadata": {},
   "source": [
    "This will work for one batch, but when we receive a new batch, the previous results will be lost. So we're going to implement some logic to update the metric over time.\n",
    "\n",
    "A `Metric` from torchmetrics has 2 functions to complete:\n",
    "- `update` which computes the score for the current batch and maintains the overall score of the metric,\n",
    "- `compute` which computes the final score for the metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c010883-b5f0-44c3-8cc0-5599194f914b",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> You will need to complete the code for the `BERTScoreTorchMetrics` class. You will need to complete the `update` and `compute` functions. An average of the precision, recall and F1 results must be returned. To do this, you will have access to 3 attributes: `precision`, `recall`, `f1` and `total` (the number of candidates processed).\n",
    "In the constructor, you can see lines like this:\n",
    "```python\n",
    "self.add_state(\"precision\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "```\n",
    "`precision` is an attribute of the class accessible via `self.precision`. The variable is initialized with a tensor having a value of 0.0. `dist_reduce_fx` is useful in distributed environments. Imagine you have batches on different GPUs and you need to group the results. Here `sum` means that with 2 GPUs, for example, the sum of the `self.precision` attributes will be performed. Torchmetrics makes it easy to create metrics in distributed environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc809d-3c2a-4ddc-b4cb-b228c90da817",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTScoreTorchMetrics(Metric):\n",
    "    full_state_update=False  # optimization: the metric state of one batch is completely independent of the state of other batches -> faster to compute\n",
    "    def __init__(self, model_type):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scorer = BERTScorer(model_type=model_type)\n",
    "        \n",
    "        self.add_state(\"precision\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"recall\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"f1\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        \n",
    "    \n",
    "    def update(self, candidates, references):\n",
    "        ############ Complete here ############\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #######################################\n",
    "   \n",
    "\n",
    "    def compute(self):\n",
    "        ############ Complete here ############\n",
    "        return {\n",
    "            \"precision\":\n",
    "            \"recall\":\n",
    "            \"f1\":\n",
    "        }\n",
    "        #######################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fcf399-6f1f-4c30-8c4c-938a2891365e",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0c4d022-19c9-46bc-9598-a509ba6d93ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "class BERTScoreTorchMetrics(Metric):\n",
    "    full_state_update=False  # optimization: the metric state of one batch is completely independent of the state of other batches -> faster to compute\n",
    "    def __init__(self, model_type):\n",
    "        super().__init__()\n",
    "\n",
    "        self.scorer = BERTScorer(model_type=model_type)\n",
    "        \n",
    "        self.add_state(\"precision\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"recall\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"f1\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "                \n",
    "    \n",
    "    def update(self, candidates, references):\n",
    "        P, R, F1 = map(sum, self.scorer.score(candidates, references))\n",
    "        self.precision += P\n",
    "        self.recall += R\n",
    "        self.f1 += F1\n",
    "        \n",
    "        self.total += len(candidates)\n",
    "        \n",
    "\n",
    "    def compute(self):\n",
    "        return {\n",
    "            \"precision\": self.precision / self.total,\n",
    "            \"recall\": self.recall / self.total,\n",
    "            \"f1\": self.f1 / self.total\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f433b6-9155-4669-b9b5-f53786d6f952",
   "metadata": {},
   "source": [
    "You can now try the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9eaae4-4fa4-4e63-9bde-266a86fa9d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bertscore_metric = BERTScoreTorchMetrics(model_type=\"microsoft/deberta-large-mnli\").to(\"cuda\")\n",
    "\n",
    "# bertscore_metric(candidates, references) calls `update` internally\n",
    "score_1 = bertscore_metric([\"a candidate\", \"another candidate\"], [\"a good reference\", \"another reference\"])\n",
    "score_2 = bertscore_metric([\"new candidate\"], [\"new reference\"])\n",
    "\n",
    "print(f\"{score_1=}\")\n",
    "print(f\"{score_2=}\")\n",
    "print(f\"{bertscore_metric.compute()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6363b2db-d275-4026-82a1-cc2caecb2985",
   "metadata": {},
   "source": [
    "**Recall can be biaised in our case**. It measures whether all the information in the reference can be found in the candidate answer. However, as we've limited the size of the generated outputs to 100 tokens, there's a good chance that our answers will be incomplete. Nevertheless, if this score is good, it's a good thing in view of our concise answers.  \n",
    "**Precision** will check whether our candidate's information can be found in our reference.. The **F1 score, which is a compromise between the 2**, is also interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77917ad-f36f-4793-b562-8132bb74a300",
   "metadata": {},
   "source": [
    "We'll do the same for **toxicity** with `Detoxify`. We'll give you the implementation. The idea is similar.\n",
    "\n",
    "As a reminder, `detoxify` takes as input only a list of strings and returns several scores: \n",
    "- `toxicity`, `severe_toxicity`, `identity_attack`, `obscene`, `insult`, `threat`, `sexual_explicit`.\n",
    "\n",
    "The implementation will return the **average score for each of these indicators**, as well as the **maximum score**. We don't want our LLM to ever make inappropriate comments.\n",
    "In addition, **if an output exceeds a threshold (configurable), we'll display it**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db98fe8d-b468-466c-bff0-a0da931d19b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DetoxifyTorchMetrics(Metric):\n",
    "    full_state_update=False  # optimization: the metric state of one batch is completely independent of the state of other batches -> faster to compute\n",
    "    def __init__(self, threshold: float = 0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.detoxify = Detoxify(\n",
    "            model_type='multilingual',\n",
    "            device=\"cuda\"\n",
    "        )\n",
    "        self.metrics = [\"toxicity\", \"severe_toxicity\", \"identity_attack\", \"obscene\", \"insult\", \"threat\", \"sexual_explicit\"]\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            self.add_state(metric, default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "            self.add_state(\"max_\" + metric, default=torch.tensor(0.0), dist_reduce_fx=\"max\")\n",
    "        \n",
    "        \n",
    "    def update(self, output: str, batch_idx: int = 0):\n",
    "        results = self.detoxify.predict(output)\n",
    "        batch_size = len(output)\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            setattr(self, metric, getattr(self, metric) + sum(results[metric]))\n",
    "            setattr(self, \"max_\" + metric, torch.max(getattr(self, \"max_\" + metric), torch.tensor(max(results[metric]))))\n",
    "            \n",
    "            for local_idx, score in enumerate(results[metric]):\n",
    "                if score > self.threshold:\n",
    "                    print(f\"{'#'*10} \\033[1m{metric}\\033[0m threshold exceeded | score : {round(score, 2)} {'#'*10}\\n {output[local_idx]}\")\n",
    "        \n",
    "        self.total += len(output)\n",
    "\n",
    "        \n",
    "    def compute(self):\n",
    "        return {metric: getattr(self, metric) / self.total for metric in self.metrics}, {\"max_\" + metric: getattr(self, \"max_\" + metric) for metric in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507373de-1949-4628-90f1-bdf2d3fd06d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "A little try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ab710-9156-4378-a595-a599b5052d51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "detoxify_metric = DetoxifyTorchMetrics(threshold=0.5).to(\"cuda\")\n",
    "\n",
    "score_1 = detoxify_metric([\"bonjour comment ca va ?\", \"shut up\"])\n",
    "score_2 = detoxify_metric([\"nous avons bien reçu votre demande\"])\n",
    "\n",
    "global_score = detoxify_metric.compute()\n",
    "global_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a0fa1-5781-46d9-9954-0eee4fbae9da",
   "metadata": {},
   "source": [
    "Everything is almost ready. All that remains is to **write the evaluation loop**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598ec5c9-3a33-47e6-9cf1-ca8777578719",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid red\"> \n",
    "\n",
    "> <span style=\"color:red\">**Task**:</span> Your task is to complete the validation loop function. Its signature is: `def eval_loop(model, dataloader, chrf_metric, bertscore_metric, detoxify_metric, sampling_params, test=True, write_results=True)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350285f3-bfeb-44e1-b63e-b5cb036c79a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_loop(model, dataloader, chrf_metric, bertscore_metric, detoxify_metric, sampling_params, test=True):\n",
    "    # reset metrics to initial state\n",
    "    chrf_metric.reset()\n",
    "    bertscore_metric.reset()\n",
    "    detoxify_metric.reset()\n",
    "\n",
    "    loop = tqdm(eval_dataloader, desc=\"eval_dataloader\")\n",
    "\n",
    "    for i, (prompts, targets) in enumerate(loop):\n",
    "        ############ Complete here ############\n",
    "        outputs = \n",
    "        outputs = \n",
    "        \n",
    "        score_chrf = \n",
    "        score_bertscore = \n",
    "        score_detoxify = \n",
    "\n",
    "        loop.set_postfix(\n",
    "            average_chrf=,\n",
    "            chrf=,\n",
    "            average_bertscore=,\n",
    "            bertscore=\n",
    "        )\n",
    "        #######################################\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "    \n",
    "    ############ Complete here ############\n",
    "    # Toxicity is only displayed at the end of training, as it is too verbose during training.\n",
    "    detoxify_score = \n",
    "    print(f\"Average detoxify: {}\")\n",
    "    print(f\"Max detoxify: {}\")\n",
    "    #######################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602f0606-3c5d-4078-8676-611bca635b19",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b04c0d60-b7dc-4bb0-b45d-32e73331ab63",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "def eval_loop(model, dataloader, chrf_metric, bertscore_metric, detoxify_metric, sampling_params, test=True):\n",
    "    # reset metrics to initial state\n",
    "    chrf_metric.reset()\n",
    "    bertscore_metric.reset()\n",
    "    detoxify_metric.reset()\n",
    "\n",
    "    loop = tqdm(eval_dataloader, desc=\"eval_dataloader\")\n",
    "\n",
    "    for i, (prompts, targets) in enumerate(loop):\n",
    "        outputs = model.generate(prompts, sampling_params, use_tqdm=False)\n",
    "        outputs = [out.outputs[0].text for out in outputs]\n",
    "        \n",
    "        score_chrf = chrf_metric(outputs, targets)\n",
    "        score_bertscore = bertscore_metric(outputs, targets)\n",
    "        score_detoxify = detoxify_metric(outputs, i)\n",
    "\n",
    "        loop.set_postfix(\n",
    "            average_chrf=chrf_metric.compute().item(),\n",
    "            chrf=score_chrf.item(),\n",
    "            average_bertscore=bertscore_metric.compute(),\n",
    "            bertscore=score_bertscore\n",
    "        )\n",
    "\n",
    "        if i >= 50 and test:\n",
    "            loop.close()\n",
    "            break\n",
    "    \n",
    "    detoxify_score = detoxify_metric.compute()\n",
    "    print(f\"Average detoxify: {detoxify_score[0]}\")\n",
    "    print(f\"Max detoxify: {detoxify_score[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff09d84-6fe0-4b40-8af4-4fa80450c874",
   "metadata": {},
   "source": [
    "Now that our loop has been created. All we have to do is initialize our metrics and put them on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdab78ef-60ac-4816-b01e-964ba9a5a3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chrf_metric = CHRFScore().to(\"cuda\")\n",
    "bertscore_metric = BERTScoreTorchMetrics(model_type=\"microsoft/deberta-large-mnli\").to(\"cuda\")\n",
    "detoxify_metric = DetoxifyTorchMetrics(threshold=0.5).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ca7aa-ee15-48f1-88d3-7f5b6129a86d",
   "metadata": {},
   "source": [
    "Let's start the evaluation. The results will be displayed in the terminal. In practice, we can log all our results on a platform such as `MLFlow` or `Weights & Biases`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689a4e88-5c54-44c0-ac44-357cb0780dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_loop(\n",
    "    model=llm, \n",
    "    dataloader=eval_dataloader, \n",
    "    chrf_metric=chrf_metric,\n",
    "    bertscore_metric=bertscore_metric,\n",
    "    detoxify_metric=detoxify_metric,\n",
    "    sampling_params=sampling_params,\n",
    "    test=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc29adc-9885-45fb-bc11-7adbfd7393f7",
   "metadata": {},
   "source": [
    "Well, we've got results for the model we trained. Now let's **compare them with the basic model**.\n",
    "\n",
    "The basic `phi-2` model cannot be evaluated in this notebook now because almost all the memory is already occupied by the first vLLM and it's complicated to free it up.\n",
    "We could re-run this exact notebook by changing the model path to a `phi-2` foundation model but **to make it quicker we've provided the results obtained with the foundation model below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143c4ee-8ded-416d-8ac9-ed35377138b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Max memory allocated: {torch.cuda.max_memory_allocated(device='cuda') / (1024 ** 3)} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c6d370-5697-4eb9-a054-e0d26daa58a6",
   "metadata": {},
   "source": [
    "**Results `phi-2`:**\n",
    "```\n",
    "CHRFScore: 0.1326485127210617\n",
    "\n",
    "BERTScoreTorchMetrics:\n",
    "{\n",
    "    'precision': 0.4066,\n",
    "    'recall':0.4033,\n",
    "    'f1': 0.4010\n",
    "}\n",
    "\n",
    "DetoxifyTorchMetrics: (\n",
    "    {\n",
    "        'toxicity': 0.0080,\n",
    "        'severe_toxicity': 0.0006,\n",
    "        'identity_attack': 0.0003,\n",
    "        'obscene': 0.0063,\n",
    "        'insult': 0.0034,\n",
    "        'threat': 0.0001,\n",
    "        'sexual_explicit': 0.0016\n",
    "    },\n",
    "    {\n",
    "        'max_toxicity': 0.9953,\n",
    "        'max_severe_toxicity': 0.2655,\n",
    "        'max_identity_attack': 0.0319,\n",
    "        'max_obscene': 0.9910,\n",
    "        'max_insult': 0.9479,\n",
    "        'max_threat': 0.0252,\n",
    "        'max_sexual_explicit': 0.7255\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740866c3-c386-4a29-9b0f-cbad023fa68f",
   "metadata": {},
   "source": [
    "**Note that to use LLM-assisted metrics**, we would have had to generate all our predictions, save them and then use a second notebook with another LLM evaluator. Two LLMs loaded with vLLM do not pass through a kernel (and practically everywhere). With an evaluator LLM via an API, this could have worked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
